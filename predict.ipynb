{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The following code is a classification neural network\n",
    "\n",
    "# Import all packages and libraries for NN\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy\n",
    "import keras\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# Preparing Iris dataset for usage as a dataframe\n",
    "# Encoding output variables\n",
    "# Shuffle rows randomly for variety\n",
    "# Spliting dataset to test and train dataframes. Test dataset will be 80% of entire dataframe.\n",
    "# Parsing dataframe to features and labels (x_train and y_train)\n",
    "\n",
    "# Constants\n",
    "_training_split = 0.8\n",
    "\n",
    "\n",
    "# Spliting dataframe to training and testing\n",
    "def split_to_training(dataframe):\n",
    "    train_df = dataframe[:int(len(dataframe)*_training_split)]\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_to_testing(dataframe):\n",
    "    test_df = dataframe[int(len(dataframe)*_training_split):]\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "\n",
    "# Spliting training or testing dataset to x and y\n",
    "def split_to_x(dataframe):\n",
    "    x = dataframe[features].values\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def split_to_y(dataframe):\n",
    "    y = dataframe[labels].values\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Encoding all labels with HOT Encoder\n",
    "def encode_dataframe(label_y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(label_y)\n",
    "    encoded_Y = encoder.transform(label_y)\n",
    "    new_y = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    return new_y\n",
    "\n",
    "\n",
    "def prep_data(df):\n",
    "    pass\n",
    "    \n",
    "# Load in dataset\n",
    "dataframe = pandas.read_csv('iris.csv')\n",
    "df = shuffle(dataframe)\n",
    "features = list(df.columns.values)[:-1]\n",
    "labels = list(df.columns.values)[-1]\n",
    "input_dim = len(list(df.columns.values)[:-1])\n",
    "\n",
    "train_df = split_to_training(df)\n",
    "x_train = split_to_x(train_df)\n",
    "y_train = encode_dataframe(split_to_y(train_df))\n",
    "\n",
    "test_df = split_to_testing(df)\n",
    "x_test = split_to_x(test_df)\n",
    "y_test = encode_dataframe(split_to_y(test_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasNeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        base_number_layers = int(input_dim**.5)\n",
    "        list_of_loss = []\n",
    "        list_of_models = []\n",
    "        list_of_history = []\n",
    "        for i in range(5):\n",
    "            model = Sequential()\n",
    "            # Base number of layers\n",
    "            model.add(Dense(input_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "            for i in range(base_number_layers):\n",
    "                model.add(Dense(5, activation='sigmoid'))\n",
    "            model.add(Dense(3, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "            list_of_models.append(model)\n",
    "\n",
    "            \n",
    "            history = model.fit(self.x_train, self.y_train, epochs=30)\n",
    "            list_of_history.append(history)\n",
    "        \n",
    "        \n",
    "            # Compare loss after each epoch\n",
    "            loss_after_each_epoch = history.history['loss']\n",
    "            avg_roc = abs((loss_after_each_epoch[0]-loss_after_each_epoch[-1])/len(loss_after_each_epoch))\n",
    "            list_of_loss.append(avg_roc)\n",
    "            base_number_layers += 1\n",
    "            \n",
    "            scores = model.evaluate(self.x_test, self.y_test)\n",
    "            print(\"{}: {}\".format(model.metrics_names[0], scores[0]))\n",
    "            print(\"{}: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        # Take the first trough in list and use that model\n",
    "        minimal_loss_position = 0;\n",
    "        for index, value in enumerate(list_of_loss):\n",
    "            if value < list_of_loss[index+1]:\n",
    "                minimal_loss_position = index\n",
    "                break\n",
    "        print(list_of_loss)\n",
    "        print(list_of_models)\n",
    "        print('The optimal model is at index: {}'.format(minimal_loss_position))\n",
    "        optimized_model = list_of_models[minimal_loss_position]\n",
    "        optimized_history = list_of_history[minimal_loss_position]\n",
    "        \n",
    "        # Evaluating the model\n",
    "        print(optimized_history.history)\n",
    "        scores = optimized_model.evaluate(self.x_test, self.y_test)\n",
    "        print(\"{}: {}\".format(optimized_model.metrics_names[0], scores[0]))\n",
    "        print(\"{}: {}%\".format(optimized_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.5031 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.5015 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 90us/step - loss: 0.5000 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4984 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.4968 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4953 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4937 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.4921 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4906 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4890 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4875 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4860 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.4845 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.4830 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4814 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.4800 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4785 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.4770 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4755 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4741 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.4726 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4712 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.4698 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4684 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4670 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4656 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4642 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4628 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.4615 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4602 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "loss: 0.4476425051689148\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 5ms/step - loss: 0.5171 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.5158 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.5145 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5132 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5118 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5105 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5091 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5078 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5064 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5051 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5038 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.5024 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.5011 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.4998 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4985 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4971 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4958 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4945 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4932 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4919 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4906 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4893 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4880 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4867 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4854 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4841 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4827 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4815 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.4801 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4789 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "loss: 0.49950215220451355\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.5107 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5096 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.5084 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5073 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.5062 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.5050 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5039 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5028 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5017 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5006 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4995 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4983 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4972 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4961 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4950 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4939 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4928 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4917 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4906 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4895 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4884 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4873 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4862 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4851 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4840 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4830 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4819 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4808 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4797 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4787 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "loss: 0.4731665253639221\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4797 - acc: 0.3083\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4781 - acc: 0.3083\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4766 - acc: 0.3083\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4751 - acc: 0.3083\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4735 - acc: 0.3083\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4721 - acc: 0.3083\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4706 - acc: 0.3083\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4691 - acc: 0.3083\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4676 - acc: 0.3083\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4661 - acc: 0.3083\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4646 - acc: 0.3083\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4632 - acc: 0.3083\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4617 - acc: 0.3083\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4602 - acc: 0.3083\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4587 - acc: 0.3083\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4572 - acc: 0.3083\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4557 - acc: 0.3083\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4542 - acc: 0.3083\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4527 - acc: 0.3083\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4513 - acc: 0.3083\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4498 - acc: 0.3083\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4483 - acc: 0.3083\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4469 - acc: 0.3083\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4454 - acc: 0.3083\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4440 - acc: 0.3083\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4425 - acc: 0.3083\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4411 - acc: 0.3083\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4396 - acc: 0.3083\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4383 - acc: 0.3083\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4369 - acc: 0.3083\n",
      "30/30 [==============================] - 0s 5ms/step\n",
      "loss: 0.41763171553611755\n",
      "acc: 43.33333373069763%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.5319 - acc: 0.3083\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5304 - acc: 0.3083\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.5288 - acc: 0.3083\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5272 - acc: 0.3083\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5256 - acc: 0.3083\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5240 - acc: 0.3083\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5225 - acc: 0.3083\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5209 - acc: 0.3083\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5193 - acc: 0.3083\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5178 - acc: 0.3083\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5162 - acc: 0.3083\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5147 - acc: 0.3083\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5131 - acc: 0.3083\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5116 - acc: 0.3083\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5101 - acc: 0.3083\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5085 - acc: 0.3083\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5070 - acc: 0.3083\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5055 - acc: 0.3083\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5040 - acc: 0.3083\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5025 - acc: 0.3083\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5009 - acc: 0.3083\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4994 - acc: 0.3083\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4979 - acc: 0.3083\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4964 - acc: 0.3083\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4949 - acc: 0.3083\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4935 - acc: 0.3083\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4920 - acc: 0.3083\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4905 - acc: 0.3083\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4891 - acc: 0.3083\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4877 - acc: 0.3083\n",
      "30/30 [==============================] - 0s 6ms/step\n",
      "loss: 0.4649133086204529\n",
      "acc: 43.33333373069763%\n",
      "[0.0014329378472434152, 0.0012754030360115909, 0.0010697439644071771, 0.001427561839421591, 0.0014758789539337143]\n",
      "[<keras.models.Sequential object at 0x11ab73668>, <keras.models.Sequential object at 0x11ac33630>, <keras.models.Sequential object at 0x11b525ba8>, <keras.models.Sequential object at 0x11ba128d0>, <keras.models.Sequential object at 0x11c8328d0>]\n",
      "The optimal model is at index: 2\n",
      "{'loss': [0.5107439557711283, 0.5095580498377482, 0.5084113240242004, 0.5072981337706248, 0.5061778247356414, 0.5050495664278666, 0.5039398550987244, 0.502822870016098, 0.5016963303089141, 0.5005672772725424, 0.49945424596468607, 0.4983351786931356, 0.49723094900449116, 0.496112189690272, 0.4949760973453522, 0.49390648206075033, 0.4927674770355225, 0.4916761060555776, 0.49058146675427755, 0.48947630723317465, 0.48838194012641906, 0.48730350931485494, 0.4862284779548645, 0.48514347076416015, 0.4840441743532817, 0.48297364115715025, 0.4818883021672567, 0.48081025878588357, 0.4797187089920044, 0.478651636838913], 'acc': [0.3333333333333333, 0.33333333432674406, 0.3333333333333333, 0.3333333353201548, 0.33333333134651183, 0.3333333333333333, 0.3333333353201548, 0.33333333134651183, 0.3333333333333333, 0.3333333333333333, 0.3333333353201548, 0.3333333353201548, 0.3333333353201548, 0.3333333353201548, 0.33333333134651183, 0.3333333323399226, 0.3333333323399226, 0.3333333333333333, 0.33333333134651183, 0.33333333730697634, 0.3333333333333333, 0.33333333134651183, 0.33333333432674406, 0.33333333134651183, 0.33333333432674406, 0.33333333134651183, 0.3333333333333333, 0.3333333333333333, 0.33333333134651183, 0.3333333323399226]}\n",
      "30/30 [==============================] - 0s 42us/step\n",
      "loss: 0.4731665253639221\n",
      "acc: 33.33333432674408%\n"
     ]
    }
   ],
   "source": [
    "NN = KerasNeuralNetwork(x_train, y_train, x_test, y_test)\n",
    "NN.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
