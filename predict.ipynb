{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The following code is a classification neural network\n",
    "\n",
    "# Import all packages and libraries for NN\n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import numpy\n",
    "import keras\n",
    "import pandas\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# Preparing Iris dataset for usage as a dataframe\n",
    "# Encoding output variables\n",
    "# Shuffle rows randomly for variety\n",
    "# Spliting dataset to test and train dataframes. Test dataset will be 80% of entire dataframe.\n",
    "# Parsing dataframe to features and labels (x_train and y_train)\n",
    "\n",
    "# Constants\n",
    "_training_split = 0.8\n",
    "\n",
    "\n",
    "# Spliting dataframe to training and testing\n",
    "def split_to_training(dataframe):\n",
    "    train_df = dataframe[:int(len(dataframe)*_training_split)]\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_to_testing(dataframe):\n",
    "    test_df = dataframe[int(len(dataframe)*_training_split):]\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "\n",
    "# Spliting training or testing dataset to x and y\n",
    "def split_to_x(dataframe):\n",
    "    x = dataframe[features].values\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def split_to_y(dataframe):\n",
    "    y = dataframe[labels].values\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Encoding all labels with HOT Encoder\n",
    "def encode_dataframe(label_y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(label_y)\n",
    "    encoded_Y = encoder.transform(label_y)\n",
    "    new_y = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    return new_y\n",
    "\n",
    "\n",
    "def prep_data(df):\n",
    "    pass\n",
    "    \n",
    "# Load in dataset\n",
    "dataframe = pandas.read_csv('iris.csv')\n",
    "df = shuffle(dataframe)\n",
    "features = list(df.columns.values)[:-1]\n",
    "labels = list(df.columns.values)[-1]\n",
    "input_dim = len(list(df.columns.values)[:-1])\n",
    "\n",
    "train_df = split_to_training(df)\n",
    "x_train = split_to_x(train_df)\n",
    "y_train = encode_dataframe(split_to_y(train_df))\n",
    "\n",
    "test_df = split_to_testing(df)\n",
    "x_test = split_to_x(test_df)\n",
    "y_test = encode_dataframe(split_to_y(test_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasNeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        _number_of_test_models = 2\n",
    "        \n",
    "        base_number_layers = int(input_dim**.5)\n",
    "#       group_models { loss_value: respective model {}\n",
    "        group_models = {}\n",
    "\n",
    "        list_of_history = []\n",
    "        for i in range(_number_of_test_models):\n",
    "            model = Sequential()\n",
    "            \n",
    "            # Base number of layers\n",
    "            model.add(Dense(input_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "            \n",
    "            # Loop adds an extra hidden layer after each individual model is trained\n",
    "            for i in range(base_number_layers):\n",
    "                model.add(Dense(4, activation='sigmoid'))\n",
    "            model.add(Dense(3, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "            history = model.fit(self.x_train, self.y_train, epochs=300, validation_split=0.2)                 \n",
    "            scores = model.evaluate(self.x_test, self.y_test)\n",
    "            \n",
    "            \n",
    "            # Map each loss with its respective model\n",
    "            # Note that group_models[0] = 'model object' and group_models[1] = the'history object'\n",
    "            group_models[scores[0]] = model, history\n",
    "                                    \n",
    "            pyplot.plot(history.history['loss'])\n",
    "            pyplot.plot(history.history['val_loss'])\n",
    "            pyplot.title('Loss After Each Epoch')\n",
    "            pyplot.ylabel('loss')\n",
    "            pyplot.xlabel('epoch')\n",
    "            \n",
    "            scores = model.evaluate(self.x_test, self.y_test)\n",
    "            print(\"{}: {}\".format(model.metrics_names[0], scores[0]))\n",
    "            print(\"{}: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "            base_number_layers += 1\n",
    "        \n",
    "        pyplot.legend(['2HL','val_2HL','3HL','val_3HL','4HL','val_4HL','5HL',\n",
    "                       'val_5HL','6HL','val_6HL','7HL','val_7HL','8HL','val_8HL'], loc='upper right')\n",
    "        pyplot.show()\n",
    "        \n",
    "        # sort the dictionary\n",
    "        ordered_group_models = collections.OrderedDict(sorted(group_models.items()))\n",
    "        \n",
    "        \n",
    "        # Use models starting from the lowest loss. If it is overfitted, take the second lowest loss and so on.\n",
    "        # for i, v in ordered_group_models.items():\n",
    "        # overfitted = False\n",
    "        \n",
    "        overfitted = False\n",
    "        for key, value in ordered_group_models.items():\n",
    "            list_of_loss = value[1].history['loss']\n",
    "            list_of_val_loss = value[1].history['val_loss']\n",
    "            print(list_of_loss)\n",
    "            print(list_of_val_loss)\n",
    "            \n",
    "            for i in range(len(list_of_loss)):\n",
    "                if list_of_loss[i] < list_of_val_loss[i]:\n",
    "                    overfitted = True\n",
    "                    break # the model is overfitted\n",
    "                    \n",
    "            if not overfitted:\n",
    "                optimized_model = value[0]\n",
    "                optimized_history = value[1]\n",
    "                break # exit loop because we have found optimized model\n",
    "                \n",
    "            overfitted = False\n",
    "            \n",
    "                \n",
    "        # Evaluating the optimized model\n",
    "        scores = optimized_model.evaluate(self.x_test, self.y_test)\n",
    "        print(\"{}: {}\".format(optimized_model.metrics_names[0], scores[0]))\n",
    "        print(\"{}: {}%\".format(optimized_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "                \n",
    "        pyplot.plot(optimized_history.history['loss'])\n",
    "        pyplot.plot(optimized_history.history['val_loss'])\n",
    "        pyplot.title('Loss and Val_Loss vs Epoch')\n",
    "        pyplot.ylabel('loss')\n",
    "        pyplot.xlabel('epoch')\n",
    "        pyplot.legend(['Loss','Val_Loss'], loc='upper right')\n",
    "        pyplot.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 75\n",
      "Trainable params: 75\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/300\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 0.5195 - acc: 0.3438 - val_loss: 0.5291 - val_acc: 0.2500\n",
      "Epoch 2/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.5185 - acc: 0.3438 - val_loss: 0.5282 - val_acc: 0.2500\n",
      "Epoch 3/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.5175 - acc: 0.3438 - val_loss: 0.5274 - val_acc: 0.2500\n",
      "Epoch 4/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.5166 - acc: 0.3438 - val_loss: 0.5265 - val_acc: 0.2500\n",
      "Epoch 5/300\n",
      "96/96 [==============================] - 0s 93us/step - loss: 0.5156 - acc: 0.3438 - val_loss: 0.5257 - val_acc: 0.2500\n",
      "Epoch 6/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.5146 - acc: 0.3438 - val_loss: 0.5248 - val_acc: 0.2500\n",
      "Epoch 7/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.5137 - acc: 0.3438 - val_loss: 0.5240 - val_acc: 0.2500\n",
      "Epoch 8/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.5127 - acc: 0.3438 - val_loss: 0.5232 - val_acc: 0.2500\n",
      "Epoch 9/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.5117 - acc: 0.3438 - val_loss: 0.5223 - val_acc: 0.2500\n",
      "Epoch 10/300\n",
      "96/96 [==============================] - 0s 140us/step - loss: 0.5108 - acc: 0.3438 - val_loss: 0.5215 - val_acc: 0.2500\n",
      "Epoch 11/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.5098 - acc: 0.3438 - val_loss: 0.5206 - val_acc: 0.2500\n",
      "Epoch 12/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.5088 - acc: 0.3438 - val_loss: 0.5198 - val_acc: 0.2500\n",
      "Epoch 13/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.5079 - acc: 0.3438 - val_loss: 0.5189 - val_acc: 0.2500\n",
      "Epoch 14/300\n",
      "96/96 [==============================] - 0s 92us/step - loss: 0.5070 - acc: 0.3438 - val_loss: 0.5181 - val_acc: 0.2500\n",
      "Epoch 15/300\n",
      "96/96 [==============================] - 0s 95us/step - loss: 0.5060 - acc: 0.3438 - val_loss: 0.5173 - val_acc: 0.2500\n",
      "Epoch 16/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.5050 - acc: 0.3438 - val_loss: 0.5164 - val_acc: 0.2500\n",
      "Epoch 17/300\n",
      "96/96 [==============================] - 0s 91us/step - loss: 0.5041 - acc: 0.3438 - val_loss: 0.5156 - val_acc: 0.2500\n",
      "Epoch 18/300\n",
      "96/96 [==============================] - 0s 90us/step - loss: 0.5032 - acc: 0.3438 - val_loss: 0.5148 - val_acc: 0.2500\n",
      "Epoch 19/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.5022 - acc: 0.3438 - val_loss: 0.5140 - val_acc: 0.2500\n",
      "Epoch 20/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.5013 - acc: 0.3438 - val_loss: 0.5131 - val_acc: 0.2500\n",
      "Epoch 21/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.5004 - acc: 0.3438 - val_loss: 0.5123 - val_acc: 0.2500\n",
      "Epoch 22/300\n",
      "96/96 [==============================] - 0s 93us/step - loss: 0.4994 - acc: 0.3438 - val_loss: 0.5115 - val_acc: 0.2500\n",
      "Epoch 23/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4985 - acc: 0.3438 - val_loss: 0.5107 - val_acc: 0.2500\n",
      "Epoch 24/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.4976 - acc: 0.3438 - val_loss: 0.5099 - val_acc: 0.2500\n",
      "Epoch 25/300\n",
      "96/96 [==============================] - 0s 89us/step - loss: 0.4967 - acc: 0.3438 - val_loss: 0.5091 - val_acc: 0.2500\n",
      "Epoch 26/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4958 - acc: 0.3438 - val_loss: 0.5083 - val_acc: 0.2500\n",
      "Epoch 27/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4948 - acc: 0.3438 - val_loss: 0.5074 - val_acc: 0.2500\n",
      "Epoch 28/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.4939 - acc: 0.3438 - val_loss: 0.5066 - val_acc: 0.2500\n",
      "Epoch 29/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4931 - acc: 0.3438 - val_loss: 0.5059 - val_acc: 0.2500\n",
      "Epoch 30/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4921 - acc: 0.3438 - val_loss: 0.5050 - val_acc: 0.2500\n",
      "Epoch 31/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4912 - acc: 0.3438 - val_loss: 0.5042 - val_acc: 0.2500\n",
      "Epoch 32/300\n",
      "96/96 [==============================] - 0s 90us/step - loss: 0.4903 - acc: 0.3438 - val_loss: 0.5034 - val_acc: 0.2500\n",
      "Epoch 33/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.4894 - acc: 0.3438 - val_loss: 0.5027 - val_acc: 0.2500\n",
      "Epoch 34/300\n",
      "96/96 [==============================] - 0s 93us/step - loss: 0.4885 - acc: 0.3438 - val_loss: 0.5019 - val_acc: 0.2500\n",
      "Epoch 35/300\n",
      "96/96 [==============================] - 0s 92us/step - loss: 0.4876 - acc: 0.3438 - val_loss: 0.5011 - val_acc: 0.2500\n",
      "Epoch 36/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.4867 - acc: 0.3438 - val_loss: 0.5003 - val_acc: 0.2500\n",
      "Epoch 37/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.4858 - acc: 0.3438 - val_loss: 0.4995 - val_acc: 0.2500\n",
      "Epoch 38/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.4849 - acc: 0.3438 - val_loss: 0.4987 - val_acc: 0.2500\n",
      "Epoch 39/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4841 - acc: 0.3438 - val_loss: 0.4979 - val_acc: 0.2500\n",
      "Epoch 40/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4832 - acc: 0.3438 - val_loss: 0.4972 - val_acc: 0.2500\n",
      "Epoch 41/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4823 - acc: 0.3438 - val_loss: 0.4964 - val_acc: 0.2500\n",
      "Epoch 42/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4814 - acc: 0.3438 - val_loss: 0.4956 - val_acc: 0.2500\n",
      "Epoch 43/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4805 - acc: 0.3438 - val_loss: 0.4948 - val_acc: 0.2500\n",
      "Epoch 44/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4797 - acc: 0.3438 - val_loss: 0.4941 - val_acc: 0.2500\n",
      "Epoch 45/300\n",
      "96/96 [==============================] - 0s 96us/step - loss: 0.4788 - acc: 0.3438 - val_loss: 0.4933 - val_acc: 0.2500\n",
      "Epoch 46/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4779 - acc: 0.3438 - val_loss: 0.4925 - val_acc: 0.2500\n",
      "Epoch 47/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4771 - acc: 0.3438 - val_loss: 0.4918 - val_acc: 0.2500\n",
      "Epoch 48/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4762 - acc: 0.3438 - val_loss: 0.4910 - val_acc: 0.2500\n",
      "Epoch 49/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4754 - acc: 0.3438 - val_loss: 0.4903 - val_acc: 0.2500\n",
      "Epoch 50/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4745 - acc: 0.3438 - val_loss: 0.4895 - val_acc: 0.2500\n",
      "Epoch 51/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4737 - acc: 0.3438 - val_loss: 0.4888 - val_acc: 0.2500\n",
      "Epoch 52/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4729 - acc: 0.3438 - val_loss: 0.4880 - val_acc: 0.2500\n",
      "Epoch 53/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4720 - acc: 0.3438 - val_loss: 0.4873 - val_acc: 0.2500\n",
      "Epoch 54/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4712 - acc: 0.3438 - val_loss: 0.4866 - val_acc: 0.2500\n",
      "Epoch 55/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 124us/step - loss: 0.4704 - acc: 0.3438 - val_loss: 0.4858 - val_acc: 0.2500\n",
      "Epoch 56/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4695 - acc: 0.3438 - val_loss: 0.4851 - val_acc: 0.2500\n",
      "Epoch 57/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4688 - acc: 0.3438 - val_loss: 0.4844 - val_acc: 0.2500\n",
      "Epoch 58/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4680 - acc: 0.3438 - val_loss: 0.4837 - val_acc: 0.2500\n",
      "Epoch 59/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4671 - acc: 0.3438 - val_loss: 0.4830 - val_acc: 0.2500\n",
      "Epoch 60/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.4664 - acc: 0.3438 - val_loss: 0.4823 - val_acc: 0.2500\n",
      "Epoch 61/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4656 - acc: 0.3438 - val_loss: 0.4816 - val_acc: 0.2500\n",
      "Epoch 62/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4648 - acc: 0.3438 - val_loss: 0.4809 - val_acc: 0.2500\n",
      "Epoch 63/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.4640 - acc: 0.3438 - val_loss: 0.4802 - val_acc: 0.2500\n",
      "Epoch 64/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4633 - acc: 0.3438 - val_loss: 0.4795 - val_acc: 0.2500\n",
      "Epoch 65/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4625 - acc: 0.3438 - val_loss: 0.4788 - val_acc: 0.2500\n",
      "Epoch 66/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.4617 - acc: 0.3438 - val_loss: 0.4781 - val_acc: 0.2500\n",
      "Epoch 67/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4610 - acc: 0.3438 - val_loss: 0.4774 - val_acc: 0.2500\n",
      "Epoch 68/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4603 - acc: 0.3438 - val_loss: 0.4768 - val_acc: 0.2500\n",
      "Epoch 69/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4595 - acc: 0.3438 - val_loss: 0.4761 - val_acc: 0.2500\n",
      "Epoch 70/300\n",
      "96/96 [==============================] - 0s 140us/step - loss: 0.4588 - acc: 0.3438 - val_loss: 0.4754 - val_acc: 0.2500\n",
      "Epoch 71/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4581 - acc: 0.3438 - val_loss: 0.4748 - val_acc: 0.2500\n",
      "Epoch 72/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.4574 - acc: 0.3438 - val_loss: 0.4741 - val_acc: 0.2500\n",
      "Epoch 73/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4566 - acc: 0.3438 - val_loss: 0.4735 - val_acc: 0.2500\n",
      "Epoch 74/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.4559 - acc: 0.3438 - val_loss: 0.4728 - val_acc: 0.2500\n",
      "Epoch 75/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4552 - acc: 0.3438 - val_loss: 0.4721 - val_acc: 0.2500\n",
      "Epoch 76/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.4545 - acc: 0.3438 - val_loss: 0.4715 - val_acc: 0.2500\n",
      "Epoch 77/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4538 - acc: 0.3438 - val_loss: 0.4709 - val_acc: 0.2500\n",
      "Epoch 78/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4531 - acc: 0.3438 - val_loss: 0.4702 - val_acc: 0.2500\n",
      "Epoch 79/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.4525 - acc: 0.3438 - val_loss: 0.4696 - val_acc: 0.2500\n",
      "Epoch 80/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.4518 - acc: 0.3438 - val_loss: 0.4689 - val_acc: 0.2500\n",
      "Epoch 81/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4511 - acc: 0.3438 - val_loss: 0.4683 - val_acc: 0.2500\n",
      "Epoch 82/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.4504 - acc: 0.3438 - val_loss: 0.4677 - val_acc: 0.2500\n",
      "Epoch 83/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.4497 - acc: 0.3438 - val_loss: 0.4670 - val_acc: 0.2500\n",
      "Epoch 84/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4491 - acc: 0.3438 - val_loss: 0.4664 - val_acc: 0.2500\n",
      "Epoch 85/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4484 - acc: 0.3438 - val_loss: 0.4658 - val_acc: 0.2500\n",
      "Epoch 86/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.4478 - acc: 0.3438 - val_loss: 0.4652 - val_acc: 0.2500\n",
      "Epoch 87/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.4471 - acc: 0.3438 - val_loss: 0.4645 - val_acc: 0.2500\n",
      "Epoch 88/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.4465 - acc: 0.3438 - val_loss: 0.4639 - val_acc: 0.2500\n",
      "Epoch 89/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4458 - acc: 0.3438 - val_loss: 0.4633 - val_acc: 0.2500\n",
      "Epoch 90/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.4452 - acc: 0.3438 - val_loss: 0.4627 - val_acc: 0.2500\n",
      "Epoch 91/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4445 - acc: 0.3438 - val_loss: 0.4621 - val_acc: 0.2500\n",
      "Epoch 92/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4439 - acc: 0.3438 - val_loss: 0.4615 - val_acc: 0.2500\n",
      "Epoch 93/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.4433 - acc: 0.3438 - val_loss: 0.4609 - val_acc: 0.2500\n",
      "Epoch 94/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.4427 - acc: 0.3438 - val_loss: 0.4603 - val_acc: 0.2500\n",
      "Epoch 95/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4421 - acc: 0.3438 - val_loss: 0.4597 - val_acc: 0.2500\n",
      "Epoch 96/300\n",
      "96/96 [==============================] - 0s 98us/step - loss: 0.4414 - acc: 0.3438 - val_loss: 0.4591 - val_acc: 0.2500\n",
      "Epoch 97/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4408 - acc: 0.3438 - val_loss: 0.4585 - val_acc: 0.2500\n",
      "Epoch 98/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.4402 - acc: 0.3438 - val_loss: 0.4579 - val_acc: 0.2500\n",
      "Epoch 99/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4396 - acc: 0.3438 - val_loss: 0.4573 - val_acc: 0.2500\n",
      "Epoch 100/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4390 - acc: 0.3438 - val_loss: 0.4567 - val_acc: 0.2500\n",
      "Epoch 101/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4384 - acc: 0.3438 - val_loss: 0.4561 - val_acc: 0.2500\n",
      "Epoch 102/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.4378 - acc: 0.3438 - val_loss: 0.4555 - val_acc: 0.2500\n",
      "Epoch 103/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4372 - acc: 0.3438 - val_loss: 0.4549 - val_acc: 0.2500\n",
      "Epoch 104/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.4366 - acc: 0.3438 - val_loss: 0.4543 - val_acc: 0.2500\n",
      "Epoch 105/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4360 - acc: 0.3438 - val_loss: 0.4538 - val_acc: 0.2500\n",
      "Epoch 106/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.4354 - acc: 0.3438 - val_loss: 0.4532 - val_acc: 0.2500\n",
      "Epoch 107/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4348 - acc: 0.3438 - val_loss: 0.4526 - val_acc: 0.2500\n",
      "Epoch 108/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.4342 - acc: 0.3438 - val_loss: 0.4520 - val_acc: 0.2500\n",
      "Epoch 109/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4337 - acc: 0.3438 - val_loss: 0.4514 - val_acc: 0.2500\n",
      "Epoch 110/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4331 - acc: 0.3438 - val_loss: 0.4509 - val_acc: 0.2500\n",
      "Epoch 111/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.4325 - acc: 0.3438 - val_loss: 0.4503 - val_acc: 0.2500\n",
      "Epoch 112/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.4320 - acc: 0.3438 - val_loss: 0.4497 - val_acc: 0.2500\n",
      "Epoch 113/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4314 - acc: 0.3438 - val_loss: 0.4491 - val_acc: 0.2500\n",
      "Epoch 114/300\n",
      "96/96 [==============================] - 0s 96us/step - loss: 0.4308 - acc: 0.3438 - val_loss: 0.4486 - val_acc: 0.2500\n",
      "Epoch 115/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4303 - acc: 0.3438 - val_loss: 0.4480 - val_acc: 0.2500\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 94us/step - loss: 0.4297 - acc: 0.3438 - val_loss: 0.4474 - val_acc: 0.2500\n",
      "Epoch 117/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.4291 - acc: 0.3438 - val_loss: 0.4468 - val_acc: 0.2500\n",
      "Epoch 118/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.4286 - acc: 0.3438 - val_loss: 0.4463 - val_acc: 0.2500\n",
      "Epoch 119/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.4280 - acc: 0.3438 - val_loss: 0.4457 - val_acc: 0.2500\n",
      "Epoch 120/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.4275 - acc: 0.3438 - val_loss: 0.4451 - val_acc: 0.2500\n",
      "Epoch 121/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4270 - acc: 0.3438 - val_loss: 0.4446 - val_acc: 0.2500\n",
      "Epoch 122/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.4264 - acc: 0.3438 - val_loss: 0.4440 - val_acc: 0.2500\n",
      "Epoch 123/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4259 - acc: 0.3438 - val_loss: 0.4435 - val_acc: 0.2500\n",
      "Epoch 124/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4253 - acc: 0.3438 - val_loss: 0.4429 - val_acc: 0.2500\n",
      "Epoch 125/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4248 - acc: 0.3438 - val_loss: 0.4423 - val_acc: 0.2500\n",
      "Epoch 126/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4243 - acc: 0.3438 - val_loss: 0.4418 - val_acc: 0.2500\n",
      "Epoch 127/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4237 - acc: 0.3438 - val_loss: 0.4412 - val_acc: 0.2500\n",
      "Epoch 128/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4232 - acc: 0.3438 - val_loss: 0.4407 - val_acc: 0.2500\n",
      "Epoch 129/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.4227 - acc: 0.3438 - val_loss: 0.4401 - val_acc: 0.2500\n",
      "Epoch 130/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4222 - acc: 0.3438 - val_loss: 0.4396 - val_acc: 0.2500\n",
      "Epoch 131/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4217 - acc: 0.3438 - val_loss: 0.4390 - val_acc: 0.2500\n",
      "Epoch 132/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4211 - acc: 0.3438 - val_loss: 0.4385 - val_acc: 0.2500\n",
      "Epoch 133/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.4206 - acc: 0.3438 - val_loss: 0.4379 - val_acc: 0.2500\n",
      "Epoch 134/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.4201 - acc: 0.3438 - val_loss: 0.4374 - val_acc: 0.2500\n",
      "Epoch 135/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4196 - acc: 0.3438 - val_loss: 0.4368 - val_acc: 0.2500\n",
      "Epoch 136/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4191 - acc: 0.3438 - val_loss: 0.4363 - val_acc: 0.2500\n",
      "Epoch 137/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4186 - acc: 0.3438 - val_loss: 0.4357 - val_acc: 0.2500\n",
      "Epoch 138/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4181 - acc: 0.3438 - val_loss: 0.4352 - val_acc: 0.2500\n",
      "Epoch 139/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4176 - acc: 0.3438 - val_loss: 0.4347 - val_acc: 0.2500\n",
      "Epoch 140/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4171 - acc: 0.3438 - val_loss: 0.4341 - val_acc: 0.2500\n",
      "Epoch 141/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4166 - acc: 0.3438 - val_loss: 0.4336 - val_acc: 0.2500\n",
      "Epoch 142/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.4162 - acc: 0.3438 - val_loss: 0.4331 - val_acc: 0.2500\n",
      "Epoch 143/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4156 - acc: 0.3438 - val_loss: 0.4326 - val_acc: 0.2500\n",
      "Epoch 144/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.4152 - acc: 0.3438 - val_loss: 0.4320 - val_acc: 0.2500\n",
      "Epoch 145/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4147 - acc: 0.3438 - val_loss: 0.4315 - val_acc: 0.2500\n",
      "Epoch 146/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.4142 - acc: 0.3438 - val_loss: 0.4310 - val_acc: 0.2500\n",
      "Epoch 147/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4138 - acc: 0.3438 - val_loss: 0.4305 - val_acc: 0.2500\n",
      "Epoch 148/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4133 - acc: 0.3438 - val_loss: 0.4299 - val_acc: 0.2500\n",
      "Epoch 149/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4128 - acc: 0.3438 - val_loss: 0.4294 - val_acc: 0.2500\n",
      "Epoch 150/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.4124 - acc: 0.3438 - val_loss: 0.4289 - val_acc: 0.2500\n",
      "Epoch 151/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.4119 - acc: 0.3438 - val_loss: 0.4284 - val_acc: 0.2500\n",
      "Epoch 152/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.4114 - acc: 0.3438 - val_loss: 0.4279 - val_acc: 0.2500\n",
      "Epoch 153/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4110 - acc: 0.3438 - val_loss: 0.4274 - val_acc: 0.2500\n",
      "Epoch 154/300\n",
      "96/96 [==============================] - 0s 98us/step - loss: 0.4105 - acc: 0.3438 - val_loss: 0.4269 - val_acc: 0.2500\n",
      "Epoch 155/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.4100 - acc: 0.3438 - val_loss: 0.4264 - val_acc: 0.2500\n",
      "Epoch 156/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4096 - acc: 0.3438 - val_loss: 0.4259 - val_acc: 0.2500\n",
      "Epoch 157/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.4091 - acc: 0.3438 - val_loss: 0.4254 - val_acc: 0.2500\n",
      "Epoch 158/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.4087 - acc: 0.3438 - val_loss: 0.4249 - val_acc: 0.2500\n",
      "Epoch 159/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.4082 - acc: 0.3438 - val_loss: 0.4244 - val_acc: 0.2500\n",
      "Epoch 160/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.4078 - acc: 0.3438 - val_loss: 0.4239 - val_acc: 0.2500\n",
      "Epoch 161/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4073 - acc: 0.3438 - val_loss: 0.4234 - val_acc: 0.2500\n",
      "Epoch 162/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4069 - acc: 0.3438 - val_loss: 0.4229 - val_acc: 0.2500\n",
      "Epoch 163/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.4065 - acc: 0.3438 - val_loss: 0.4224 - val_acc: 0.2500\n",
      "Epoch 164/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.4060 - acc: 0.3438 - val_loss: 0.4219 - val_acc: 0.2500\n",
      "Epoch 165/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4056 - acc: 0.3438 - val_loss: 0.4214 - val_acc: 0.2500\n",
      "Epoch 166/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4052 - acc: 0.3438 - val_loss: 0.4209 - val_acc: 0.2500\n",
      "Epoch 167/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4048 - acc: 0.3438 - val_loss: 0.4204 - val_acc: 0.2500\n",
      "Epoch 168/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.4044 - acc: 0.3438 - val_loss: 0.4199 - val_acc: 0.2500\n",
      "Epoch 169/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.4039 - acc: 0.3438 - val_loss: 0.4195 - val_acc: 0.2500\n",
      "Epoch 170/300\n",
      "96/96 [==============================] - 0s 154us/step - loss: 0.4035 - acc: 0.3438 - val_loss: 0.4190 - val_acc: 0.2500\n",
      "Epoch 171/300\n",
      "96/96 [==============================] - 0s 156us/step - loss: 0.4031 - acc: 0.3438 - val_loss: 0.4185 - val_acc: 0.2500\n",
      "Epoch 172/300\n",
      "96/96 [==============================] - 0s 146us/step - loss: 0.4027 - acc: 0.3438 - val_loss: 0.4180 - val_acc: 0.2500\n",
      "Epoch 173/300\n",
      "96/96 [==============================] - 0s 152us/step - loss: 0.4023 - acc: 0.3438 - val_loss: 0.4175 - val_acc: 0.2500\n",
      "Epoch 174/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.4019 - acc: 0.3438 - val_loss: 0.4170 - val_acc: 0.2500\n",
      "Epoch 175/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.4015 - acc: 0.3438 - val_loss: 0.4166 - val_acc: 0.2500\n",
      "Epoch 176/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4010 - acc: 0.3438 - val_loss: 0.4161 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/300\n",
      "96/96 [==============================] - 0s 146us/step - loss: 0.4007 - acc: 0.3438 - val_loss: 0.4157 - val_acc: 0.2500\n",
      "Epoch 178/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.4002 - acc: 0.3438 - val_loss: 0.4152 - val_acc: 0.2500\n",
      "Epoch 179/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.3999 - acc: 0.3438 - val_loss: 0.4147 - val_acc: 0.2500\n",
      "Epoch 180/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3995 - acc: 0.3438 - val_loss: 0.4143 - val_acc: 0.2500\n",
      "Epoch 181/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3991 - acc: 0.3438 - val_loss: 0.4138 - val_acc: 0.2500\n",
      "Epoch 182/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3987 - acc: 0.3438 - val_loss: 0.4134 - val_acc: 0.2500\n",
      "Epoch 183/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3983 - acc: 0.3438 - val_loss: 0.4129 - val_acc: 0.2500\n",
      "Epoch 184/300\n",
      "96/96 [==============================] - 0s 146us/step - loss: 0.3979 - acc: 0.3438 - val_loss: 0.4125 - val_acc: 0.2500\n",
      "Epoch 185/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3975 - acc: 0.3438 - val_loss: 0.4120 - val_acc: 0.2500\n",
      "Epoch 186/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3972 - acc: 0.3438 - val_loss: 0.4116 - val_acc: 0.2500\n",
      "Epoch 187/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3968 - acc: 0.3438 - val_loss: 0.4111 - val_acc: 0.2500\n",
      "Epoch 188/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.3964 - acc: 0.3438 - val_loss: 0.4107 - val_acc: 0.2500\n",
      "Epoch 189/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3960 - acc: 0.3438 - val_loss: 0.4103 - val_acc: 0.2500\n",
      "Epoch 190/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.3957 - acc: 0.3438 - val_loss: 0.4098 - val_acc: 0.2500\n",
      "Epoch 191/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3953 - acc: 0.3438 - val_loss: 0.4094 - val_acc: 0.2500\n",
      "Epoch 192/300\n",
      "96/96 [==============================] - 0s 102us/step - loss: 0.3949 - acc: 0.3438 - val_loss: 0.4089 - val_acc: 0.2500\n",
      "Epoch 193/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3946 - acc: 0.3438 - val_loss: 0.4085 - val_acc: 0.2500\n",
      "Epoch 194/300\n",
      "96/96 [==============================] - 0s 98us/step - loss: 0.3942 - acc: 0.3438 - val_loss: 0.4081 - val_acc: 0.2500\n",
      "Epoch 195/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3938 - acc: 0.3438 - val_loss: 0.4077 - val_acc: 0.2500\n",
      "Epoch 196/300\n",
      "96/96 [==============================] - 0s 92us/step - loss: 0.3935 - acc: 0.3438 - val_loss: 0.4072 - val_acc: 0.2500\n",
      "Epoch 197/300\n",
      "96/96 [==============================] - 0s 98us/step - loss: 0.3931 - acc: 0.3438 - val_loss: 0.4068 - val_acc: 0.2500\n",
      "Epoch 198/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3928 - acc: 0.3438 - val_loss: 0.4064 - val_acc: 0.2500\n",
      "Epoch 199/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3924 - acc: 0.3438 - val_loss: 0.4060 - val_acc: 0.2500\n",
      "Epoch 200/300\n",
      "96/96 [==============================] - 0s 103us/step - loss: 0.3921 - acc: 0.3438 - val_loss: 0.4056 - val_acc: 0.2500\n",
      "Epoch 201/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3917 - acc: 0.3438 - val_loss: 0.4051 - val_acc: 0.2500\n",
      "Epoch 202/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3914 - acc: 0.3438 - val_loss: 0.4047 - val_acc: 0.2500\n",
      "Epoch 203/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3910 - acc: 0.3438 - val_loss: 0.4043 - val_acc: 0.2500\n",
      "Epoch 204/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3907 - acc: 0.3438 - val_loss: 0.4039 - val_acc: 0.2500\n",
      "Epoch 205/300\n",
      "96/96 [==============================] - 0s 96us/step - loss: 0.3904 - acc: 0.3438 - val_loss: 0.4035 - val_acc: 0.2500\n",
      "Epoch 206/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.3900 - acc: 0.3438 - val_loss: 0.4031 - val_acc: 0.2500\n",
      "Epoch 207/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.3897 - acc: 0.3438 - val_loss: 0.4027 - val_acc: 0.2500\n",
      "Epoch 208/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3894 - acc: 0.3438 - val_loss: 0.4023 - val_acc: 0.2500\n",
      "Epoch 209/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3891 - acc: 0.3438 - val_loss: 0.4019 - val_acc: 0.2500\n",
      "Epoch 210/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.3887 - acc: 0.3438 - val_loss: 0.4015 - val_acc: 0.2500\n",
      "Epoch 211/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3884 - acc: 0.3438 - val_loss: 0.4011 - val_acc: 0.2500\n",
      "Epoch 212/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.3881 - acc: 0.3438 - val_loss: 0.4008 - val_acc: 0.2500\n",
      "Epoch 213/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3877 - acc: 0.3438 - val_loss: 0.4004 - val_acc: 0.2500\n",
      "Epoch 214/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3874 - acc: 0.3438 - val_loss: 0.4000 - val_acc: 0.2500\n",
      "Epoch 215/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.3871 - acc: 0.3438 - val_loss: 0.3996 - val_acc: 0.2500\n",
      "Epoch 216/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3868 - acc: 0.3438 - val_loss: 0.3992 - val_acc: 0.2500\n",
      "Epoch 217/300\n",
      "96/96 [==============================] - 0s 94us/step - loss: 0.3865 - acc: 0.3438 - val_loss: 0.3988 - val_acc: 0.2500\n",
      "Epoch 218/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3862 - acc: 0.3438 - val_loss: 0.3985 - val_acc: 0.2500\n",
      "Epoch 219/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3859 - acc: 0.3438 - val_loss: 0.3981 - val_acc: 0.2500\n",
      "Epoch 220/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3856 - acc: 0.3438 - val_loss: 0.3977 - val_acc: 0.2500\n",
      "Epoch 221/300\n",
      "96/96 [==============================] - 0s 97us/step - loss: 0.3852 - acc: 0.3438 - val_loss: 0.3973 - val_acc: 0.2500\n",
      "Epoch 222/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3850 - acc: 0.3438 - val_loss: 0.3970 - val_acc: 0.2500\n",
      "Epoch 223/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3847 - acc: 0.3438 - val_loss: 0.3966 - val_acc: 0.2500\n",
      "Epoch 224/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3844 - acc: 0.3438 - val_loss: 0.3962 - val_acc: 0.2500\n",
      "Epoch 225/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3841 - acc: 0.3438 - val_loss: 0.3959 - val_acc: 0.2500\n",
      "Epoch 226/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3838 - acc: 0.3438 - val_loss: 0.3955 - val_acc: 0.2500\n",
      "Epoch 227/300\n",
      "96/96 [==============================] - 0s 98us/step - loss: 0.3835 - acc: 0.3438 - val_loss: 0.3952 - val_acc: 0.2500\n",
      "Epoch 228/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3832 - acc: 0.3438 - val_loss: 0.3948 - val_acc: 0.2500\n",
      "Epoch 229/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3829 - acc: 0.3438 - val_loss: 0.3945 - val_acc: 0.2500\n",
      "Epoch 230/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3826 - acc: 0.3438 - val_loss: 0.3941 - val_acc: 0.2500\n",
      "Epoch 231/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3823 - acc: 0.3438 - val_loss: 0.3938 - val_acc: 0.2500\n",
      "Epoch 232/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3820 - acc: 0.3438 - val_loss: 0.3934 - val_acc: 0.2500\n",
      "Epoch 233/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3818 - acc: 0.3438 - val_loss: 0.3931 - val_acc: 0.2500\n",
      "Epoch 234/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3815 - acc: 0.3438 - val_loss: 0.3927 - val_acc: 0.2500\n",
      "Epoch 235/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3812 - acc: 0.3438 - val_loss: 0.3924 - val_acc: 0.2500\n",
      "Epoch 236/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3810 - acc: 0.3438 - val_loss: 0.3920 - val_acc: 0.2500\n",
      "Epoch 237/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3807 - acc: 0.3438 - val_loss: 0.3917 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/300\n",
      "96/96 [==============================] - 0s 96us/step - loss: 0.3804 - acc: 0.3438 - val_loss: 0.3914 - val_acc: 0.2500\n",
      "Epoch 239/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3801 - acc: 0.3438 - val_loss: 0.3910 - val_acc: 0.2500\n",
      "Epoch 240/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3799 - acc: 0.3438 - val_loss: 0.3907 - val_acc: 0.2500\n",
      "Epoch 241/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3796 - acc: 0.3438 - val_loss: 0.3904 - val_acc: 0.2500\n",
      "Epoch 242/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3794 - acc: 0.3438 - val_loss: 0.3901 - val_acc: 0.2500\n",
      "Epoch 243/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3791 - acc: 0.3438 - val_loss: 0.3897 - val_acc: 0.2500\n",
      "Epoch 244/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3788 - acc: 0.3438 - val_loss: 0.3894 - val_acc: 0.2500\n",
      "Epoch 245/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.3786 - acc: 0.3438 - val_loss: 0.3891 - val_acc: 0.2500\n",
      "Epoch 246/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3783 - acc: 0.3438 - val_loss: 0.3888 - val_acc: 0.2500\n",
      "Epoch 247/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.3781 - acc: 0.3438 - val_loss: 0.3885 - val_acc: 0.2500\n",
      "Epoch 248/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3778 - acc: 0.3438 - val_loss: 0.3882 - val_acc: 0.2500\n",
      "Epoch 249/300\n",
      "96/96 [==============================] - 0s 109us/step - loss: 0.3776 - acc: 0.3438 - val_loss: 0.3879 - val_acc: 0.2500\n",
      "Epoch 250/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3773 - acc: 0.3438 - val_loss: 0.3875 - val_acc: 0.2500\n",
      "Epoch 251/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3771 - acc: 0.3438 - val_loss: 0.3872 - val_acc: 0.2500\n",
      "Epoch 252/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3768 - acc: 0.3438 - val_loss: 0.3869 - val_acc: 0.2500\n",
      "Epoch 253/300\n",
      "96/96 [==============================] - 0s 99us/step - loss: 0.3766 - acc: 0.3438 - val_loss: 0.3866 - val_acc: 0.2500\n",
      "Epoch 254/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.3763 - acc: 0.3438 - val_loss: 0.3863 - val_acc: 0.2500\n",
      "Epoch 255/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3761 - acc: 0.3438 - val_loss: 0.3860 - val_acc: 0.2500\n",
      "Epoch 256/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3759 - acc: 0.3438 - val_loss: 0.3857 - val_acc: 0.2500\n",
      "Epoch 257/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.3756 - acc: 0.3438 - val_loss: 0.3855 - val_acc: 0.2500\n",
      "Epoch 258/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3754 - acc: 0.3438 - val_loss: 0.3851 - val_acc: 0.2500\n",
      "Epoch 259/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3752 - acc: 0.3438 - val_loss: 0.3849 - val_acc: 0.2500\n",
      "Epoch 260/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3749 - acc: 0.3438 - val_loss: 0.3846 - val_acc: 0.2500\n",
      "Epoch 261/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3747 - acc: 0.3438 - val_loss: 0.3843 - val_acc: 0.2500\n",
      "Epoch 262/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3745 - acc: 0.3438 - val_loss: 0.3840 - val_acc: 0.2500\n",
      "Epoch 263/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3742 - acc: 0.3438 - val_loss: 0.3837 - val_acc: 0.2500\n",
      "Epoch 264/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3740 - acc: 0.3438 - val_loss: 0.3835 - val_acc: 0.2500\n",
      "Epoch 265/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3738 - acc: 0.3438 - val_loss: 0.3832 - val_acc: 0.2500\n",
      "Epoch 266/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3736 - acc: 0.3438 - val_loss: 0.3829 - val_acc: 0.2500\n",
      "Epoch 267/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.3733 - acc: 0.3438 - val_loss: 0.3826 - val_acc: 0.2500\n",
      "Epoch 268/300\n",
      "96/96 [==============================] - 0s 104us/step - loss: 0.3731 - acc: 0.3438 - val_loss: 0.3824 - val_acc: 0.2500\n",
      "Epoch 269/300\n",
      "96/96 [==============================] - 0s 100us/step - loss: 0.3729 - acc: 0.3438 - val_loss: 0.3821 - val_acc: 0.2500\n",
      "Epoch 270/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3727 - acc: 0.3438 - val_loss: 0.3818 - val_acc: 0.2500\n",
      "Epoch 271/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3725 - acc: 0.3438 - val_loss: 0.3815 - val_acc: 0.2500\n",
      "Epoch 272/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3723 - acc: 0.3438 - val_loss: 0.3813 - val_acc: 0.2500\n",
      "Epoch 273/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.3721 - acc: 0.3438 - val_loss: 0.3810 - val_acc: 0.2500\n",
      "Epoch 274/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3718 - acc: 0.3438 - val_loss: 0.3808 - val_acc: 0.2500\n",
      "Epoch 275/300\n",
      "96/96 [==============================] - 0s 96us/step - loss: 0.3717 - acc: 0.3438 - val_loss: 0.3805 - val_acc: 0.2500\n",
      "Epoch 276/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3714 - acc: 0.3438 - val_loss: 0.3802 - val_acc: 0.2500\n",
      "Epoch 277/300\n",
      "96/96 [==============================] - 0s 105us/step - loss: 0.3712 - acc: 0.3438 - val_loss: 0.3800 - val_acc: 0.2500\n",
      "Epoch 278/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3710 - acc: 0.3438 - val_loss: 0.3797 - val_acc: 0.2500\n",
      "Epoch 279/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.3708 - acc: 0.3438 - val_loss: 0.3795 - val_acc: 0.2500\n",
      "Epoch 280/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3706 - acc: 0.3438 - val_loss: 0.3792 - val_acc: 0.2500\n",
      "Epoch 281/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3704 - acc: 0.3438 - val_loss: 0.3790 - val_acc: 0.2500\n",
      "Epoch 282/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3702 - acc: 0.3438 - val_loss: 0.3787 - val_acc: 0.2500\n",
      "Epoch 283/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3700 - acc: 0.3438 - val_loss: 0.3785 - val_acc: 0.2500\n",
      "Epoch 284/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.3698 - acc: 0.3438 - val_loss: 0.3782 - val_acc: 0.2500\n",
      "Epoch 285/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3697 - acc: 0.3438 - val_loss: 0.3780 - val_acc: 0.2500\n",
      "Epoch 286/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3695 - acc: 0.3438 - val_loss: 0.3777 - val_acc: 0.2500\n",
      "Epoch 287/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.3693 - acc: 0.3438 - val_loss: 0.3775 - val_acc: 0.2500\n",
      "Epoch 288/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3691 - acc: 0.3438 - val_loss: 0.3773 - val_acc: 0.2500\n",
      "Epoch 289/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3689 - acc: 0.3438 - val_loss: 0.3770 - val_acc: 0.2500\n",
      "Epoch 290/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3687 - acc: 0.3438 - val_loss: 0.3768 - val_acc: 0.2500\n",
      "Epoch 291/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3685 - acc: 0.3438 - val_loss: 0.3766 - val_acc: 0.2500\n",
      "Epoch 292/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3683 - acc: 0.3438 - val_loss: 0.3763 - val_acc: 0.2500\n",
      "Epoch 293/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3682 - acc: 0.3438 - val_loss: 0.3761 - val_acc: 0.2500\n",
      "Epoch 294/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3680 - acc: 0.3438 - val_loss: 0.3759 - val_acc: 0.2500\n",
      "Epoch 295/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3678 - acc: 0.3438 - val_loss: 0.3756 - val_acc: 0.2500\n",
      "Epoch 296/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3676 - acc: 0.3438 - val_loss: 0.3754 - val_acc: 0.2500\n",
      "Epoch 297/300\n",
      "96/96 [==============================] - 0s 140us/step - loss: 0.3674 - acc: 0.3438 - val_loss: 0.3752 - val_acc: 0.2500\n",
      "Epoch 298/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.3673 - acc: 0.3438 - val_loss: 0.3750 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.3671 - acc: 0.3438 - val_loss: 0.3748 - val_acc: 0.2500\n",
      "Epoch 300/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3669 - acc: 0.3438 - val_loss: 0.3745 - val_acc: 0.2500\n",
      "30/30 [==============================] - 0s 30us/step\n",
      "30/30 [==============================] - 0s 28us/step\n",
      "loss: 0.3643434941768646\n",
      "acc: 36.666667461395264%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 95\n",
      "Trainable params: 95\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/300\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 0.5152 - acc: 0.3125 - val_loss: 0.5074 - val_acc: 0.4167\n",
      "Epoch 2/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.5144 - acc: 0.3125 - val_loss: 0.5066 - val_acc: 0.4167\n",
      "Epoch 3/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.5136 - acc: 0.3125 - val_loss: 0.5057 - val_acc: 0.4167\n",
      "Epoch 4/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.5128 - acc: 0.3125 - val_loss: 0.5048 - val_acc: 0.4167\n",
      "Epoch 5/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.5120 - acc: 0.3125 - val_loss: 0.5040 - val_acc: 0.4167\n",
      "Epoch 6/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.5112 - acc: 0.3125 - val_loss: 0.5031 - val_acc: 0.4167\n",
      "Epoch 7/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.5104 - acc: 0.3125 - val_loss: 0.5023 - val_acc: 0.4167\n",
      "Epoch 8/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.5096 - acc: 0.3125 - val_loss: 0.5014 - val_acc: 0.4167\n",
      "Epoch 9/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.5088 - acc: 0.3125 - val_loss: 0.5006 - val_acc: 0.4167\n",
      "Epoch 10/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.5080 - acc: 0.3125 - val_loss: 0.4998 - val_acc: 0.4167\n",
      "Epoch 11/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.5072 - acc: 0.3125 - val_loss: 0.4990 - val_acc: 0.4167\n",
      "Epoch 12/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.5064 - acc: 0.3125 - val_loss: 0.4981 - val_acc: 0.4167\n",
      "Epoch 13/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.5056 - acc: 0.3125 - val_loss: 0.4973 - val_acc: 0.4167\n",
      "Epoch 14/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.5048 - acc: 0.3125 - val_loss: 0.4965 - val_acc: 0.4167\n",
      "Epoch 15/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.5040 - acc: 0.3125 - val_loss: 0.4956 - val_acc: 0.4167\n",
      "Epoch 16/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.5033 - acc: 0.3125 - val_loss: 0.4948 - val_acc: 0.4167\n",
      "Epoch 17/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.5025 - acc: 0.3125 - val_loss: 0.4939 - val_acc: 0.4167\n",
      "Epoch 18/300\n",
      "96/96 [==============================] - 0s 110us/step - loss: 0.5017 - acc: 0.3125 - val_loss: 0.4931 - val_acc: 0.4167\n",
      "Epoch 19/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.5009 - acc: 0.3125 - val_loss: 0.4923 - val_acc: 0.4167\n",
      "Epoch 20/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.5001 - acc: 0.3125 - val_loss: 0.4914 - val_acc: 0.4167\n",
      "Epoch 21/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4993 - acc: 0.3125 - val_loss: 0.4906 - val_acc: 0.4167\n",
      "Epoch 22/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4985 - acc: 0.3125 - val_loss: 0.4897 - val_acc: 0.4167\n",
      "Epoch 23/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4977 - acc: 0.3125 - val_loss: 0.4889 - val_acc: 0.4167\n",
      "Epoch 24/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.4969 - acc: 0.3125 - val_loss: 0.4880 - val_acc: 0.4167\n",
      "Epoch 25/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.4961 - acc: 0.3125 - val_loss: 0.4872 - val_acc: 0.4167\n",
      "Epoch 26/300\n",
      "96/96 [==============================] - 0s 145us/step - loss: 0.4953 - acc: 0.3125 - val_loss: 0.4863 - val_acc: 0.4167\n",
      "Epoch 27/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4944 - acc: 0.3125 - val_loss: 0.4855 - val_acc: 0.4167\n",
      "Epoch 28/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.4936 - acc: 0.3125 - val_loss: 0.4846 - val_acc: 0.4167\n",
      "Epoch 29/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.4928 - acc: 0.3125 - val_loss: 0.4838 - val_acc: 0.4167\n",
      "Epoch 30/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4920 - acc: 0.3125 - val_loss: 0.4829 - val_acc: 0.4167\n",
      "Epoch 31/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4912 - acc: 0.3125 - val_loss: 0.4821 - val_acc: 0.4167\n",
      "Epoch 32/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4904 - acc: 0.3125 - val_loss: 0.4812 - val_acc: 0.4167\n",
      "Epoch 33/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4896 - acc: 0.3125 - val_loss: 0.4804 - val_acc: 0.4167\n",
      "Epoch 34/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4888 - acc: 0.3125 - val_loss: 0.4796 - val_acc: 0.4167\n",
      "Epoch 35/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4880 - acc: 0.3125 - val_loss: 0.4787 - val_acc: 0.4167\n",
      "Epoch 36/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.4872 - acc: 0.3125 - val_loss: 0.4779 - val_acc: 0.4167\n",
      "Epoch 37/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4863 - acc: 0.3125 - val_loss: 0.4770 - val_acc: 0.4167\n",
      "Epoch 38/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4855 - acc: 0.3125 - val_loss: 0.4762 - val_acc: 0.4167\n",
      "Epoch 39/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4847 - acc: 0.3125 - val_loss: 0.4753 - val_acc: 0.4167\n",
      "Epoch 40/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4839 - acc: 0.3125 - val_loss: 0.4745 - val_acc: 0.4167\n",
      "Epoch 41/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4831 - acc: 0.3125 - val_loss: 0.4736 - val_acc: 0.4167\n",
      "Epoch 42/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4823 - acc: 0.3125 - val_loss: 0.4728 - val_acc: 0.4167\n",
      "Epoch 43/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4815 - acc: 0.3125 - val_loss: 0.4720 - val_acc: 0.4167\n",
      "Epoch 44/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4806 - acc: 0.3125 - val_loss: 0.4711 - val_acc: 0.4167\n",
      "Epoch 45/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.4798 - acc: 0.3125 - val_loss: 0.4703 - val_acc: 0.4167\n",
      "Epoch 46/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4790 - acc: 0.3125 - val_loss: 0.4694 - val_acc: 0.4167\n",
      "Epoch 47/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4782 - acc: 0.3125 - val_loss: 0.4686 - val_acc: 0.4167\n",
      "Epoch 48/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4774 - acc: 0.3125 - val_loss: 0.4677 - val_acc: 0.4167\n",
      "Epoch 49/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4766 - acc: 0.3125 - val_loss: 0.4669 - val_acc: 0.4167\n",
      "Epoch 50/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.4757 - acc: 0.3125 - val_loss: 0.4661 - val_acc: 0.4167\n",
      "Epoch 51/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 119us/step - loss: 0.4749 - acc: 0.3125 - val_loss: 0.4652 - val_acc: 0.4167\n",
      "Epoch 52/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4741 - acc: 0.3125 - val_loss: 0.4644 - val_acc: 0.4167\n",
      "Epoch 53/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4733 - acc: 0.3125 - val_loss: 0.4636 - val_acc: 0.4167\n",
      "Epoch 54/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4724 - acc: 0.3125 - val_loss: 0.4627 - val_acc: 0.4167\n",
      "Epoch 55/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.4716 - acc: 0.3125 - val_loss: 0.4619 - val_acc: 0.4167\n",
      "Epoch 56/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.4708 - acc: 0.3125 - val_loss: 0.4610 - val_acc: 0.4167\n",
      "Epoch 57/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4700 - acc: 0.3125 - val_loss: 0.4602 - val_acc: 0.4167\n",
      "Epoch 58/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4692 - acc: 0.3125 - val_loss: 0.4594 - val_acc: 0.4167\n",
      "Epoch 59/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4683 - acc: 0.3125 - val_loss: 0.4585 - val_acc: 0.4167\n",
      "Epoch 60/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4675 - acc: 0.3125 - val_loss: 0.4577 - val_acc: 0.4167\n",
      "Epoch 61/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4667 - acc: 0.3125 - val_loss: 0.4569 - val_acc: 0.4167\n",
      "Epoch 62/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4658 - acc: 0.3125 - val_loss: 0.4560 - val_acc: 0.4167\n",
      "Epoch 63/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4650 - acc: 0.3125 - val_loss: 0.4552 - val_acc: 0.4167\n",
      "Epoch 64/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4642 - acc: 0.3125 - val_loss: 0.4544 - val_acc: 0.4167\n",
      "Epoch 65/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4634 - acc: 0.3125 - val_loss: 0.4535 - val_acc: 0.4167\n",
      "Epoch 66/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4625 - acc: 0.3125 - val_loss: 0.4527 - val_acc: 0.4167\n",
      "Epoch 67/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4617 - acc: 0.3125 - val_loss: 0.4519 - val_acc: 0.4167\n",
      "Epoch 68/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4609 - acc: 0.3125 - val_loss: 0.4511 - val_acc: 0.4167\n",
      "Epoch 69/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4601 - acc: 0.3125 - val_loss: 0.4502 - val_acc: 0.4167\n",
      "Epoch 70/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4592 - acc: 0.3125 - val_loss: 0.4494 - val_acc: 0.4167\n",
      "Epoch 71/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4584 - acc: 0.3125 - val_loss: 0.4486 - val_acc: 0.4167\n",
      "Epoch 72/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4576 - acc: 0.3125 - val_loss: 0.4478 - val_acc: 0.4167\n",
      "Epoch 73/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4567 - acc: 0.3125 - val_loss: 0.4469 - val_acc: 0.4167\n",
      "Epoch 74/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4559 - acc: 0.3125 - val_loss: 0.4461 - val_acc: 0.4167\n",
      "Epoch 75/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.4551 - acc: 0.3125 - val_loss: 0.4453 - val_acc: 0.4167\n",
      "Epoch 76/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4543 - acc: 0.3125 - val_loss: 0.4445 - val_acc: 0.4167\n",
      "Epoch 77/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.4535 - acc: 0.3125 - val_loss: 0.4437 - val_acc: 0.4167\n",
      "Epoch 78/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4526 - acc: 0.3125 - val_loss: 0.4428 - val_acc: 0.4167\n",
      "Epoch 79/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4518 - acc: 0.3125 - val_loss: 0.4420 - val_acc: 0.4167\n",
      "Epoch 80/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.4510 - acc: 0.3125 - val_loss: 0.4412 - val_acc: 0.4167\n",
      "Epoch 81/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4502 - acc: 0.3125 - val_loss: 0.4404 - val_acc: 0.4167\n",
      "Epoch 82/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4493 - acc: 0.3125 - val_loss: 0.4396 - val_acc: 0.4167\n",
      "Epoch 83/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4485 - acc: 0.3125 - val_loss: 0.4388 - val_acc: 0.4167\n",
      "Epoch 84/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4477 - acc: 0.3125 - val_loss: 0.4380 - val_acc: 0.4167\n",
      "Epoch 85/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.4469 - acc: 0.3125 - val_loss: 0.4372 - val_acc: 0.4167\n",
      "Epoch 86/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4461 - acc: 0.3125 - val_loss: 0.4365 - val_acc: 0.4167\n",
      "Epoch 87/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4453 - acc: 0.3125 - val_loss: 0.4357 - val_acc: 0.4167\n",
      "Epoch 88/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4445 - acc: 0.3125 - val_loss: 0.4349 - val_acc: 0.4167\n",
      "Epoch 89/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.4437 - acc: 0.3125 - val_loss: 0.4341 - val_acc: 0.4167\n",
      "Epoch 90/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4429 - acc: 0.3125 - val_loss: 0.4333 - val_acc: 0.4167\n",
      "Epoch 91/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4421 - acc: 0.3125 - val_loss: 0.4325 - val_acc: 0.4167\n",
      "Epoch 92/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.4413 - acc: 0.3125 - val_loss: 0.4318 - val_acc: 0.4167\n",
      "Epoch 93/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.4405 - acc: 0.3125 - val_loss: 0.4310 - val_acc: 0.4167\n",
      "Epoch 94/300\n",
      "96/96 [==============================] - 0s 142us/step - loss: 0.4397 - acc: 0.3125 - val_loss: 0.4302 - val_acc: 0.4167\n",
      "Epoch 95/300\n",
      "96/96 [==============================] - 0s 148us/step - loss: 0.4389 - acc: 0.3125 - val_loss: 0.4295 - val_acc: 0.4167\n",
      "Epoch 96/300\n",
      "96/96 [==============================] - 0s 160us/step - loss: 0.4381 - acc: 0.3125 - val_loss: 0.4287 - val_acc: 0.4167\n",
      "Epoch 97/300\n",
      "96/96 [==============================] - 0s 154us/step - loss: 0.4373 - acc: 0.3125 - val_loss: 0.4280 - val_acc: 0.4167\n",
      "Epoch 98/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.4366 - acc: 0.3125 - val_loss: 0.4272 - val_acc: 0.4167\n",
      "Epoch 99/300\n",
      "96/96 [==============================] - 0s 149us/step - loss: 0.4358 - acc: 0.3125 - val_loss: 0.4265 - val_acc: 0.4167\n",
      "Epoch 100/300\n",
      "96/96 [==============================] - 0s 148us/step - loss: 0.4350 - acc: 0.3125 - val_loss: 0.4257 - val_acc: 0.4167\n",
      "Epoch 101/300\n",
      "96/96 [==============================] - 0s 146us/step - loss: 0.4342 - acc: 0.3125 - val_loss: 0.4250 - val_acc: 0.4167\n",
      "Epoch 102/300\n",
      "96/96 [==============================] - 0s 150us/step - loss: 0.4335 - acc: 0.3125 - val_loss: 0.4242 - val_acc: 0.4167\n",
      "Epoch 103/300\n",
      "96/96 [==============================] - 0s 153us/step - loss: 0.4327 - acc: 0.3125 - val_loss: 0.4235 - val_acc: 0.4167\n",
      "Epoch 104/300\n",
      "96/96 [==============================] - 0s 167us/step - loss: 0.4320 - acc: 0.3125 - val_loss: 0.4228 - val_acc: 0.4167\n",
      "Epoch 105/300\n",
      "96/96 [==============================] - 0s 152us/step - loss: 0.4312 - acc: 0.3125 - val_loss: 0.4221 - val_acc: 0.4167\n",
      "Epoch 106/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.4304 - acc: 0.3125 - val_loss: 0.4213 - val_acc: 0.4167\n",
      "Epoch 107/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.4297 - acc: 0.3125 - val_loss: 0.4206 - val_acc: 0.4167\n",
      "Epoch 108/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.4290 - acc: 0.3125 - val_loss: 0.4199 - val_acc: 0.4167\n",
      "Epoch 109/300\n",
      "96/96 [==============================] - 0s 144us/step - loss: 0.4282 - acc: 0.3125 - val_loss: 0.4192 - val_acc: 0.4167\n",
      "Epoch 110/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4275 - acc: 0.3125 - val_loss: 0.4185 - val_acc: 0.4167\n",
      "Epoch 111/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.4267 - acc: 0.3125 - val_loss: 0.4178 - val_acc: 0.4167\n",
      "Epoch 112/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 126us/step - loss: 0.4260 - acc: 0.3125 - val_loss: 0.4171 - val_acc: 0.4167\n",
      "Epoch 113/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4253 - acc: 0.3125 - val_loss: 0.4164 - val_acc: 0.4167\n",
      "Epoch 114/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4246 - acc: 0.3125 - val_loss: 0.4158 - val_acc: 0.4167\n",
      "Epoch 115/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4238 - acc: 0.3125 - val_loss: 0.4151 - val_acc: 0.4167\n",
      "Epoch 116/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.4231 - acc: 0.3125 - val_loss: 0.4144 - val_acc: 0.4167\n",
      "Epoch 117/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.4224 - acc: 0.3125 - val_loss: 0.4137 - val_acc: 0.4167\n",
      "Epoch 118/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.4217 - acc: 0.3125 - val_loss: 0.4131 - val_acc: 0.4167\n",
      "Epoch 119/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.4210 - acc: 0.3125 - val_loss: 0.4124 - val_acc: 0.4167\n",
      "Epoch 120/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4203 - acc: 0.3125 - val_loss: 0.4117 - val_acc: 0.4167\n",
      "Epoch 121/300\n",
      "96/96 [==============================] - 0s 143us/step - loss: 0.4196 - acc: 0.3125 - val_loss: 0.4111 - val_acc: 0.4167\n",
      "Epoch 122/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.4189 - acc: 0.3125 - val_loss: 0.4104 - val_acc: 0.4167\n",
      "Epoch 123/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.4182 - acc: 0.3125 - val_loss: 0.4098 - val_acc: 0.4167\n",
      "Epoch 124/300\n",
      "96/96 [==============================] - 0s 157us/step - loss: 0.4176 - acc: 0.3125 - val_loss: 0.4092 - val_acc: 0.4167\n",
      "Epoch 125/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.4169 - acc: 0.3125 - val_loss: 0.4085 - val_acc: 0.4167\n",
      "Epoch 126/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.4162 - acc: 0.3125 - val_loss: 0.4079 - val_acc: 0.4167\n",
      "Epoch 127/300\n",
      "96/96 [==============================] - 0s 151us/step - loss: 0.4155 - acc: 0.3125 - val_loss: 0.4073 - val_acc: 0.4167\n",
      "Epoch 128/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4149 - acc: 0.3125 - val_loss: 0.4067 - val_acc: 0.4167\n",
      "Epoch 129/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4142 - acc: 0.3125 - val_loss: 0.4061 - val_acc: 0.4167\n",
      "Epoch 130/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.4136 - acc: 0.3125 - val_loss: 0.4054 - val_acc: 0.4167\n",
      "Epoch 131/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.4129 - acc: 0.3125 - val_loss: 0.4048 - val_acc: 0.4167\n",
      "Epoch 132/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.4123 - acc: 0.3125 - val_loss: 0.4042 - val_acc: 0.4167\n",
      "Epoch 133/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4116 - acc: 0.3125 - val_loss: 0.4036 - val_acc: 0.4167\n",
      "Epoch 134/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4110 - acc: 0.3125 - val_loss: 0.4030 - val_acc: 0.4167\n",
      "Epoch 135/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4103 - acc: 0.3125 - val_loss: 0.4025 - val_acc: 0.4167\n",
      "Epoch 136/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4097 - acc: 0.3125 - val_loss: 0.4019 - val_acc: 0.4167\n",
      "Epoch 137/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.4091 - acc: 0.3125 - val_loss: 0.4013 - val_acc: 0.4167\n",
      "Epoch 138/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4085 - acc: 0.3125 - val_loss: 0.4007 - val_acc: 0.4167\n",
      "Epoch 139/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.4079 - acc: 0.3125 - val_loss: 0.4001 - val_acc: 0.4167\n",
      "Epoch 140/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4073 - acc: 0.3125 - val_loss: 0.3996 - val_acc: 0.4167\n",
      "Epoch 141/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4066 - acc: 0.3125 - val_loss: 0.3990 - val_acc: 0.4167\n",
      "Epoch 142/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4061 - acc: 0.3125 - val_loss: 0.3985 - val_acc: 0.4167\n",
      "Epoch 143/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.4054 - acc: 0.3125 - val_loss: 0.3979 - val_acc: 0.4167\n",
      "Epoch 144/300\n",
      "96/96 [==============================] - 0s 107us/step - loss: 0.4049 - acc: 0.3125 - val_loss: 0.3974 - val_acc: 0.4167\n",
      "Epoch 145/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.4043 - acc: 0.3125 - val_loss: 0.3968 - val_acc: 0.4167\n",
      "Epoch 146/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.4037 - acc: 0.3125 - val_loss: 0.3963 - val_acc: 0.4167\n",
      "Epoch 147/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.4031 - acc: 0.3125 - val_loss: 0.3958 - val_acc: 0.4167\n",
      "Epoch 148/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4025 - acc: 0.3125 - val_loss: 0.3953 - val_acc: 0.4167\n",
      "Epoch 149/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.4020 - acc: 0.3125 - val_loss: 0.3947 - val_acc: 0.4167\n",
      "Epoch 150/300\n",
      "96/96 [==============================] - 0s 147us/step - loss: 0.4014 - acc: 0.3125 - val_loss: 0.3942 - val_acc: 0.4167\n",
      "Epoch 151/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.4008 - acc: 0.3125 - val_loss: 0.3937 - val_acc: 0.4167\n",
      "Epoch 152/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.4003 - acc: 0.3125 - val_loss: 0.3932 - val_acc: 0.4167\n",
      "Epoch 153/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3997 - acc: 0.3125 - val_loss: 0.3927 - val_acc: 0.4167\n",
      "Epoch 154/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3992 - acc: 0.3125 - val_loss: 0.3922 - val_acc: 0.4167\n",
      "Epoch 155/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3986 - acc: 0.3125 - val_loss: 0.3917 - val_acc: 0.4167\n",
      "Epoch 156/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.3981 - acc: 0.3125 - val_loss: 0.3912 - val_acc: 0.4167\n",
      "Epoch 157/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.3975 - acc: 0.3125 - val_loss: 0.3907 - val_acc: 0.4167\n",
      "Epoch 158/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3970 - acc: 0.3125 - val_loss: 0.3902 - val_acc: 0.4167\n",
      "Epoch 159/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3965 - acc: 0.3125 - val_loss: 0.3897 - val_acc: 0.4167\n",
      "Epoch 160/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.3960 - acc: 0.3125 - val_loss: 0.3893 - val_acc: 0.4167\n",
      "Epoch 161/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3954 - acc: 0.3125 - val_loss: 0.3888 - val_acc: 0.4167\n",
      "Epoch 162/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3949 - acc: 0.3125 - val_loss: 0.3883 - val_acc: 0.4167\n",
      "Epoch 163/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.3944 - acc: 0.3125 - val_loss: 0.3879 - val_acc: 0.4167\n",
      "Epoch 164/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3939 - acc: 0.3125 - val_loss: 0.3874 - val_acc: 0.4167\n",
      "Epoch 165/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.3934 - acc: 0.3125 - val_loss: 0.3870 - val_acc: 0.4167\n",
      "Epoch 166/300\n",
      "96/96 [==============================] - 0s 106us/step - loss: 0.3929 - acc: 0.3125 - val_loss: 0.3865 - val_acc: 0.4167\n",
      "Epoch 167/300\n",
      "96/96 [==============================] - 0s 150us/step - loss: 0.3924 - acc: 0.3125 - val_loss: 0.3861 - val_acc: 0.4167\n",
      "Epoch 168/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3919 - acc: 0.3125 - val_loss: 0.3856 - val_acc: 0.4167\n",
      "Epoch 169/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3915 - acc: 0.3125 - val_loss: 0.3852 - val_acc: 0.4167\n",
      "Epoch 170/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3910 - acc: 0.3125 - val_loss: 0.3847 - val_acc: 0.4167\n",
      "Epoch 171/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3905 - acc: 0.3125 - val_loss: 0.3843 - val_acc: 0.4167\n",
      "Epoch 172/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3900 - acc: 0.3125 - val_loss: 0.3839 - val_acc: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3895 - acc: 0.3125 - val_loss: 0.3835 - val_acc: 0.4167\n",
      "Epoch 174/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3891 - acc: 0.3125 - val_loss: 0.3830 - val_acc: 0.4167\n",
      "Epoch 175/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3886 - acc: 0.3125 - val_loss: 0.3826 - val_acc: 0.4167\n",
      "Epoch 176/300\n",
      "96/96 [==============================] - 0s 111us/step - loss: 0.3882 - acc: 0.3125 - val_loss: 0.3822 - val_acc: 0.4167\n",
      "Epoch 177/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.3877 - acc: 0.3125 - val_loss: 0.3818 - val_acc: 0.4167\n",
      "Epoch 178/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.3873 - acc: 0.3125 - val_loss: 0.3814 - val_acc: 0.4167\n",
      "Epoch 179/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3868 - acc: 0.3125 - val_loss: 0.3810 - val_acc: 0.4167\n",
      "Epoch 180/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3864 - acc: 0.3125 - val_loss: 0.3806 - val_acc: 0.4167\n",
      "Epoch 181/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3859 - acc: 0.3125 - val_loss: 0.3802 - val_acc: 0.4167\n",
      "Epoch 182/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3855 - acc: 0.3125 - val_loss: 0.3798 - val_acc: 0.4167\n",
      "Epoch 183/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3851 - acc: 0.3125 - val_loss: 0.3795 - val_acc: 0.4167\n",
      "Epoch 184/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3847 - acc: 0.3125 - val_loss: 0.3791 - val_acc: 0.4167\n",
      "Epoch 185/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3842 - acc: 0.3125 - val_loss: 0.3787 - val_acc: 0.4167\n",
      "Epoch 186/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3838 - acc: 0.3125 - val_loss: 0.3783 - val_acc: 0.4167\n",
      "Epoch 187/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3834 - acc: 0.3125 - val_loss: 0.3779 - val_acc: 0.4167\n",
      "Epoch 188/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3830 - acc: 0.3125 - val_loss: 0.3776 - val_acc: 0.4167\n",
      "Epoch 189/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3826 - acc: 0.3125 - val_loss: 0.3772 - val_acc: 0.4167\n",
      "Epoch 190/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3822 - acc: 0.3125 - val_loss: 0.3769 - val_acc: 0.4167\n",
      "Epoch 191/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3818 - acc: 0.3125 - val_loss: 0.3765 - val_acc: 0.4167\n",
      "Epoch 192/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.3814 - acc: 0.3125 - val_loss: 0.3761 - val_acc: 0.4167\n",
      "Epoch 193/300\n",
      "96/96 [==============================] - 0s 141us/step - loss: 0.3810 - acc: 0.3125 - val_loss: 0.3758 - val_acc: 0.4167\n",
      "Epoch 194/300\n",
      "96/96 [==============================] - 0s 143us/step - loss: 0.3806 - acc: 0.3125 - val_loss: 0.3754 - val_acc: 0.4167\n",
      "Epoch 195/300\n",
      "96/96 [==============================] - 0s 149us/step - loss: 0.3802 - acc: 0.3125 - val_loss: 0.3751 - val_acc: 0.4167\n",
      "Epoch 196/300\n",
      "96/96 [==============================] - 0s 155us/step - loss: 0.3798 - acc: 0.3125 - val_loss: 0.3748 - val_acc: 0.4167\n",
      "Epoch 197/300\n",
      "96/96 [==============================] - 0s 151us/step - loss: 0.3794 - acc: 0.3125 - val_loss: 0.3744 - val_acc: 0.4167\n",
      "Epoch 198/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.3791 - acc: 0.3125 - val_loss: 0.3741 - val_acc: 0.4167\n",
      "Epoch 199/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3787 - acc: 0.3125 - val_loss: 0.3738 - val_acc: 0.4167\n",
      "Epoch 200/300\n",
      "96/96 [==============================] - 0s 144us/step - loss: 0.3783 - acc: 0.3125 - val_loss: 0.3734 - val_acc: 0.4167\n",
      "Epoch 201/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3779 - acc: 0.3125 - val_loss: 0.3731 - val_acc: 0.4167\n",
      "Epoch 202/300\n",
      "96/96 [==============================] - 0s 152us/step - loss: 0.3776 - acc: 0.3125 - val_loss: 0.3728 - val_acc: 0.4167\n",
      "Epoch 203/300\n",
      "96/96 [==============================] - 0s 144us/step - loss: 0.3772 - acc: 0.3125 - val_loss: 0.3725 - val_acc: 0.4167\n",
      "Epoch 204/300\n",
      "96/96 [==============================] - 0s 145us/step - loss: 0.3769 - acc: 0.3125 - val_loss: 0.3722 - val_acc: 0.4167\n",
      "Epoch 205/300\n",
      "96/96 [==============================] - 0s 149us/step - loss: 0.3765 - acc: 0.3125 - val_loss: 0.3718 - val_acc: 0.4167\n",
      "Epoch 206/300\n",
      "96/96 [==============================] - 0s 152us/step - loss: 0.3762 - acc: 0.3125 - val_loss: 0.3715 - val_acc: 0.4167\n",
      "Epoch 207/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3758 - acc: 0.3125 - val_loss: 0.3712 - val_acc: 0.4167\n",
      "Epoch 208/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3755 - acc: 0.3125 - val_loss: 0.3709 - val_acc: 0.4167\n",
      "Epoch 209/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.3751 - acc: 0.3125 - val_loss: 0.3706 - val_acc: 0.4167\n",
      "Epoch 210/300\n",
      "96/96 [==============================] - 0s 140us/step - loss: 0.3748 - acc: 0.3125 - val_loss: 0.3703 - val_acc: 0.4167\n",
      "Epoch 211/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.3745 - acc: 0.3125 - val_loss: 0.3700 - val_acc: 0.4167\n",
      "Epoch 212/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.3741 - acc: 0.3125 - val_loss: 0.3697 - val_acc: 0.4167\n",
      "Epoch 213/300\n",
      "96/96 [==============================] - 0s 147us/step - loss: 0.3738 - acc: 0.3125 - val_loss: 0.3694 - val_acc: 0.4167\n",
      "Epoch 214/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.3735 - acc: 0.3125 - val_loss: 0.3692 - val_acc: 0.4167\n",
      "Epoch 215/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.3732 - acc: 0.3125 - val_loss: 0.3689 - val_acc: 0.4167\n",
      "Epoch 216/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3729 - acc: 0.3125 - val_loss: 0.3686 - val_acc: 0.4167\n",
      "Epoch 217/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3725 - acc: 0.3125 - val_loss: 0.3683 - val_acc: 0.4167\n",
      "Epoch 218/300\n",
      "96/96 [==============================] - 0s 142us/step - loss: 0.3722 - acc: 0.3125 - val_loss: 0.3680 - val_acc: 0.4167\n",
      "Epoch 219/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3719 - acc: 0.3125 - val_loss: 0.3678 - val_acc: 0.4167\n",
      "Epoch 220/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.3716 - acc: 0.3125 - val_loss: 0.3675 - val_acc: 0.4167\n",
      "Epoch 221/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3713 - acc: 0.3125 - val_loss: 0.3672 - val_acc: 0.4167\n",
      "Epoch 222/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3710 - acc: 0.3125 - val_loss: 0.3670 - val_acc: 0.4167\n",
      "Epoch 223/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3707 - acc: 0.3125 - val_loss: 0.3667 - val_acc: 0.4167\n",
      "Epoch 224/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3704 - acc: 0.3125 - val_loss: 0.3664 - val_acc: 0.4167\n",
      "Epoch 225/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.3701 - acc: 0.3125 - val_loss: 0.3662 - val_acc: 0.4167\n",
      "Epoch 226/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3698 - acc: 0.3125 - val_loss: 0.3659 - val_acc: 0.4167\n",
      "Epoch 227/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3696 - acc: 0.3125 - val_loss: 0.3657 - val_acc: 0.4167\n",
      "Epoch 228/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3693 - acc: 0.3125 - val_loss: 0.3654 - val_acc: 0.4167\n",
      "Epoch 229/300\n",
      "96/96 [==============================] - 0s 129us/step - loss: 0.3690 - acc: 0.3125 - val_loss: 0.3652 - val_acc: 0.4167\n",
      "Epoch 230/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3687 - acc: 0.3125 - val_loss: 0.3649 - val_acc: 0.4167\n",
      "Epoch 231/300\n",
      "96/96 [==============================] - 0s 138us/step - loss: 0.3684 - acc: 0.3125 - val_loss: 0.3647 - val_acc: 0.4167\n",
      "Epoch 232/300\n",
      "96/96 [==============================] - 0s 116us/step - loss: 0.3682 - acc: 0.3125 - val_loss: 0.3644 - val_acc: 0.4167\n",
      "Epoch 233/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3679 - acc: 0.3125 - val_loss: 0.3642 - val_acc: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3676 - acc: 0.3125 - val_loss: 0.3640 - val_acc: 0.4167\n",
      "Epoch 235/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3674 - acc: 0.3125 - val_loss: 0.3637 - val_acc: 0.4167\n",
      "Epoch 236/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3671 - acc: 0.3125 - val_loss: 0.3635 - val_acc: 0.4167\n",
      "Epoch 237/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3668 - acc: 0.3125 - val_loss: 0.3633 - val_acc: 0.4167\n",
      "Epoch 238/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3666 - acc: 0.3125 - val_loss: 0.3630 - val_acc: 0.4167\n",
      "Epoch 239/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3663 - acc: 0.3125 - val_loss: 0.3628 - val_acc: 0.4167\n",
      "Epoch 240/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3661 - acc: 0.3125 - val_loss: 0.3626 - val_acc: 0.4167\n",
      "Epoch 241/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3658 - acc: 0.3125 - val_loss: 0.3624 - val_acc: 0.4167\n",
      "Epoch 242/300\n",
      "96/96 [==============================] - 0s 147us/step - loss: 0.3656 - acc: 0.3125 - val_loss: 0.3622 - val_acc: 0.4167\n",
      "Epoch 243/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3653 - acc: 0.3125 - val_loss: 0.3619 - val_acc: 0.4167\n",
      "Epoch 244/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.3651 - acc: 0.3125 - val_loss: 0.3617 - val_acc: 0.4167\n",
      "Epoch 245/300\n",
      "96/96 [==============================] - 0s 134us/step - loss: 0.3648 - acc: 0.3125 - val_loss: 0.3615 - val_acc: 0.4167\n",
      "Epoch 246/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.3646 - acc: 0.3125 - val_loss: 0.3613 - val_acc: 0.4167\n",
      "Epoch 247/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.3644 - acc: 0.3125 - val_loss: 0.3611 - val_acc: 0.4167\n",
      "Epoch 248/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.3641 - acc: 0.3125 - val_loss: 0.3609 - val_acc: 0.4167\n",
      "Epoch 249/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3639 - acc: 0.3125 - val_loss: 0.3607 - val_acc: 0.4167\n",
      "Epoch 250/300\n",
      "96/96 [==============================] - 0s 133us/step - loss: 0.3637 - acc: 0.3125 - val_loss: 0.3605 - val_acc: 0.4167\n",
      "Epoch 251/300\n",
      "96/96 [==============================] - 0s 135us/step - loss: 0.3635 - acc: 0.3125 - val_loss: 0.3603 - val_acc: 0.4167\n",
      "Epoch 252/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3632 - acc: 0.3125 - val_loss: 0.3601 - val_acc: 0.4167\n",
      "Epoch 253/300\n",
      "96/96 [==============================] - 0s 139us/step - loss: 0.3630 - acc: 0.3125 - val_loss: 0.3599 - val_acc: 0.4167\n",
      "Epoch 254/300\n",
      "96/96 [==============================] - 0s 145us/step - loss: 0.3628 - acc: 0.3125 - val_loss: 0.3597 - val_acc: 0.4167\n",
      "Epoch 255/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3626 - acc: 0.3125 - val_loss: 0.3595 - val_acc: 0.4167\n",
      "Epoch 256/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.3623 - acc: 0.3125 - val_loss: 0.3593 - val_acc: 0.4167\n",
      "Epoch 257/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.3621 - acc: 0.3125 - val_loss: 0.3591 - val_acc: 0.4167\n",
      "Epoch 258/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3619 - acc: 0.3125 - val_loss: 0.3589 - val_acc: 0.4167\n",
      "Epoch 259/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3617 - acc: 0.3125 - val_loss: 0.3587 - val_acc: 0.4167\n",
      "Epoch 260/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3615 - acc: 0.3125 - val_loss: 0.3586 - val_acc: 0.4167\n",
      "Epoch 261/300\n",
      "96/96 [==============================] - 0s 131us/step - loss: 0.3613 - acc: 0.3125 - val_loss: 0.3584 - val_acc: 0.4167\n",
      "Epoch 262/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3611 - acc: 0.3125 - val_loss: 0.3582 - val_acc: 0.4167\n",
      "Epoch 263/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3609 - acc: 0.3125 - val_loss: 0.3580 - val_acc: 0.4167\n",
      "Epoch 264/300\n",
      "96/96 [==============================] - 0s 108us/step - loss: 0.3607 - acc: 0.3125 - val_loss: 0.3578 - val_acc: 0.4167\n",
      "Epoch 265/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3605 - acc: 0.3125 - val_loss: 0.3577 - val_acc: 0.4167\n",
      "Epoch 266/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3603 - acc: 0.3125 - val_loss: 0.3575 - val_acc: 0.4167\n",
      "Epoch 267/300\n",
      "96/96 [==============================] - 0s 128us/step - loss: 0.3601 - acc: 0.3125 - val_loss: 0.3573 - val_acc: 0.4167\n",
      "Epoch 268/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3599 - acc: 0.3125 - val_loss: 0.3572 - val_acc: 0.4167\n",
      "Epoch 269/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3597 - acc: 0.3125 - val_loss: 0.3570 - val_acc: 0.4167\n",
      "Epoch 270/300\n",
      "96/96 [==============================] - 0s 118us/step - loss: 0.3595 - acc: 0.3125 - val_loss: 0.3568 - val_acc: 0.4167\n",
      "Epoch 271/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3593 - acc: 0.3125 - val_loss: 0.3566 - val_acc: 0.4167\n",
      "Epoch 272/300\n",
      "96/96 [==============================] - 0s 137us/step - loss: 0.3591 - acc: 0.3125 - val_loss: 0.3565 - val_acc: 0.4167\n",
      "Epoch 273/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3590 - acc: 0.3125 - val_loss: 0.3563 - val_acc: 0.4167\n",
      "Epoch 274/300\n",
      "96/96 [==============================] - 0s 122us/step - loss: 0.3588 - acc: 0.3125 - val_loss: 0.3562 - val_acc: 0.4167\n",
      "Epoch 275/300\n",
      "96/96 [==============================] - 0s 124us/step - loss: 0.3586 - acc: 0.3125 - val_loss: 0.3560 - val_acc: 0.4167\n",
      "Epoch 276/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3584 - acc: 0.3125 - val_loss: 0.3558 - val_acc: 0.4167\n",
      "Epoch 277/300\n",
      "96/96 [==============================] - 0s 125us/step - loss: 0.3582 - acc: 0.3125 - val_loss: 0.3557 - val_acc: 0.4167\n",
      "Epoch 278/300\n",
      "96/96 [==============================] - 0s 120us/step - loss: 0.3581 - acc: 0.3125 - val_loss: 0.3555 - val_acc: 0.4167\n",
      "Epoch 279/300\n",
      "96/96 [==============================] - 0s 126us/step - loss: 0.3579 - acc: 0.3125 - val_loss: 0.3554 - val_acc: 0.4167\n",
      "Epoch 280/300\n",
      "96/96 [==============================] - 0s 101us/step - loss: 0.3577 - acc: 0.3125 - val_loss: 0.3552 - val_acc: 0.4167\n",
      "Epoch 281/300\n",
      "96/96 [==============================] - 0s 127us/step - loss: 0.3575 - acc: 0.3125 - val_loss: 0.3551 - val_acc: 0.4167\n",
      "Epoch 282/300\n",
      "96/96 [==============================] - 0s 112us/step - loss: 0.3574 - acc: 0.3125 - val_loss: 0.3549 - val_acc: 0.4167\n",
      "Epoch 283/300\n",
      "96/96 [==============================] - 0s 113us/step - loss: 0.3572 - acc: 0.3125 - val_loss: 0.3548 - val_acc: 0.4167\n",
      "Epoch 284/300\n",
      "96/96 [==============================] - 0s 130us/step - loss: 0.3570 - acc: 0.3125 - val_loss: 0.3546 - val_acc: 0.4167\n",
      "Epoch 285/300\n",
      "96/96 [==============================] - 0s 123us/step - loss: 0.3569 - acc: 0.3125 - val_loss: 0.3545 - val_acc: 0.4167\n",
      "Epoch 286/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3567 - acc: 0.3125 - val_loss: 0.3543 - val_acc: 0.4167\n",
      "Epoch 287/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3566 - acc: 0.3125 - val_loss: 0.3542 - val_acc: 0.4167\n",
      "Epoch 288/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3564 - acc: 0.3125 - val_loss: 0.3541 - val_acc: 0.4167\n",
      "Epoch 289/300\n",
      "96/96 [==============================] - 0s 119us/step - loss: 0.3562 - acc: 0.3125 - val_loss: 0.3539 - val_acc: 0.4167\n",
      "Epoch 290/300\n",
      "96/96 [==============================] - 0s 114us/step - loss: 0.3561 - acc: 0.3125 - val_loss: 0.3538 - val_acc: 0.4167\n",
      "Epoch 291/300\n",
      "96/96 [==============================] - 0s 117us/step - loss: 0.3559 - acc: 0.3125 - val_loss: 0.3536 - val_acc: 0.4167\n",
      "Epoch 292/300\n",
      "96/96 [==============================] - 0s 121us/step - loss: 0.3558 - acc: 0.3125 - val_loss: 0.3535 - val_acc: 0.4167\n",
      "Epoch 293/300\n",
      "96/96 [==============================] - 0s 115us/step - loss: 0.3556 - acc: 0.3125 - val_loss: 0.3534 - val_acc: 0.4167\n",
      "Epoch 294/300\n",
      "96/96 [==============================] - 0s 149us/step - loss: 0.3555 - acc: 0.3125 - val_loss: 0.3532 - val_acc: 0.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/300\n",
      "96/96 [==============================] - 0s 142us/step - loss: 0.3553 - acc: 0.3125 - val_loss: 0.3531 - val_acc: 0.4167\n",
      "Epoch 296/300\n",
      "96/96 [==============================] - 0s 160us/step - loss: 0.3552 - acc: 0.3125 - val_loss: 0.3530 - val_acc: 0.4167\n",
      "Epoch 297/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.3550 - acc: 0.3125 - val_loss: 0.3528 - val_acc: 0.4167\n",
      "Epoch 298/300\n",
      "96/96 [==============================] - 0s 136us/step - loss: 0.3549 - acc: 0.3125 - val_loss: 0.3527 - val_acc: 0.4167\n",
      "Epoch 299/300\n",
      "96/96 [==============================] - 0s 132us/step - loss: 0.3547 - acc: 0.3125 - val_loss: 0.3526 - val_acc: 0.4167\n",
      "Epoch 300/300\n",
      "96/96 [==============================] - 0s 154us/step - loss: 0.3546 - acc: 0.3125 - val_loss: 0.3525 - val_acc: 0.4167\n",
      "30/30 [==============================] - 0s 50us/step\n",
      "30/30 [==============================] - 0s 38us/step\n",
      "loss: 0.3544222414493561\n",
      "acc: 33.33333432674408%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzs3Xd8FMX7wPHPkx4hEFpIpaNA6B2VKihFBAWkBAJILxaKgoogTUDQL1WRXqSj2GjSm3QMIiIaSiollBBCSJ/fH3vwOyG5JCSXBJj367Uv725nZ2cTvCe7M/OMKKXQNE3TtEdlk9MN0DRN0x5vOpBomqZpmaIDiaZpmpYpOpBomqZpmaIDiaZpmpYpOpBomqZpmaIDiaY9QEReF5EQEYkWkWo53Z70EpEeIrI/p9thiYgoESmT0+3QspYOJFqOEJGLItI0B8/fyPSlNiKF3dOAwUqpvEqp363RVlOdd03B6t42OyvP8Ti0QXsy2OV0AzQth3QHbgD+wJQH9hUHTmfFSUREAFFKJaewu7VSantWnCcTckMbtMecviPRch0R6SMigSJyQ0R+EhFP0+ciIv8TkasiEiUip0SkomlfSxH5S0Rui0iYiAy3UH8eoD0wCCgrIjVNnzuKSDRgC5wUkXMishwoBvxs+ov9A1PZuiLym4hEishJEWlkVv9uEZkoIgeAGKBUBq+/tIjsFJHrInJNRFaIiKvZfh8R+V5EIkxlZj9w/DQRuSkiF0SkRUbObVZHDxE5ICKzReSWiPwtIi+Z7fc0/W5umH5Xfcz22YrIR6af320ROS4iPmbVNxWRf00/uzmmYKs9zpRSetNbtm/ARaBpCp83Aa4B1QFHYBaw17TvFeA44AoIUB7wMO27BNQ3vS4AVLdw7m6m8rbAz8CsB/YroExqbQW8gOtAS4w/xpqZ3hcx7d8NBAO+GHf99um9ftO+MqY6HYEiwF5gummfLXAS+B+QB3ACXjTt6wEkAH1M5QYA4Rh3ROn+HZjVlQgMAeyBjsAtoKBp/17gK9P5qwIRQBPTvveBU8Bzpt9TFaCQ2c/2F9PvsJjpuOY5/e9Rb5nbcrwBens6NwuBZCHwudn7vKYvxxKmIPMPUBeweeC4YKAfkC8d595u9sXc2fRlZm+2P61AMgJY/kCdW4Hupte7gXHpuP5oINJs65NK2bbA76bX9UzttUuhXA8g0Oz9M6Zrcc9oG0x1/ScIAUcwgrAPkAS4mO2bBCwxvT4LtEnlnApT4DO9XwuMzOl/j3rL3KYfbWm5jScQdO+NUioa4699L6XUTmA2MAe4KiLzRCSfqWg7jDuEIBHZIyL1Uqrc9IilMbDC9NGPGH9Vt8pAG4sDHUyPZiJFJBJ4EfAwKxOSjnraKqVczbb5pjYWFZHVpkd0UcC3QGHTMT5AkFIqMZU6L997oZSKMb3Mm9E2mIQp07e9SRDG78cTuKGUuv3APi+zNp6zcM7LZq9j0mif9hjQgUTLbcIxvqiB+/0ZhYAwAKXUTKVUDaAC8CzGYxSUUkeVUm0AN+AHjL90U9IN49/9zyJyGTiPEUi6W2jTgymyQzDuSMy/gPMopSZbOCYjPjMdX0kplQ/oivGI6N65i4lIdgyU8Xqg/6IYxu8nHCgoIi4P7Asza2PpbGiflkvoQKLlJHsRcTLb7IBVQE8RqSoijhhfqoeVUhdFpJaI1BERe+AOEAski4iDiPiJSH6lVAIQBaQ0SgqMgDEW47n+va0d0FJECqVyzBX+22H+LdBaRF4xdSw7mYYTe2fux3GfC8Yjp1si4oUpWJocwejfmSwieUznfiGLzvsgN+AdEbEXkQ4YfVKblFIhwG/AJNP5KwO9MH4uAAuA8SJS1jRAorKFn632BNCBRMtJm4C7ZtunyhiK+gnwHcYXZmmgk6l8PmA+cBPjUcp1YKppXzfgoulRUH/A78GTiUhdjLudOUqpy2bbT0AgRn9JSiYBo0yPsYabvkjbAB9h9FeEYHzZZ/T/p3sjwe5tG0yfj8UYbHAL2Ah8f+8ApVQS0BqjQz4YCMXoCH9UqbUB4DBQFmPww0SgvVLqumlfZ4x+q3BgAzBG/f8w4i8x7gh/xQjqCwHnTLRRy+Xkv49ANU3TjOG/QG+l1Is53RYt99N3JJqmaVqm6ECiaZqmZYp+tKVpmqZlir4j0TRN0zLlqUjaWLhwYVWiRImcboamadpj5fjx49eUUkXSKvdUBJISJUpw7NixnG6GpmnaY0VEgtIupR9taZqmaZmkA4mmaZqWKTqQaJqmaZnyVPSRaJr29ElISCA0NJTY2Nicbkqu5+TkhLe3N/b29o90vA4kmqY9kUJDQ3FxcaFEiRLoRRhTp5Ti+vXrhIaGUrJkyUeqQz/a0jTtiRQbG0uhQoV0EEmDiFCoUKFM3bnpQKJp2hNLB5H0yezPSQcSS/7eCCdX53QrNE3TcjUdSFKjFBxfChv6weFvcro1mqY9hkJCQmjcuDEVKlTA19eXGTNmANCjRw/Wr1//n7J58xorDl+8eJGKFStme1szQ3e2p0YE3lwG3/WCzR/A3Uho+IHxuaZpWjrY2dnxxRdfUL16dW7fvk2NGjVo1qxZTjcry1n1jkREmovIWREJFJGRKezvISIRIhJg2nqbPq8qIgdF5LSI/CEiHc2OWSIiF8yOqWq1C7B3gg5LoUpn2P0ZbP0IklNbwVXTNO2/PDw8qF69OgAuLi6UL1+esLCwNI56/FjtjkREbIE5QDOM5UCPishPSqm/Hii6Rik1+IHPYgB/pdS/IuIJHBeRrUqpSNP+95VS68kOtnbQ5itwyg+HvoLYW9B6pvG5pmmPhbE/n+av8KgsrbOCZz7GtPZNd/mLFy/y+++/U6dOHVasWMH777/PhAkTsrRNOcWa34a1gUCl1HkAEVmNsc71g4HkIUqpf8xeh4vIVaAIEJn6UVZkYwPNJ4NzAdg9yQgm7ReBnWOONEfTtMdLdHQ07dq1Y/r06eTLlw+AqVOn0r59+/tl7vWRPI6sGUi8gBCz96FAnRTKtRORBsA/wBCllPkxiEhtwAE4Z/bxRBEZDewARiql4h6sVET6An0BihUrlpnruFchNBpp3JlsGQkrOkCnFeDokvm6NU2zqozcOWS1hIQE2rVrh5+fH2+88UaOtcOacnrU1s9ACaVUZWAbsNR8p4h4AMuBnkqpe50THwLlgFpAQWBEShUrpeYppWoqpWoWKZJmOv30qzsA2s6Fi/thaWu4cy3r6tY07YmilKJXr16UL1+eoUOH5nRzrMaagSQM8DF772367D6l1HWzu4kFQI17+0QkH7AR+FgpdcjsmEvKEAcsxniElr2qdjbuRq6egUXNITIk7WM0TXvqHDhwgOXLl7Nz506qVq1K1apV2bRpU5rHnT17Fm9v7/vbunXrsqG1j86aj7aOAmVFpCRGAOkEdDEvICIeSqlLprevAWdMnzsAG4BlD3aq3ztGjKmYbYE/rXgNqXuuBXTbACs7wcKXjddu5XKkKZqm5U4vvvgiSqmHPm/ZsuVDn0VHRwPGQnwJCQlWb1tWstodiVIqERgMbMUIEGuVUqdFZJyIvGYq9o5piO9J4B2gh+nzN4EGQI8UhvmuEJFTwCmgMGDVYQ8p/SO4r/jz0HMjqCRY3BxCjlqzKZqmabmSWPyifELUrFlTZXSpXaUUX277hxt34pnQtqLlXDQ3LsDy1yH6CnRcDmWaZrLFmqZl1pkzZyhfvnxON+OxkdLPS0SOK6VqpnVsTne251oiQmKyYsXhYCZv+dvynUnBkvDWVihY2njUdSp7prhomqblBnpWnQUfvPIc0bGJfLPnPPmc7BnUuEzqhV2KGo+5VnWG73rD3ZtQu0/2NVbTNC2H6EBigYgw9jVfouMSmbr1LHkd7ej+fInUD3DKD12/g/VvwabhxtDgRiN1fi5N055oOpBYEJcUhw02TG1fmei4RMb8dJq8jna0q+Gd+kH2zvDmcvj5XdgzGWKuQYvPwcY2+xquaZqWjXQfSSqUUozaP4r+2/sTkxTNrM7VeKFMId5ff5Itf162fLCtHbSZDc+/A0cXGI+6EuOzp+GapmnZTAeSVIgIDbwbcOLqCbpv7k5kfATzutWkqo8r76z6nX3/RqRVAbw8HpqNg9Pfw8o3IS46exqvadpjyVK+rYCAAOrVq4evry+VK1dmzZo19/c1atQI85Gp5mua7N69m1dffdV6jUYHEotal27N3KZzuXznMn4b/Qi5E8jiHrUp7ZaXvsuOc+zijbQreeFdaDMHLuyFZa/BnevWb7imaU+cZ555hmXLlnH69Gm2bNnCe++9R2RkzuSxfZDuI0lDHY86LGuxjAHbB9B9c3f+1+h/LHurNh2/OUjPxUdZ1bcuFb3yW66kWldwLgjrehgTF7t+B65ZkEhS07T02TwSLp/K2jrdK0GLyRaLjBw5Eh8fHwYNGgTAp59+ip2dHbt27eLmzZskJCQwYcIE2rRpk+bpnn322fuvPT09cXNzIyIiAldX18xdRxbQdyTpULZAWVa0XIG3izeDdgxi/+VNLO9dBxcnO/wXHeHfK7fTrqRcSyONyu0rsKAZXPrD+g3XNC1HdezYkbVr195/v3btWrp3786GDRs4ceIEu3btYtiwYZbnqaXgyJEjxMfHU7p06fuf+fn53c/nlVIKFmvSdyTpVDRPUZY2X8rQ3UMZ/dtoBlS5xLe9/ek47xBdFhxmbb96lCycx3IlJV6AXlvh23awuCV0XAalm2TPBWja0yyNOwdrqVatGlevXiU8PJyIiAgKFCiAu7s7Q4YMYe/evdjY2BAWFsaVK1dwd3dPV52XLl2iW7duLF26FBub/78XWLFiBTVrGpPQL168aPV+EXP6jiQD8jrkZU7TObQp3YavT37NorOTWfZWDZKTFV3mHyLkRkzalbiVh97boUBxY02TgFXWb7imaTmmQ4cOrF+/njVr1tCxY0dWrFhBREQEx48fJyAggKJFixIbG5uuuqKiomjVqhUTJ06kbt26Vm55+ulAkkH2NvaMf2E8A6oM4MdzPzL9z5HM616RmPgkOs8/RHjk3bQryecJPTdB8Rfgh/6wdyo8BTnPNO1p1LFjR1avXs369evp0KEDt27dws3NDXt7e3bt2kVQUFC66omPj+f111/H39//Pysr5gY6kFgQd+ECsWf/eehzEWFg1YGMfX4shy8dZsrJd5nZtRS3YhLoMv8QV6PS8deFU37wWw+VO8LOCfDLEEhKtMJVaJqWk3x9fbl9+zZeXl54eHjg5+fHsWPHqFSpEsuWLaNcufQtP7F27Vr27t3LkiVL7veFBAQEpOvYHTt2/Gd9k4MHD2bmkh6is/+mQilFUBc/4s6dw+ebuTxTrVqK5faF7mPYnmEUcCzA4AqTGbH6Cp6uzqzuW5fCedOxprtSsGMc7P8Snm1urAXvkEZfi6ZpadLZfzNGZ/+1AhHBc+pUbAu4EvxWL6L37U+xXH3v+ixuvpi4pDgmnxzMh687Enozhq4LDhMZk47Z7CLQdAy0+gL+/dVYvjc6jcmOmqZpuYgOJBY4eHtRYsUKHEqUIGTgQKJSWSLTt5Av37b8loJOBZn+53AGtLzD+Wt36LbwCFGx6VzprFZv6PgtXPkLFjaD6+ey8Eo0TXtcnDp16v6jq3tbnTp1crpZFulHW+mQFBVFyICB3D1xAvcxoynQqVOK5SJjI3l759ucjDhJm2L9Wb29BJW88rOsVx3yOqZzpHXIUVjV0XjdZS14p3lXqWlaCvSjrYzRj7aszDZfPootmE/eBg24/OlYrs2dm+IEIlcnV+a/PJ8mxZrwQ/DXvPziEU6G3uStJUe5G5+UvpP51IJe28AxHyx5Ff5O+S5I0zQtt9CBJJ1snJ3xnj2LfK1bEzF9BlcnT0ElJz9UzsnOiS8afkGXcl3Ye/U76tTZzLGgK/RdfozYhHQGk0KljWDiVh7W+BkZhDVN03IpqwYSEWkuImdFJFBERqawv4eIRIhIgGnrbbavu4j8a9q6m31eQ0ROmeqcKRYXU8/i67G3x3PKZAp07cqNpUu59NHHqMSHh+za2tgysvZIhtUYxqnIvVSotoZ954IZuOIE8YkPB58U5S0CPX6Bsi/DxmHw6yeQQuDSNE3LaVYLJCJiC8wBWgAVgM4iUiGFomuUUlVN2wLTsQWBMUAdoDYwRkQKmMp/DfQBypq25ta6hpSIjQ1FP/6Iwm8P5tYPPxD67nskx8U9XE6EHhV78HmDzwmPPUuJSovZFXiWt1edICEpnQHBIQ90XAE1e8FvM2GdP8SnY/a8pmm5QmxsLLVr16ZKlSr4+voyZswYIOfTvmc1a96R1AYClVLnlVLxwGog7RSXhleAbUqpG0qpm8A2oLmIeAD5lFKHlNFJsQxoa43GWyIiFBk0iKKjRhG9YwchffqSFJ3yWiMtSrbgm2bfEK8iKVpuPtsCf2fo2pMkJadzkIOtnTE0+JVJcOYXWNLKSPyoaVqu5+joyM6dOzl58iQBAQFs2bKFQ4cO5XSzspw1A4kXEGL2PtT02YPaicgfIrJeRHzSONbL9DqtOhGRviJyTESORURYZ15Gwa5+eE79nJgTJwju3oPEGymvT1LLvRZLWywlr6M9rqXnsylwDyO++4Pk9AYTEag3EDqthIi/YcFLcOV0Fl6JpmnWICL3F6tKSEggISGBbHwan21yOvvvz8AqpVSciPQDlgJZkg5XKTUPmAfG8N+sqDMl+Vu3xsbFhbB33yOoix/FFi3E3tPzoXL3UtEP2DGAQLWEHwKjcPjRholtK6b/H1a5ltBzM6zqBAtfgQ5LoGzTrL0gTXsCTTkyhb9v/J2ldZYrWI4RtUekWS4pKYkaNWoQGBjIoEGD7s8J8fPzw9nZGTDyaJln8n3cWLPlYYCP2Xtv02f3KaWuK6XudTAsAGqkcWyY6XWqdeYEl0aNKLZwAYnXrnHRlFYlJfdS0ddyr4Gz5zrWBy5l7M+nM7YWgWdV6L0DCpQwlu/VI7o0LVeztbUlICCA0NBQjhw5wp9//gkYad8DAgIICAhgUyqTnR8X1rwjOQqUFZGSGF/2nYAu5gVExEMpdcn09jXgjOn1VuAzsw72l4EPlVI3RCRKROoChwF/YJYVryHdnqlZk+LLlxHcpy9Bfl3xmT8f50oVHyrn4uDC3KZz+eTAJ2xkI6vOR2K/+T0+auGb/juT/F7w1hb4rpcxouv6eWN9eBvbLL4qTXsypOfOwdpcXV1p3LgxW7ZsyemmZDmr3ZEopRKBwRhB4QywVil1WkTGichrpmLviMhpETkJvAP0MB17AxiPEYyOAuNMnwEMxLh7CQTOAZutdQ0Z5VS+PCVWfItNnjwEd+/OnVQ61ext7fms/mf0qtgLhwKH+fb8WCZv+SNjdyaOeY0+kzoD4NAcWNMV4lLu8Nc0LWdERETcX1f97t27bNu2Ld3Zfh8nVn0op5TapJR6VilVWik10fTZaKXUT6bXHyqlfJVSVZRSjZVSf5sdu0gpVca0LTb7/JhSqqKpzsEql+V4cShenOIrV2Dv5UlIn75EbduWYjkbseG9Gu/xUe2PsHM5y/KgD/lsy9GMBRMbW2Plt5bT4J8tsLgFRIVn0ZVompZZly5donHjxlSuXJlatWrRrFmzdA3ttXba96ymc21ZSVJkJMH9+hF76k88xo/DtV27VMvuCNrJ0N3vkxjvQgfvsXzaomHGT/jPr7C+p5Fapcsa8KicidZr2uNP59rKGJ1rKxeydXWl+KJF5Klbl0sfj+L64iWpln2peBOWNl+Io0M868JH8NGmXzJ+wmdfhre2gtjAouZw9sl7DqtpWu6kA4kV2eTJg/fcr3F55RWuTpnC1enTU310VbVoVb5rs5I89nn46cpohv68IuMndK8IfXZA4bKwujMc/Eov4atpmtXpQGJlNg4OeH35Bfnbt+P63G+4Mn58iskeAUq6lmBjuzXkt/Ph1+tT6P/jIwxIc3E31oMv1wq2fgg/DoLEh1O4aJqmZRUdSLKB2NriMX48Bd96i5srVxH+wQhUQsoLXhXJU5itb66kiG0VDkTOo+t3Y0jOaLJGhzzQYRk0HAkBK0xpVS5nwZVomqY9TAeSbCIiuL0/nCJDhhD1yy+EDn6b5NjYFMvmdczDls6L8LZtzMno73l93WDiE9OxbK85Gxto/CG8ucxYdXFeIwg9nvkL0TRNe4AOJNlIRCjcry/un44heu9eQnr3Ien27RTLOtrZ80vn6Tzn8CbnY/fRcm13ouKiMn7SCm2g169ga28MDz65OpNXoWma9l86kOSAAp064TltKjEBARaTPdra2rCu0yhqPNOfy/F/0XJdZy7feYRHVO4Voc9u8KkNG/rB1o8h6eF1VDRN0x6FDiQ5JH+rVvjMmU3cuXME+XUl4dKlFMuJCIvbD6RR/pFExl+m9Xcd+efGvxk/YZ5C0G0D1O4LB2fDyg5w92Ymr0LTtKx0L1NwSoKCgqhevTpVq1bF19eXuXPn3t9XokQJrl27dv+9+ZomS5YsYfDgwdZrNDqQ5Ki8DRsayR4jIoxkjxcupFhORJjVthOvFplATEICHX/uyqHwIxk/oa09tJwKrWfChX0wrzFc/jOTV6FpWnbw8PDg4MGDBAQEcPjwYSZPnkx4eO7IZJHTaeSfes/UrEnxZUsJ7t2HIL+uFFswH6cKDy8kKSJMerU5eba6sCp4DH239eOzFyfwaulWGT9pje5QpBys9YcFTaH1DKjSMQuuRtNyp8uffUbcmaxNI+9YvhzuH31ksczIkSPx8fFh0KBBAHz66afY2dmxa9cubt68SUJCAhMmTKBNm7TX/HNwcLj/Oi4uLuOjOa1I35HkAk4VKlB8xbeIoyNB/t2JSSWdi4jwSfMX6VVqGgkx3ny4fyQLTy3OWH6ue4rVgX57was6bOgLm96HjI4M0zTNoo4dO7J27dr779euXUv37t3ZsGEDJ06cYNeuXQwbNizd/w+HhIRQuXJlfHx8GDFiBJ5max81btyYqlWrUrVqVXr37p3l12KJviPJJRxLlqTEyhUEv9WL4F698Z45g7wNU865NaxpNfLYT2X6H+OYfuJLLt+5xMjaI7DNaBp5l6Lg/yNs/9ToN7l0EjoshXwemb8gTctF0rpzsJZq1apx9epVwsPDiYiIoECBAri7uzNkyBD27t2LjY0NYWFhXLlyBXd39zTr8/Hx4Y8//iA8PJy2bdvSvn17ihYtCsCuXbsoXLgwYPSRTJs2zarXZk7fkeQi9h4eFF/xLY6lSxMyaDC3Nm5MtWz/huX4qMYE4q+/yOqzqxiyexixiSnPS7HI1h5emQjtFxv9Jd80gIv7M3EVmqaZ69ChA+vXr2fNmjV07NiRFStWEBERwfHjxwkICKBo0aLEpjKnLDWenp5UrFiRffv2WanVGaMDSS5jV7AgxZYt5ZmqVQkf/j43V6c+78P/+VKMr/8RcVdasSt4J7229iEyNvLRTlzxDeizE5zyw9LX4LdZOk+XpmWBjh07snr1atavX0+HDh24desWbm5u2Nvbs2vXLoKCgtJVT2hoKHfv3gXg5s2b7N+/n+eee86aTU83HUhyIdu8efFZMJ+8DRty+dOxXPtmXqrPUN+s5cPnzd4mLrwLpyL+xG9TN0Jvhz7aid3KGcGkXEv4dZSxNnxMynNcNE1LH19fX27fvo2XlxceHh74+flx7NgxKlWqxLJly9K90NWZM2eoU6cOVapUoWHDhgwfPpxKlSql69glS5b8Z32T0NBH/I5IhV6PJBdTCQmEf/gRUb/8QsFeb+E2fHiqy/FuPnWJd3/4HmefZbg6OzOv2Tc8V/AR/1pRCo7MM4JJniLQbiEUr5eJK9G07KfXI8kYvR7JE0rs7fH8fAoFunTmxsJFXB49GpWUlGLZFpU8+Kpde2KD+nMrJonum7tz9PLRRzyxQJ1+ptQqDkbSx73TIBcNN9Q0LffQgSSXExsbin7yCYUG9Cdy3XrChg0nOT7lYbpNKxRlfudXiQ0eSGysC/229ePXi78++sk9qxlDhH3bws7x8O0bEH310evTNC1Np06duj+M995Wp06dnG6WRVYNJCLSXETOikigiIy0UK6diCgRqWl67yciAWZbsohUNe3bbarz3j43a15DbiAiuL37Lm4jRnB7yxZCBwwk+c6dFMs2eLYIy/xfISlsIEmx3gzfM5zVf2ciUaNTPuPRVuuZEHwQvn4Bzu9+9Po0LRs9jo/uK1WqREBAwH+2w4cPW/Wcmf05WS2QiIgtMAdoAVQAOovIQ1O2RcQFeBe4/5NSSq1QSlVVSlUFugEXlFIBZof53duvlHpq/kQu1LMHHhMncufgQYLf6kVSZMojtGqXLMia3i9hd7U/xJRn4uGJzPp91qP/YxExZsP32QnOBWBZW9g5QSd+1HI1Jycnrl+//lgGk+yklOL69es4OTk9ch3WnJBYGwhUSp0HEJHVQBvgrwfKjQemAO+nUk9nQOc+N3Ft9wY2+VwIHzqMoG7++CxYgH3Rh2/KKnjm4/v+DfFbaE9k4irm/TGP63evM6ruKOxsHvHXXtQX+u6CTR/A3qlw8QC0WwD5vTJ5VZqW9e6NToqIiMjppuR6Tk5OeHt7P/LxVhu1JSLtgeZKqd6m992AOkqpwWZlqgMfK6XaichuYLhS6tgD9ZwD2iil/jS93w0UApKA74AJKoWLEJG+QF+AYsWK1UjvWO3HxZ1DhwkdOBDbAgUotmghDsWLp1jualQs3RYdJjj5e+wK7aSRTyOmNpiKk92j//UBwMk18MsQsHOE12ZB+VczV5+mablOrh+1JSI2wJfAMAtl6gAx94KIiZ9SqhJQ37R1S+lYpdQ8pVRNpVTNIkWKZGHLc4c8detQbOlSku/c4aJfV2L/TjkhnVs+J9b2fR5f547EXX6N3SF76LetH7fibmWuAVU6Qr894OoDa/zgp3cgPuV+G03TnmzWDCRhgI/Ze2/TZ/e4ABWB3SJyEagL/HSvw92kE7DKvFKlVJjpv7eBlRiP0J5KzpUqUnzlCsTOjqBu/sQcT3kp3fzP2LO8Vx1ecGvD3dDOBFz9g+5buj/aIlnmCpeFXtvhhffgxDIjvUrYicxACX5GAAAgAElEQVTVqWnaY8eageQoUFZESoqIA0ZQ+OneTqXULaVUYaVUCaVUCeAQ8Nq9R1umO5Y3MesfERE7ESlsem0PvAo81QtqOJYqRYmVK7ArVIjgXr2J3rMnxXLODrbM869J6zItiA7qSdCtcLpt6sb5yPOZa4CdAzQbC91/goS7sLAZ7PsCklOe76Jp2pPHaoFEKZUIDAa2AmeAtUqp0yIyTkReS0cVDYCQe531Jo7AVhH5AwjAuMOZn8VNf+zYe3pSfOUKHEuVMpI9/vxLyuVsbfiiQxW6VX2JW+d6cyPmLv6b/Qm4GpBi+Qwp2QAGHIByr8KOcbC0NUSGZL5eTdNyPZ0i5QmSFB1N6ICBxBw7RtFRH1PQzy/FckopZu8M5MvdBylUeinYRfFFw2k09Ek5bX2GKAUnVxnrm4gtvPolVGqf+Xo1Tct2ub6zXct695M9Nm7MlfETiJgzJ8Ux9CLC2y+VZcKrDbkR2BcV58Y7u95lw78bMt8IEajaBfrvgyLPwne94Lveen14TXuC6UDyhLFxdMR75gzyt23LtVmzufLZJFQqObL86hRnYbfGxAb3Qe6WYfRvo1lwakHWTOAqWAp6boFGH8HpDfBVPQjcnvl6NU3LdXQgeQKJnR0en02kYHd/bi5fTvjIkaiEhBTLNn7OjXX9muB4vQ/qdjVmnJjB5COTSVZZkKDR1g4ajYDe2411Tr5tZ8w9iYvOfN2apuUaOpA8ocTGBreRIyny3rtE/fQzoe+8S3Iqq7BV8MzHj4Ma4pPUi4Qb9Vn590pG7B1BfFIWreHuWQ367oF6g+HYYpj7IgQfypq6NU3LcTqQPMFEhML9++M+ZjTRu3cT0rsPSbdvp1jWPb8T6/u/wPMFehJ7pQVbLm5h0PZB3EnIokmG9k7Gkr49NoJKgsUtYNsYSIzLmvo1TcsxOpA8BQp07ozntKnEBAQQ5N+dxFRyD+VxtGNetxp0fs6fu+EdOHT5CD029+Ta3WtZ15gSL8CA36BaVzgwHeY1hsunsq5+TdOynQ4kT4n8rVrh8/VXxF+8yMUufsQHB6dYzs7WhrGv+fJh/W7cDfHn7I1z+G3sRkhUFs4JcXQx8nN1WQsx14xgsneaziasaY8pHUieInnr16f4ksUk377Nxc5diP3rwUTMBhGh14slmfO6Hwmhfbh0+wadN3blzPUzWdugZ1+BgYegXCtj4ayFTeHK6aw9h6ZpVqcDyVPGuUoVIz+XgwNB3fy5cyj1Tu9XfN1Z06MTThHvEnknmW6benD4UhYvsPNMQeiwBNovMmbCf9MQdk2CxCzq6Nc0zep0IHkKOZYqRYlVK7H39CCkT1+itmxNtWxlb1d+GfAGpRJHEhPjQt9f+7P5/JasbZAIVGwHg44Yy/rumQzzGkFYykkoNU3LXXQgeUrZu7tTfPlynCpWJGzIEG6uWpVqWTcXJ9b1bk6LQuOJj/Hmg70fsPjUt1nfqDyFjIWyOq+GuzdgQVP49RMjGaSmabmWDiRPMVtXV4otWkjehg25PHYcETNTX47X0c6Wae3q8n7laSRFl+fLE1MYd+AL6yxj+lwLo++kWlf4baaxTnzQb1l/Hk3TsoQOJE85G2dnvGfPIv8bb3Dtq6+4/OlYVFLKKeBFhLdeeI75LWYi0XVYF7iEPptGkJhshdFWzq7GyC7/HyE50Zh3snE4xKU8D0bTtJyjA4lmpFSZOIFCfXoTuWYNYe8NITku9YmCL5QuysbOM8gf14LD1zbTem0fouNirNO4Uo1g4EGoMwCOLjDl7NphnXNpmvZIdCDRAONuw23YMNxGjuD2tm2E9Omb6ix4AJ+CedjecxIVHXsQEnucl1b6ceFGyhMdM80hD7SYDG9tBXtn+PYN+GGgziisabmEDiTafxTq0QPPqZ8Tc+IEQd38U50FD+Bkb8uqTsPo4DOSO1ygzfd+bP/nrPUaV6wO9NsH9YfBydUwpw6c+dl659M0LV10INEekr91a2MWfFCQMQs+KMhi+TEvdeGTWl+ibG/w7t7efLFrn3U64cHI2fXSaOi7C/K6wZqusK4HRFvpbkjTtDTpQKKl6D+z4Lv4cfe05RnnHSs2YdEri3G0T2bx+eF0X7me6DgrpjzxqAJ9dkGTUfD3RphTG/5Ya6zQqGlattKBREvV/Vnwjg4E+3e3OAseoJZnJTa0XUV+p3yciJ/Ey3Pncir0lvUaaGsPDd43HncVKg3f94GVHeFWmPXOqWnaQ6waSESkuYicFZFAERlpoVw7EVEiUtP0voSI3BWRANM216xsDRE5ZapzpoiINa/haWfMgl9lNgve8qz24vmL8eMbqyieryS388+n/bczWbj/gvUedQG4lTM64l+ZBBf2Gn0nh7+B5JSHMWualrWsFkhExBaYA7QAKgCdRaRCCuVcgHeBB5M4nVNKVTVt/c0+/xroA5Q1bc2t0X7t/9kXLUrxb7/FqVIlwoYM5cbKlRbLF3YuzNrXllGjaE0cPNYy+beveWvpUa5HW3HtERtbqDfQGCrsUxs2f2DMjL/0h/XOqWkaYN07ktpAoFLqvFIqHlgNtEmh3HhgCpDy8n1mRMQDyKeUOqSMP3GXAW2zsM1aKmzz56fYwgXkbdSIK+PGW5wFD5DHPg/zXv6a5iWa41R0M4duLqHFzD0cPHfdug0tWBK6fgftFsKtUCNn19aP9fK+mmZF1gwkXoD5Ihahps/uE5HqgI9SamMKx5cUkd9FZI+I1DerM9RSnWZ19xWRYyJyLMLCEFYt/WycnfGeNTNds+ABHGwdmNJgCl3KdcGu4D6SC62iy4L9fPnrWRKTsmBN+NSIQKX2MPgIVPeHg7Phq7pwNouTTWqaBuRgZ7uI2ABfAsNS2H0JKKaUqgYMBVaKSL6M1K+UmqeUqqmUqlmkSJHMN1gDzGfB90nXLHgbsWFk7ZG8W/1d4pyOUbzCGmbuOk3n+YcIi7RyMkbnAtB6utF/4pAXVnWEtf4Qdcm659W0p4w1A0kY4GP23tv02T0uQEVgt4hcBOoCP4lITaVUnFLqOoBS6jhwDnjWdLy3hTq1bGDMgh9K0Q9HGrPgLawFf69870q9Gff8OCLVX5SrtoK/roTTcsY+tp6+bP0GF6sL/fYa80/+2Qqza8GR+bozXtOyiDUDyVGgrIiUFBEHoBPw072dSqlbSqnCSqkSSqkSwCHgNaXUMREpYuqsR0RKYXSqn1dKXQKiRKSuabSWP/CjFa9Bs6Bg9+54Tp1KzO+/E9TNn4SrVy2Wf73s60xvPJ3r8cF4VliAR+Hb9Ft+nE9++JPYBCt/qds5GDPiBx4E75qwaTgsbKbXi9e0LJCuQCIi74pIPjEsFJETIvKypWOUUonAYGArcAZYq5Q6LSLjROS1NE7ZAPhDRAKA9UB/pdQN076BwAIgEONOZXN6rkGzjvytX8Xn66+JDw4mKB2z4Bv5NGLhKwuJT75LdMH/0bZuHMsPBfHqrP3WnXNyT8FS0G0DvLEAIoONFRl/HQXxd6x/bk17Qkl6xveLyEmlVBUReQXoB3wCLFdKVbd2A7NCzZo11bFjx3K6GU+0u3/8QUjffmBjg8/8eTj7+losH3I7hIHbBxIeHU7X0iNYs7sQ16LjeLtJWQY2Lo29bTZ038XcgO2fwomlkL8YtPoCnrX495GmPVVE5LhSqmZa5dL7f+u9SX8tMQLIabPPNA3nypUpvnIl4uRIcDd/7hw8aLG8j4sPy1ssp2Lhiiz6dzz+zc/TqpI7/9v+D+2+/o3Aq9kwXPeZgvDaTOi5BRyegZUdYG133RmvaRmU3kByXER+xQgkW02TCK04flN7HDmWKmnMgvfyIqRvP6I2W37q6OrkyryX59G8RHO+PjWTwiU2MqtLFUJuxNBq5j4W7b9AcnI25M4qXs9Is9JkFJzdbOTt0p3xmpZu6X20ZQNUxejwjhSRgoC3UuqxmDasH21lr6RbtwgZMJC7v/9O0VEfU9DPz2L5ZJXMjBMzWPTnIhp4N+CD6uMZ9+M5dvx9lXqlCjHtzSp4uTpnT+Ovn4NfhsCFPeBV03jc5Vk1e86tablMeh9tpTeQvAAEKKXuiEhXoDowQylluWc1l9CBJPslx8YSNmQo0bt2UahPb4oMGYLYWL4BXnt2LRMPT+S5As8xu8lsdv0Vy7if/8JGhDGv+dKuuhfZklpNKSOT8K8fw51rUKuXcbfiXMD659a0XCSr+0i+BmJEpArGBMJzGOlJNC1FNk5OeM+aiWvHjlyfv4CwYcMsTlwEePO5N5nVZBYXoy7SdXNXapSJZ8t7DSjvkY/h607Se+kxLt9KM5NO5olAlY4w+BjU6QfHFsGsGnBiOSTrJ7qa9qD0BpJEU26rNsBspdQcjAmFmpYqsbPD/dMxuL0/nNubtxDcoyeJNy0vj9vAuwFLmi8hITkB/83+XIr7k1V96zKqVXkOnLtGsy/3sOpIsHWzCd/j7AotphiTGQuVhZ8Gw6KXITzA+ufWtMdIegPJbRH5EOgGbDT1mdhbr1nak0JEKNSrF17T/0fs6dNc7NSJ+IsXLR5ToVAFVrZcSdE8Rem3vR+bLvxC7/ql2PJuA3y98vHh96fwW3CY4Osx2XMR7pXgrS3Qdi7cvGgkgtw4TK8Zr2km6e0jcQe6AEeVUvtEpBjQSCn1WDze0n0kuUPMid8JHTQIlML7qzk8U93yNKSo+CiG7BrCkctHGFR1EP0q90MpWHU0mEmb/iYpWTH8lefo8XwJbG2yaTT63UjYPQmOzDP6TJqOhap+kEb/j6Y9jrK0s91UYVGgluntEaWU5XwYuYgOJLlHfHAwIX36knDpEp6TJ5GvZUuL5ROSEhjz2xh+Pv8zbcu0ZXS90djb2BMeeZePN5xi19kIqhdz5fP2lSnjlo1PWy+fgo3DIeQQeNeCltP06C7tiZOlne0i8iZwBOgAvAkcFpH2mWui9jRyKFaM4qtXGYtkDR3GtfnzLfZ32NvaM/HFifSv0p8fAn9g0PZBRMdH4+nqzKIetZjesSrnr92h5Yz9zN75LwnWTE9vTj/u0rT70p0iBWh27y5ERIoA25VSVazcviyh70hyn+S4OC599DFRGzfi2qED7qM/Qewtd7tt+HcD4w6Oo0T+EsxsMhMfFyO59LXoOMb8dJqNf1yigkc+Pm9fmYpe+bPjMgwPPu5qMgqqdzdWbdS0x1hWD/+1eeBR1vUMHKtpD7FxdMRz6ucU6t+PyHXrCBkwkKRoy2lRXi/7OnObzeVqzFW6bOzC0ctHASic15E5XarzTbcaRETH0WbOASZtPkNMfGJ2XMp/R3cVfs6Y0PhNA2P9eE17CqQ3GGwRka0i0kNEegAbgU3Wa5b2NBAbG9zeew+PCeO5c/AgQV38SLhseX2SOh51WNVqFQWcCtD3175898939/e94uvO9iENaVfdi2/2nKfpF3vY8uel7BkqDMbjrp6boMMSiI2Cpa1htR/cuJA959e0HJKRzvZ2wAumt/uUUhus1qosph9t5X7RBw4Q9u572DzzDD7fzMWpfHmL5aPio/hgzwccCD9A1/JdGVZzGHY2dvf3H7t4g1E//Mnfl2/T8NkijH3NlxKF81j7Mv5fwl1jid99/4PkBKg7EBoMB0c9/Up7fGT5qK3HmQ4kj4fYs/8Q0r8/ybdu4fnFNFwaN7ZYPjE5kS+OfcG3Z77lBc8XmNpwKi4O//9FnZiUzLKDQXy57R/ik5Lp37A0AxuVxsk+G/suoi7BjrFwchXkcTNWaazaRfefaI+FLAkkInIbSKmAAEoplaF11HOKDiSPj4QrVwkdOJDYv/7CbdhQCvbqlWZ+rfX/rGfioYn45PNhdpPZFMtX7D/7r0bFMnHTGX4MCMenoDNjX/OlSbmi1ryMh4Uehy0jIfQIuFc2+lSKP5+9bdC0DNJ3JGZ0IHm8JN+9y6WPPyZq02byt3kN93HjsHF0tHjM0ctHGbp7KMkqmS8bfUkdjzoPlfkt8Bqf/Pgn5yLu0KxCUUa/WgGfgs9Y6zIephT8+R1sGw1RYVChLTQbBwWKZ18bNC0DdCAxowPJ40cpxfW5c4mYMROnypXxnj0Lezc3i8eE3A7h7R1vczHqIiNrj6Tjcx0fupuJT0xm4f4LzNzxLwrF4MZl6NOgFI522fioKT4GfpsJ+6eDSobn34YXh4Bj3uxrg6algw4kZnQgeXxFbdtG+IiR2Lq44D17Ns6VKlosHx0fzYh9I9gbupc3yr7Bx3U+xsHW4aFyYZF3mfDLX2z+8zKlCudhbBtf6pctYq3LSNmtUGOp31PrwMXDmH9SpbPuP9FyjayeR/KojWguImdFJFBERloo105ElIjUNL1vJiLHReSU6b9NzMruNtUZYNos/5mqPdbyNWtGiVUrEVtbgrp25dbGjRbL53XIy6wms+hbuS/f//s9Pbf05MqdKw+V83J15uuuNVjSsxbJStFt4REGrTjBpVt3rXUpD8vvDe0WQK9tkM8TfhxkzD8J3J59bdC0LGC1OxIRsQX+AZoBocBRoLNS6q8HyrlgzEtxAAYrpY6JSDXgilIqXEQqAluVUl6m8ruB4UqpdN9i6DuSx1/i9euEvvMud48fp1C/fhR59500F8raHrSdj/d/jLOdM182+pLqRVNOEhmbkMS8veeZsysQWxvhvaZl6flCSexts3HOrVJw+nvYPhYig6BUY6P/xKNy9rVB0x6QG+5IagOBSqnzSql4YDXGeiYPGg9MAe6vWKSU+l0pFW56expwFhHLva3aE82uUCGKL15E/vbtuP7NN4S+/Q5J0XcsHtO0eFNWtlpJXoe89Nrai9V/r05xcqKTvS3vvFSWbUMaUq9UIT7b9DetZu7j0Pnr1rqch4lAxXYw+Ci8MgkuBRh3Jxv6Q2RI9rVD0x6BNQOJF2D+f0Co6bP7RKQ64KOUsvS8oh1wQillvrzeYtNjrU8klbGhItJXRI6JyLGIiIhHvAQtNxEHBzzGj6foRx8RvWsXQV26EB8aavGY0q6lWdlqJfU86zHx8ETG/DaGuKSUV2osVugZFvaoxXz/mtyJS6LTvEMMWRPA1dvZsCrjPXaOUG8gvBMAL7wDf35vrM64bQzE3sq+dmhaBuRYvizT4lhfYizdm1oZX4y7lX5mH/sppSoB9U1bt5SOVUrNU0rVVErVLFIkmztRNasREQr6d8Nn3jwSLl/mYrv2RO/bZ/GYfA75mP3SbPpW7suGwA302NyDsOiwVMs3q1CU7UMbMqhxaX75I5wm0/bw1e5AYhOSsvpyUufsajzaevs4+L4OB2bAjKpw6GtIjM++dmhaOlgzkIQBPmbvvU2f3eMCVAR2i8hFoC7wk1mHuzewAfBXSp27d5BSKsz039vASoxHaNpTJu+LL1By3VrsihYlpG8/ImbPQVlYT91GbHi72ttMbzydi1EXefPnN9kdsjvV8s4Otrz/Sjm2vNeAuqUK8vmWszSZtpsNv4eSnJyNIx1dfeCNb6DfHiOX15aRMLsmnFwDydkY2DTNAmt2ttthdLa/hBFAjgJdlFKnUym/G1Mnuoi4AnuAsUqp7x+o01UpdU1E7IFVGOns51pqi+5sf3Il373LpTFjiPrpZ/I0qI/X559j6+pq8ZiQqBCG7RnGmRtn6Onbk7erv429jeUU9r+du8Znm87wZ1gUFb3y8VHL8jxfunBWXkralILAHbDjU2NhLbcK0OQTeK6F0ceiaVksV8wjEZGWwHTAFliklJooIuOAY0qpnx4ou5v/DySjgA+Bf82KvAzcAfZirBdvC2wHhiqlLP5ppgPJk00pxc1Vq7gyaTL2bm54zZyBs6+vxWPikuKYenQqa86uoZpbNT5v8DnuedwtHpOcrPjxZBhTt5wl/FYsL5Vz48OW5bJ3ZUajIfDXBtg5EW6cA+/aRg6vkvWztx3aEy9XBJLcQgeSp8PdgABC3xtC0o0buI8ZjWu7dmkes+n8Jj49+ClOtk5Mrj+Z573Szn8Vm5DE4gMX+WpXIDEJSXSq5cN7TZ+liEs2DyxMSoCAFbB7CtwOh9JNjIDiWS1726E9sXQgMaMDydMj8cYNwoYOI+bQIVw7tKfoqFFp5uk6f+s8w3YP41zkOfpW7suAKgOwTcfs8uvRcczc8S8rDgfjaGdD/4al6V2/FM4O2TwzPeEuHJkP+780lvqt0NaYJV+4bPa2Q3vi6EBiRgeSp4tKTCRixkyuz5+Pk68vXjNm4ODtZfGYu4l3mXR4EhsCN1DbvTZTGkyhsHP6+kDOR0QzZcvfbD19Bfd8Tgx7+VneqO6NrU0291vE3oLfZsPBOZAYa6SrbzTSmEGvaY9ABxIzOpA8nW7v2EH4iJFga4vnpM9wadIkzWN+CPyBiYcmktchL583+Jxa7rXSfb4jF24wceNfnAy9RXmPfHzUslz25+8CiI6AfV/AsYWAQK3eUH8o5MnmwQHaY08HEjM6kDy94oOCCBsylNi//qJgd3/chg1DHB5O4mju35v/MnT3UIJvBzOwykB6V+qdrkddYHTI/3LqEp9v+ZvQm3epX7YwH7xSjkre+bPicjImMtjoPzm5EuyfgXqDoO4AcC6Q/W3RHks6kJjRgeTplhwfz9XPp3Lz229xqlQJr/99iYO35cc9MQkxjD04lk0XNlGzaE0m1Z+U5qguc3GJSSw/GMScXYHcjEmgVWUPhjV7llJFciBVfMRZ2DkBzvwEDi5Qpy/UHQR5CmV/W7THig4kZnQg0QCifv2VSx+PAsBjwgTyvfKyxfJKKX489yOfHf4Mext7xtQbw8slLB/z0DljE1iw9zwL9l8gLjGZdtW9eLtJ2exdUOuey6dg7zT460fjDqVWL2MtlLw6gbaWMh1IzOhAot0THxpqPOo6dYoCXTrj9sEH2Dg5WTwmOCqYkftGcuraKdqWacuHtT/kGfuMBYKI23HM2RXIyiPBJCcrOtT0ZlDjMngXyIGAcvVv2DfNWK3R1hFq9DDyeuXzzP62aLmaDiRmdCDRzKn4eK5++T9uLFmCQ5nSeE2bhlO5chaPSUhO4OuAr1lwagE+Lj5MaTCFioUtL7KVksu3YvlqdyCrj4SgUHSs5cOgxmXwyO/8qJfz6K4FGkOGT642FtOq1s1YqdHVJ+1jtaeCDiRmdCDRUhK9bz/hH31IcuQtigwbSkF//zTXODl2+Rgf7v+QazHXGFRtED19e6a7I95ceORd5uwKZO2xEAShU20fBjYqg3t+y3dHVnHjAuz/HwSsNN5X7QwvDoWCJbO/LVquogOJGR1ItNQk3rjBpVGfEL1zJ3mefx6PSZOwL2q5z+BW3C3GHxrP1otbqVm0JhNenIBXXsvzVFITejOGObsCWXcsFBsboUvtYgxsVBq3fDkQUCJDjCzDJ5ZBciJU7gj1h0HhMtnfFi1X0IHEjA4kmiVKKSLXrOXK5MnYODnhMWE8Lk2bpnnMj+d+ZNLhSQAMqzmMDs92IJXlcdIUciOGWTv/5bsTYdjZCF3rFqd/w9LZn3YFIOoS/DYTji2GpDjwfQMaDAe38tnfFi1H6UBiRgcSLT3izp8nbPhw4v46g2uHDhQdOQKbPHksHhMWHcaYA2M4fPkw9TzqMfb5sXjk9XjkNgRdv8PMHYFs+D0UBzsb/OuVoG+DUhTOmwMBJToCDs6CIwsgIQbKt4YG7+vlf58iOpCY0YFESy8VH0/EzJlcX7gIey8vPD6bSJ7alpe8SVbJrDu7ji+Of4GN2PBBrQ94vczrj3x3AkbalVk7A/kxIAwHOxs61SpGnwal8HLNgU75mBtw6Cs4/A3ERUGZpsaw4ZINdfr6J5wOJGZ0INEyKub4ccI//IiE4GAK+HfDbcgQbJwtf4mH3g5l9G+jOXr5KC94vcCn9T7N0CTGlJyLiGbu7nNs+N1YE+71al70b1Sa0jkxsfFuJBydD4fnwZ2r4F4Znn8HfNuCreX1XLTHkw4kZnQg0R5FckwMV6d9wc2VK3EoUQKPSZ/xTDXLKdqTVTKr/17N9BPTsRM73q/1Pm3LtM3U3QlAWORd5u89z+qjwcQlJtOyogcDGpWmolcOpF5JiIVTa+G3WXDtH8jnbaReqe4PTvmyvz2a1ehAYkYHEi0z7hw8SPjHH5N4+QqFer1F4bffxiaNfF0hUSGMOjCKE1dPUN+rPqPrjc703QnAteg4Fh+4wLLfgrgdl0iDZ4swoGFp6pYqmOlglWHJyRC4DQ7MhKD94JjPmNxYd4Ce3PiE0IHEjA4kWmYlRUdzdcoUItetx7FsGTzGj8e5alWLxySrZFb9vYrpx6dja2PL29XeptNznR5p3smDomITWH4wiMUHLnAtOp4qPq4MaFiKlyu4Y5Pd6esBwo4bKez/+gHEBip1gHqDwT3jkza13EMHEjM6kGhZJXrvXi6NHkPilSsU8POjyHvvYZvX8siu0NuhTDg0gQPhB6hUuBJj6o3huYLPZUl7YhOSWH88lHl7zxN8I4ZShfPQr2Ep2lbzwtEumxfYArh5EQ7NNeaiJNwxVm18/m0o1Vh3zD+GdCAxowOJlpWSoqOJ+N90bq5ciZ27O+5jRuPSqJHFY5RSbLqwic+Pfs6tuFv4+/ozoMoAnO2yZhRWYlIym/+8zNw95zgdHkURF0f86xbHr25xCuax/BjOKmJuwPHFxkiv6CtQpLyRdbhyR3CwHHi13CNXBBIRaQ7MAGyBBUqpyamUawesB2oppY6ZPvu/9u4zvKrrzvf4d6n3XpGQkAQ6VCHTiymm2BhccBIH7Az2jJ3xYxs/Tu5M4iQ3icfXc2cmmRl77Mnk4iTEk7gFx90GbA9gU00TRoAkkEBCAgnUezt13Rd7AweCRJF0jgT/z/Och3P22XtrLTbwY6+19lo/AR4FnMDTWuvPr+Wc7iRIxEDoPHiQ6mefxXr8BBFLl5L4v3+CX1zvi0e1WFt4Ie8FPjjxASlhKfx8xs+ZnTK738qktWbniWKjE68AACAASURBVHp+t+Mk20vqCPTz4b5bUvib2RlYksL77edcNYcVjrwLe1+B6sMQFGnM6TX1uzIFyxDg9SBRSvkCJcBioBLYDzygtS66ZL9wYAMQADyltc5TSo0F/gRMA4YBm4Fs85ArnvNSEiRioGibjfq1a2lY8woqJITEZ54h8htXfoZkf/V+nt/9POWt5SzNWMoPp/7wqpf2vVrHa9r476/Kef/rSrrtLuaMiuOR2RnMy473fD+K1nBqD+z7DRR9DNoF2UuMuxRp9hq0BkOQzASe01rfYX7+CYDW+l8u2e8lYBPwQ+AHZpBctK9S6nPgOfOQK57zUhIkYqBZy8o4+/Nn6TpwgJAZM0h69lkCM3v/H7fVaWXtkbWsPbKWIN8gVueuZuXolfj5+PVr2Zo6bLy17xSv7S6nptVKZnwofz1rBN+YlEpYYP/+rKvSegbyXjWmYOmshzgLTPtbmPgABHrh+RjRo6sNkt6nOu2bFOC02+dKc9t5SqlJwHCt9YarPPaK53Q792NKqTylVF5dXd311UCIqxSYmUn666+R9NxzdBcWUnbvvdS+8AKujo6ej/ENZHXuat675z0mxE3gl/t/yf2f3M/+6v39Wrbo0ABW3zaSnT9awMsrcwkP8ufZjwqZ+c9beP6TIioaei7jgIgYBgt+Bv+rEJa/AgEhsPEH8OIY+PTHxvT2YkgZyCDplVLKB3gR+PuBOL/W+rda6yla6ynx8fED8SOEuIjy8SF65QqyPt1I5LJlNPxuLaXL7qL100/p7c4/MzKT3yz+DS/Nf4lOeyePfP4Iz2x7huqO6n4tn7+vD/fmpvDR6tl88OQsFoxJ4LXd5cz/961894/72V5Sh9PlwcE3/kHGlPV/+yU8uhmy7zCenP+vyfCHu4y+FXu358ojrpvXmraUUpFAKdBuHpIENAL3YPSBSNOWGNI6vz5I9T/+I9ajRwmZOYOkn/2MwKysXo/pdnTzasGrvFrwKj7Kh8dyHuOhsQ8R4DswI69qWrt5c08Fb+49RUOHjeTIIL4zPY0HpqUR642JItuqIf9NOPBHaK6A4GiY+CBMfhji+2fItLh6g6GPxA+jY3whUIXRMf6g1rqwh/23cqGPZBzwFhc627cAowB1Lec8R4JEeIt2Oml6+23qXnoZV2cnMQ89RNyTT+Ab1ntfQGVbJf+2/9/44vQXpEek88zUZ5iTMmfAnl7vtjvZVFTDn/NOs+N4PQF+PiybkMx3pqcxOT3aO0/Nn9wGB/4AxzaAyw5pM2HSw8bcXv5emLzyJuT1IDELsRR4CWOo7qta639SSj0P5GmtP75k362YQWJ+/inwCOAAvq+1/rSnc16pHBIkwtscjY3UvvgiLe++h29sLPHfe5qob34T5dv7Q4O7qnbxi32/oLy1nOlJ0/n7KX/PmNiBXRfkRG0br+2u4IOvq2izOshODOM709O5b1IKEUFemJyxvQ4OvWWESmOZMYQ4Z6Vxl5I4zvPluYkMiiAZLCRIxGDRdeQINb/4JV0HDhCYnU3ij39E6KxZvR5jd9l5p/gd1hxaQ4u1hbsy7+LpSU/3y9xdvem0Ofjk0Bne3HuKw5UtBPv7cvfEZFZMTWNSWpTn71K0hvKdRqAc/RicNkidatyljP+GPOg4ACRI3EiQiMFEa03b5/9D7b//O/bKSsLmzSPhmR9esf+kzdbG2iNreaPoDZRSrBq7ikfHP0pYwMAPmT1S2cJb+yr4KP8MnTYnIxPCWDFlOPdNSvHOolsdDXB4ndGXUl8MAWEw9l7jyfkRc8DHa+OIbigSJG4kSMRg5LLZaHr9DerXrMHV1UX0ypXEPbUav+joXo87036GXx38FevL1hMTFMMTE5/gm9nfxN9n4Jud2q0ONhw+w9v7T/P1qWb8fBQLxyTw7SnDmZcdj5+vh/8BP/egY/4bUPgR2NqMae1z7jeavxJGe7Y8NxgJEjcSJGIwczQ2Uv9f/0XT23/GJyiImEcfIfbhh6+4zG9hQyEv5L3A/ur9pIWn8fjEx1masbRfZhe+Gsdr2njnQCXvf11JfbuN+PBAvnFLCvdPSWVkghemY7F1QvFGOPw2nNgC2gnJucaDjuO/CWHyGMC1kiBxI0EihgJraSl1L71M26ZN+MbGEvf440Sv+Daql7VPtNZsr9zOrw7+iuKmYjIjM3ki9wluT78dH+WZuwO708UXx2p5J6+SL4trcbo0ucOjuH9KKndPHOalDvpa4zmUQ38y5vhSvsYSwRNXguVOGfV1lSRI3EiQiKGk69Ahal94kc59+/BPSSH+e08TsWxZryO8XNrF5orN/Dr/15S1lJEdnc2TuU+yYPgCj3aK17VZ+fBgFe8cOE1JTTuBfj4sGpPI3ROTmW9JIMjfC1Pb1xQZ/SmH34G2M8YCXOOWG01faTOlP6UXEiRuJEjEUKO1pmPXV9S++ALWoqMEZmcT//3vEXbbbb0Gg9Pl5LPyz1hzaA0VrRWMjR3L6tzVA/oMSk/lP1LVwrsHKtl45Cz17TZCA3xZPDaRu3KGMSc7zvPrpbicUL4DDq0zJo60d0BUmtFBP+4bkDBGJo+8hASJGwkSMVRpl4u2zz6j9uWXsVecInDsGOJXryZsQe93Gg6Xg/Vl63nl0CtUtVeRE5/DU7lPMSN5hseH7TqcLvaUNbL+8Bk+K6ymudNORJAfd4xL4q6Jw5iVFYu/pzvpbR1wdL1xp1K21ZiNOC4bxi437lYSxkqoIEFyEQkSMdRph4OWT9ZTv2YN9lOnCBwzhrgnnyB84UJUL00zdqedD0s/5LeHf0t1RzWTEyezOnc1U5OmerD07uVxsfNEPZ8cOsOmwhrarA5iQgNYMj6Ju3KSmZ4Ri6+np7hvrzWeSyn8ECp2GaESO8oIlLHLjYceb9JQkSBxI0EibhTa4aBlvRkoFacItFiIW/0k4YsW9RooNqeNd0veZe2RtdR11XFLwi18d8J3Pd7k5a7b7mR7SR2fHD7LlqM1dNqcxIcHsmxCMnflJDMpLdrz66a018LRT4y158t3mqEy8sKdSuL4mypUJEjcSJCIG412OGjdsIH6Na9gKy8nMDvbuENZvLjXTvluRzfvH3+fPxT+gbMdZ7FEW3h0wqPcnn67x4YNX06XzckXx2pZf/gMXxyrxepwkRwZxLIJydw9cRg5qZGeD7z2Ojj2iXGnUr7DCJWYLOPBx3HLISnnhg8VCRI3EiTiRqWdTlo3bqT+/63BdvIk/ulpxP7NI0TetxyfwJ6fOLe77Gws28jvC37PyZaTDA8fziPjH+GerHsGbKbhq9VudbC5qIb1h8+wraQOu1OTFhPCshzjTmVscoTnQ6Wj/sKdyskdxjMqkWnGUGLLnZA+G/y8+/s2ECRI3EiQiBuddjpp27SZhrVr6S4owDcujphVq4h+YCW+ERE9HufSLr449QW/O/I7ihqKiA+O54HRD3B/9v1EBUV5sAaX19Jp5/OiatYfPsuuE/U4XZrM+FDuyhnG3TnJjEr0woOPHQ1wbD0Uf2p01Du6jCHFIxeCZanxvEpIjOfLNQAkSNxIkIibhdaazr37aFi7lo6dO/EJDSVqxQpiHn4I/8TEXo/bfXY3/13w3+w5u4cg3yDuzrqbvxr7V2RGZnqwBj1raLfyWWE16w+dZc/JBrSGUQlhLBidwG2jE5icHu2F0V+dxnT3xRuh+DPoqDUefkybeeFuJbb3OdQGMwkSNxIk4mbUffQoDWt/T+unn4KPDxFLlhDz0CqCc3J6Pa6kqYQ3it5gQ9kGbC4bt6bcyqqxq5iZPNNrHfOXqm3tZuORs2w6WsO+k43YnZrwQD/mZMdxmyWBeZZ4EsKDPFsolwvOHDRD5VOoNZdJiss2Vn8cdTsMnzGkmsAkSNxIkIibma2ykqbX36D53XdxdXQQnJtLzEOrjI55/56nL2noauDPJX9m3bF1NHY3MjJqJKvGrmJpxlKC/Dz8j3Qv2q0Odh6vZ2txLV8W11LTagVgQkokt1niuW10AjmpUZ4fVtxUASWfGcFSvstYnCsgHLLmG6EycpGxfv0gJkHiRoJECHC2d9DywQc0vv469lOn8EtKIvrBB4m6/1u9zjhsc9rYeHIjrxe9TklTCREBEdw38j6+bfk2aRFpHqzBlWmtKTrbytbiOr48VsvXp5pwaYgJDWBethEqc0fFERXi4bsCaxuc3A7H/weOb4LWKmN74gQYtdgIldSpg+5uRYLEjQSJEBdol4v2bdtofO01OnfvQQUGEnHnnUSt+DbBubk9Nl9prcmryePt4rfZUrEFh3Ywa9gsVlhWMDd1Ln4+fh6uyZU1d9rYVlLH1uI6thbX0tRpx0fBpLRobhudwG2WBMYkh3u2yU5rqD16IVRO7TZGgfmHGKO/Mucbr4SxXp8HTILEjQSJEJfXXVJC01tv0frxJ7g6Owm0WIheuYKIu+/udV35us463jv+Hu+UvENtZy1JoUncn30/y0cuJyEkwYM1uHpOl+ZQZTNbj9XyRXEtBVWtACRFBHHb6HjmWxKYPTKOsEAPB2J3i/HwY9lW41VfYmwPjYeMeReCJWq4Z8uFBMlFJEiE6J2zvYPWDRtoensd1qKjqJAQIpctI2rFCoLH97wuusPlYOvprawrXsfes3vxVb7cmnIr9426j7mpcz2y2Nb1qm3tZmuJcaeyo6SeNqsDf1/FtIwYbrMYI8Ey40I9P8CgpcoYCXYuWNprjO0xWRdCJWMOBPe+AFp/GBRBopRaArwM+AJrtda/uOT7x4HVgBNoBx7TWhcppb4D/NBt1xxgktY6Xym1FUgGuszvbtda1/ZWDgkSIa6O1pruggKa1q2jdcNGdHc3QePHE7Xi20TcuRTfsJ4X2ypvKefDEx/ycenH1HXVERsUyz1Z97B81PJBM4S4J3ani7zyJrYW1/LFsVqO17YDkBYTwuyRsczINF6JER4eZKA11B27ECrlO8HWDsrHWLQrcz5kzoPUaRAQ0u8/3utBopTyBUqAxUAlsB94QGtd5LZPhNa61Xx/D/Ck1nrJJeeZAHyotc4yP28FfqC1vupkkCAR4to5W1tp+fgTmt9eh/X4CVRwMOGLFxG1fDkhM2b0OLeXw+VgV9Uu3j/+Ptsrt+PQDnLjc7lv1H3cMeIOQv17X/lxMDjd2MnWkjq2Fdey92Qjbd0OADLiQpmRGeO9YHHaoerAhWCp3A8uB/j4Q8oko49lxGwYPh0C+/6w5mAIkpnAc1rrO8zPPwHQWv9LD/s/ADyktb7zku3/bBymf2p+3ooEiRAeo7WmKz+flg8/onXjRlxtbfglJxN5zz1ELr+XwIyMHo+t76pnfel63j/xPidbThLoG8j84fNZmrGUOSlz8PcdvE1f5zhdmqNnW9lT1sCesoaLgiUzLpTpmbHnw8XjwWJtg4rdxqzFFbuM51hcDuOhyOSJRqjMePK6hxkPhiD5FrBEa/1d8/MqYLrW+qlL9lsN/B0QACzQWh+/5PtS4F6tdYH5eSsQi9Ec9h7wf/VlKqGUegx4DCAtLW1yRUVF/1ZQiJuQy2qlfcsWmj/8kI6du8DlIjg3l8jly4lYcge+UZefVkVrzaG6Q2wo28Dn5Z/TZG0iIiCCxemLWZa5jMmJkz22NHBfDe5gaYfKfcZzKxW7jLuXp/MhMuW6TjdkgsRt/weBO7TWD7ttm47RtzLBbVuK1rpKKRWOESRvaK1f660sckciRP+z19TSuv4TWj78EOvxE+DvT9js2UQsW0r4ggX4hF6+CcvusrPnzB42ntzIllNb6HJ0kRiSyNKMpSzNXIol2jJonqC/GlcKlqkjYpg8Ipop6dFkeLrz3t4N/tcfZoMhSK61acsHaNJaR7pt+w+gTmv9zz0c89fAlJ7C6RwJEiEGjtaa7qIiWjdspHXjRhzV1aigIMLmzydi6Z2EzZvX40zEnfZOtp7eysaTG9lVtQuHdpAVmcXSzKXcOeJOhkd4fshrX7kHy+7SBvIqmmjpsgMQGxrA5PRopoyIZnJ6DONTIjy/5PA1GAxB4ofR2b4QqMLobH9Qa13ots+oc01ZSqm7gX84V2gzWE4Dc7TWZW7njNJa1yul/IE/AZu11q/0VhYJEiE8Q7tcdB08aITKZ5/hbGzEJzSU8EWLCF9yB6GzZvUYKk3dTWyq2MSGsg18Xfs1AJZoCwvTF7IobREjo0YOqTuVc1wuTWldO3kVTeSVN3GgopHyhk4AAvx8mJgayeT0GCanR5M7PIr48J6n//c0rweJWYilwEsYw39f1Vr/k1LqeSBPa/2xUuplYBFgB5qAp84FjVJqPvALrfUMt/OFAtsBf/Ocm4G/01o7eyuHBIkQnqcdDjr27qV1w0baNm3C1daGT0gIofPmEr5oEWHz5vX40OOZ9jNsrtjMllNbOFh7EI0mPSKdhWlGqIyLGzdk+lQup67NyoEKI1TyKpooqGrB7jT+LU6JCmbi8Ehyh0cxMTWK8SmRhHr6IUnToAiSwUKCRAjv0jYbHXv30rZpM21btuBsaED5+xMyayYRixcTtmABfjGXX8OjvqueL059wZZTW9h3dh8O7SAxJJGFaQtZkLaASQmThsTor950250Unmnh4KlmDlW2cOh0M6cajbsWHwXZieFMTI1iQmokE1IisSSFE+Q/8E1iEiRuJEiEGDy000lXfr4RKps2Ya+qAh8fQiZNImzhQsLmzSMgY8Rlm7FarC1sr9zO5orN7DqzC6vTSqh/KDOTZzIndQ63ptw6aKdouVYN7VYOV7Zw8HQzh043c6iymeZOo6/Fz0eRnRhOTmok41OMcBmdHN7v/S0SJG4kSIQYnLTWWI8dOx8q1uPG6H//tDTC5s0jbO5cQqZNvWy/Sqe9k71n97KjagfbK7dT02lMJTImZgy3ptzK3NS5TIib4NW16PuT1prKpi4Kqlo44vZyDxdLUjgTUvovXCRI3EiQCDE02CqraN++jY5t2+nYswdttaKCgwmdOZOwuXMJmzcX/+TkvzhOa83x5uPsqNzBjqod5Nfm49ROIgMjmTVsFnNT5zJ72GyigwZ+fipPOhcu50KloKqFw5Ut50eJ+fsqPlp9K2OH9bzccm8kSNxIkAgx9Li6u+ncu5f2bdtp37bNaAIDAkeNInTWLEJnzSRk6lR8Qv5yjqlWWytfnfmKHZU72Fm1k8buRhSKCXETmJ48nenJ05kYP3FQLdDVX9zD5XBlC99bOIrggOu7K5EgcSNBIsTQprXGVlpK+7ZtdOzaRWfeAbTNBv7+hOTmEjprJqGzZhE0bhzK7+IRTi7t4mjDUbZXbWdX1S4K6gtwaicBPgHkJuQyLWka05OnMy5u3KCerdgbJEjcSJAIcWNxdXfTeeAAnbt30/7VV1iLjgLgEx5O6IzphMyYQei0aQRkZf3F5JId9g4O1Bxg39l97Kvex7HGY2g0wX7BTE6czPSk6UxLnsbomNFDeohxf5AgcSNBIsSNzdHYSOeePXTs3k3Hrq+wnzkDgG90NCFTJhMydSohU6cSaLH8RbA0dzeTV5PH3rN72Ve9j7KWMgAiAiKYmjSVyYmTmZQwCUuMZVCuAjmQJEjcSJAIcfPQWmOvrKRz33469xuvc/0rPhERhEw+FyxTCBo9GuV/cXNWXWcd+6qNu5W9Z/dS1W4cG+wXTE58DpMSJnFLwi3kxOcMiSnx+0KCxI0EiRA3N/uZM0ao5OXRuW8/NnM2cBUURND4cYTk5hJ8yy0E5+biFxt70bE1HTUcrDvIwZqDHKw9SHFTMS7twkf5YIm2MClxEjlxOUyIm0BqeOqQnMalJxIkbiRIhBDu7DW1dB3IozM/n678Q3QfPQp2c8js8OEE5+YSnDuR4NxcgiyWizrw223tHK47zNe1X3Ow9iCH6w7T7ewGIDIwkvGx4xkfd+EVFxznlTr2BwkSNxIkQojeuLq76S4qoutgPl35+XTmH8RZVw+ACg4maOxYgsaNJXj8eILGjSNgxAiUrzGk1u6yU9pcypH6IxTWF3Kk/ggnmk/g0i4AkkKTmBA3wQiW2PGMjR1LWMDl5xgbbCRI3EiQCCGuhdYae9UZuvKNYOkuLKT76FF0t3Hn4RMSYobLOILOh0v6+Y78TnsnxxqPXRQule2VACgUGZEZjI8zQmV0zGiyo7MJD+j70rj9TYLEjQSJEKKvtMOBtayM7oJCugsKjHA5dgxttQLgExpqhMv48QSNGU2gxUJgRgYqIAAwpskvbCi8KFwauxvPnz81LBVLjAVLjIXR0aOxxFhIDk32ap+LBIkbCRIhxEDQdrsZLgV0FRTQXViE9dgx42FJAH9/AjMzCbRkE2SxEJhtIdCSjV98PAC1nbUUNxVT3FjMscZjlDSVUNFagcb4dzk8IBxLtOX8XUtWVBZZUVkeGy0mQeJGgkQI4SnabsdWXk53cQnW4mK6S4qxFpfgqK4+v49vdDSBFgtBlmwjXLJHEZCRiW9YKJ32To43Hz8fLsVNxRxvOk6Xo+v88cmhyWRFZTEyaqQRLpFGwIT4/+V0MX0hQeJGgkQI4W3O5ma6S0qwFpdgLSk2gub4cXTXhYDwS0oiMCuLgKxMAjOzCMzKNJ7Oj4qksr2S0uZSSptLOdF8gtLmUk62nMTmsp0/fljosIsCJjMyk+yYbAJ9r2/VRQkSNxIkQojBSDud2E6dwlZairW0DFtZKdYTpVhPnkR3dp7fzzcqioCsLKOZbGQWAZmZBKSno5ISqOqqvhAuLaXnA8buMoYzv3/P+4yKHnVd5ZMgcSNBIoQYSrTLhaO6+kK4lJZhLS3FVlqKs7n5wo7+/gSkpBCQnk7AiHQCRowgID0dn+GpnA2zc7KtnLkpc697BcmrDZKba+IYIYQYApSPD/7DhuE/bBjMufWi7xyNjdhOnsRWXoGtvBxbRQW2igo69u27qJlMBQQwIm04rv/MgMzMAS3vgAaJUmoJ8DLgC6zVWv/iku8fB1YDTqAdeExrXaSUGgEcBYrNXfdorR83j5kM/AEIBjYC39M3w22VEEIAfjEx+MXEEDJ58kXbtdY4amuNgKkoN3+twDd64BfzGrAgUUr5Ar8GFgOVwH6l1Mda6yK33d7SWr9i7n8P8CKwxPyuVGude5lTrwH+FtiLESRLgE8HphZCCDE0KKXwT0zEPzGR0OnTPPqzB3Ky/WnACa11mdbaBqwD7nXfQWvd6vYxFOj1zkIplQxEaK33mHchrwHL+7fYQgghrsVABkkKcNrtc6W57SJKqdVKqVLgX4Gn3b7KUEodVEptU0rNcTtn5ZXOaZ73MaVUnlIqr66uri/1EEII0QuvL/+ltf611joL+BHwM3PzWSBNa30L8HfAW0qpa1q9Xmv9W631FK31lHjzKVIhhBD9byCDpAoY7vY51dzWk3WYzVRaa6vWusF8fwAoBbLN41Ov4ZxCCCEG2EAGyX5glFIqQykVAKwEPnbfQSnl/pTMMuC4uT3e7KxHKZUJjALKtNZngVal1AxlzGT2EPDRANZBCCHEFQzYqC2ttUMp9RTwOcbw31e11oVKqeeBPK31x8BTSqlFgB1oAh42D58LPK+UsgMu4HGt9blpMp/kwvDfT5ERW0II4VXyZLsQQojLuton273e2S6EEGJouynuSJRSdUDFdR4eB9T3Y3G8SeoyOEldBqcbpS59qUe61vqKw15viiDpC6VU3tXc2g0FUpfBSeoyON0odfFEPaRpSwghRJ9IkAghhOgTCZIr+623C9CPpC6Dk9RlcLpR6jLg9ZA+EiGEEH0idyRCCCH6RIJECCFEn0iQ9EIptUQpVayUOqGU+rG3y3MtlFLlSqkjSql8pVSeuS1GKbVJKXXc/HXgl067TkqpV5VStUqpArdtly2/MvyneZ0OK6Umea/kF+uhHs8pparMa5OvlFrq9t1PzHoUK6Xu8E6pL08pNVwp9aVSqkgpVaiU+p65fShel57qMuSujVIqSCm1Tyl1yKzL/zG3Zyil9pplftuc8xClVKD5+YT5/Yg+F0JrLa/LvDDmBysFMoEA4BAw1tvluobylwNxl2z7V+DH5vsfA7/0djl7Kf9cYBJQcKXyA0sx5lxTwAxgr7fLf4V6PAf84DL7jjX/nAUCGeafP19v18GtfMnAJPN9OFBilnkoXpee6jLkro35+xtmvvfHWD12BvBnYKW5/RXgCfP9k8Ar5vuVwNt9LYPckfTsiis8DkH3An803/+RQby6pNZ6O9B4yeaeyn8v8Jo27AGizNU0va6HevTkXmCdNpZROAmcwPhzOChorc9qrb8237cBRzEWlhuK16WnuvRk0F4b8/e33fzob740sAB419x+6XU5d73eBRaas6lfNwmSnl3VCo+DmAb+Ryl1QCn1mLktURtT8QNUA4neKdp166n8Q/FaPWU297zq1sQ4ZOphNofcgvG/3yF9XS6pCwzBa6OU8lVK5QO1wCaMO6ZmrbXD3MW9vOfrYn7fAsT25edLkNy4btVaTwLuBFYrpea6f6mN+9ohO/Z7iJd/DZAF5GKsBvqCd4tzbZRSYcB7wPe11q3u3w2163KZugzJa6O1dmqtczEW+5sGjPbkz5cg6dm1rvA4qGitq8xfa4EPMP5w1ZxrWjB/rfVeCa9LT+UfUtdKa11j/sV3Ab/jQhPJoK+HUsof4x/eN7XW75ubh+R1uVxdhvK1AdBaNwNfAjMxmhLPrTnlXt7zdTG/jwQa+vJzJUh6dsUVHgcrpVSoUir83HvgdqAAo/znFg97mKG3umRP5f8YeMgcJTQDaHFrahl0LuknuA/j2oBRj5XmqJoMjJVB93m6fD0x29F/DxzVWr/o9tWQuy491WUoXhtlrCgbZb4PBhZj9Pl8CXzL3O3S63Luen0L+MK8k7x+3h5xMJhfGKNOSjDaG3/q7fJcQ7kzMUaYHAIKz5Udox10C8aSxpuBGG+XtZc6/AmjacGO0b77aE/lxxi18mvzqdamLwAAAkJJREFUOh0Bpni7/Feox+tmOQ+bf6mT3fb/qVmPYuBOb5f/krrcitFsdRjIN19Lh+h16akuQ+7aADnAQbPMBcCz5vZMjLA7AbwDBJrbg8zPJ8zvM/taBpkiRQghRJ9I05YQQog+kSARQgjRJxIkQggh+kSCRAghRJ9IkAghhOgTCRIhBjml1Hyl1Hpvl0OInkiQCCGE6BMJEiH6iVLqr8x1IfKVUr8xJ9JrV0r9h7lOxBalVLy5b65Sao85OeAHbmt4jFRKbTbXlvhaKZVlnj5MKfWuUuqYUurNvs7WKkR/kiARoh8opcYAK4DZ2pg8zwl8BwgF8rTW44BtwD+Yh7wG/EhrnYPxJPW57W8Cv9ZaTwRmYTwVD8bstN/HWBcjE5g94JUS4ir5XXkXIcRVWAhMBvabNwvBGJMXuoC3zX3eAN5XSkUCUVrrbeb2PwLvmPOjpWitPwDQWncDmOfbp7WuND/nAyOAnQNfLSGuTIJEiP6hgD9qrX9y0Ualfn7Jftc7J5HV7b0T+bsrBhFp2hKif2wBvqWUSoDz65inY/wdOzcD64PATq11C9CklJpjbl8FbNPGSn2VSqnl5jkClVIhHq2FENdB/lcjRD/QWhcppX6GsSqlD8Zsv6uBDmCa+V0tRj8KGNN4v2IGRRnwN+b2VcBvlFLPm+e434PVEOK6yOy/QgwgpVS71jrM2+UQYiBJ05YQQog+kTsSIYQQfSJ3JEIIIfpEgkQIIUSfSJAIIYToEwkSIYQQfSJBIoQQok/+P2Gla2tuNSvPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bcbb5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5151605010032654, 0.5143791238466898, 0.5135753750801086, 0.5127783417701721, 0.5119520425796509, 0.511167307694753, 0.5103645126024882, 0.509570320447286, 0.5087696313858032, 0.5079987446467081, 0.5072089036305746, 0.5064282417297363, 0.5056346654891968, 0.5048408905665079, 0.5040481785933176, 0.5032673676808676, 0.5024622678756714, 0.5016690293947855, 0.500874658425649, 0.5000800689061483, 0.4992792308330536, 0.4984790583451589, 0.497671514749527, 0.4968661367893219, 0.4960734446843465, 0.4952548146247864, 0.49444762865702313, 0.4936362604300181, 0.4928263823191325, 0.49201441804567975, 0.4912068446477254, 0.4904017945130666, 0.48959516485532123, 0.4887685378392537, 0.4879641632239024, 0.4871511956055959, 0.48633909225463867, 0.48552435636520386, 0.48470671971638996, 0.48389240105946857, 0.48309948047002155, 0.48226629694302875, 0.4814603527386983, 0.48063676555951435, 0.47981977462768555, 0.47900546590487164, 0.4781837463378906, 0.4773637354373932, 0.47655226786931354, 0.4757331907749176, 0.4749091565608978, 0.4740934669971466, 0.4732627471288045, 0.4724418918291728, 0.47162120540936786, 0.4707985321680705, 0.4699755907058716, 0.4691537320613861, 0.4683330257733663, 0.46749479571978253, 0.4666684667269389, 0.4658400813738505, 0.4650130569934845, 0.4641907215118408, 0.4633588989575704, 0.4625317056973775, 0.4617115358511607, 0.4608731468518575, 0.4600503245989482, 0.4592244227727254, 0.45839886864026386, 0.45757410923639935, 0.45674359798431396, 0.45592001080513, 0.4550958176453908, 0.45426951845486957, 0.45345012346903485, 0.4526224633057912, 0.4518052240212758, 0.4509771168231964, 0.45016977190971375, 0.4493425985177358, 0.44852588574091595, 0.4477149148782094, 0.44690242409706116, 0.44610140721003216, 0.44528698921203613, 0.4444826543331146, 0.4436777929464976, 0.44287750124931335, 0.4420807957649231, 0.4412892659505208, 0.44049110015233356, 0.43970457712809247, 0.43891920646031696, 0.43813278277715045, 0.43734320004781085, 0.4365622003873189, 0.43578527371088666, 0.4350113073984782, 0.4342428743839264, 0.43347641825675964, 0.43270976344744366, 0.43195877472559613, 0.4311979015668233, 0.4304463764031728, 0.42969629168510437, 0.42895517746607464, 0.42821181813875836, 0.42746851841608685, 0.4267378846804301, 0.42601190010706586, 0.4252764383951823, 0.42455899715423584, 0.42384154597918194, 0.42312170068422955, 0.4224226474761963, 0.421708881855011, 0.42100723584493, 0.4203069706757863, 0.4196241497993469, 0.41892654697100323, 0.4182436168193817, 0.417560875415802, 0.41688160101572674, 0.41620931029319763, 0.4155471920967102, 0.4148822824160258, 0.4142267902692159, 0.4135631322860718, 0.4129101634025574, 0.4122644563515981, 0.4116283158461253, 0.41099273165067035, 0.4103483557701111, 0.4097275634606679, 0.4090964396794637, 0.40847501158714294, 0.4078566332658132, 0.4072591960430145, 0.4066438674926758, 0.40605353315671283, 0.405445396900177, 0.40486031770706177, 0.40427544713020325, 0.4036845366160075, 0.40310609340667725, 0.4025268852710724, 0.4019595980644226, 0.4013869563738505, 0.400835524002711, 0.4002721707026164, 0.3997219204902649, 0.39917967716852826, 0.3986240526040395, 0.3980825940767924, 0.3975456158320109, 0.3970123529434204, 0.39648905396461487, 0.39595980445543927, 0.3954443732897441, 0.3949345449606578, 0.394416739543279, 0.3939092755317688, 0.393401841322581, 0.3929123878479004, 0.3924081126848857, 0.3919294277826945, 0.39145562052726746, 0.39095916350682575, 0.3904708723227183, 0.39001961549123126, 0.3895366390546163, 0.38907525936762494, 0.3886101047197978, 0.3881581525007884, 0.38770224650700885, 0.38725388050079346, 0.38681360085805255, 0.3863676389058431, 0.38592952489852905, 0.3854937156041463, 0.3850633104642232, 0.38465391596158344, 0.3842111627260844, 0.3837923804918925, 0.38337934017181396, 0.3829730451107025, 0.3825591206550598, 0.38215388854344684, 0.3817529280980428, 0.3813587824503581, 0.38096816341082257, 0.3805842200915019, 0.38019583622614544, 0.37980642914772034, 0.37943429748217267, 0.37906219561894733, 0.37868496775627136, 0.3783114055792491, 0.3779488503932953, 0.37758644421895343, 0.377227246761322, 0.3768751323223114, 0.3765211800734202, 0.3761642277240753, 0.37583570679028827, 0.37547632058461505, 0.37514355778694153, 0.3748132586479187, 0.3744798004627228, 0.37414692838986713, 0.3738144834836324, 0.3734943668047587, 0.37317269047101337, 0.37285160024960834, 0.3725469509760539, 0.37223519881566364, 0.37192192673683167, 0.3716091712315877, 0.37130920092264813, 0.37101008494695026, 0.3707147538661957, 0.3704240024089813, 0.3701288898785909, 0.36984219153722125, 0.3695538143316905, 0.36927663286526996, 0.36898545424143475, 0.36871470014254254, 0.36843815445899963, 0.3681645890076955, 0.36789514621098834, 0.36762574315071106, 0.36736053228378296, 0.36709368228912354, 0.36683860421180725, 0.36658692359924316, 0.3663238187630971, 0.3660767177740733, 0.36582085490226746, 0.3655788004398346, 0.3653219739596049, 0.365083505709966, 0.36484599113464355, 0.36461212237675983, 0.3643704454104106, 0.3641344408194224, 0.3639046251773834, 0.36367467045783997, 0.36345430215199787, 0.363221397002538, 0.36300374070803326, 0.36277978618939716, 0.3625710109869639, 0.36234206954638165, 0.362129549185435, 0.3619205256303151, 0.36170827349026996, 0.3614981174468994, 0.3612888554732005, 0.3610854744911194, 0.3608880043029785, 0.36068564653396606, 0.36048440138498944, 0.36028698086738586, 0.36009607712427777, 0.3599003156026204, 0.3597061038017273, 0.3595230281352997, 0.35933224360148114, 0.3591480851173401, 0.35896239678064984, 0.35877830783526105, 0.3585967421531677, 0.35841888189315796, 0.35824082295099896, 0.35806538661321, 0.357891043027242, 0.3577168683211009, 0.3575487832228343, 0.3573784828186035, 0.3572094639142354, 0.35704641540845233, 0.3568797707557678, 0.35671648383140564, 0.35655643542607623, 0.3563959002494812, 0.35623786846796673, 0.3560788730780284, 0.3559292753537496, 0.3557703693707784, 0.3556138575077057, 0.3554638723532359, 0.3553139368693034, 0.3551679154237111, 0.35501553614934284, 0.35487322012583417, 0.3547295431296031, 0.35458145538965863]\n",
      "[0.5074172616004944, 0.5065680742263794, 0.5056968331336975, 0.5048220753669739, 0.5039770007133484, 0.50311678647995, 0.5022872090339661, 0.5014476776123047, 0.5006255507469177, 0.49979838728904724, 0.4989659786224365, 0.4981275498867035, 0.4972974359989166, 0.49647045135498047, 0.49563273787498474, 0.4947892129421234, 0.49394965171813965, 0.4931093454360962, 0.4922691583633423, 0.4914151132106781, 0.4905604422092438, 0.48971155285835266, 0.48886585235595703, 0.48801741003990173, 0.4871629774570465, 0.48631751537323, 0.48546895384788513, 0.4846251904964447, 0.48377975821495056, 0.482943058013916, 0.4820971190929413, 0.48124828934669495, 0.4803963005542755, 0.4795564115047455, 0.478710412979126, 0.47786059975624084, 0.4770108163356781, 0.47616061568260193, 0.4753246307373047, 0.47449323534965515, 0.47364482283592224, 0.47280728816986084, 0.4719589948654175, 0.47111430764198303, 0.4702723026275635, 0.4694213569164276, 0.4685805141925812, 0.46774622797966003, 0.46690383553504944, 0.46606555581092834, 0.46523284912109375, 0.46438154578208923, 0.46355053782463074, 0.46271681785583496, 0.46188077330589294, 0.46104463934898376, 0.4602123498916626, 0.459367960691452, 0.45853129029273987, 0.4576944410800934, 0.456857830286026, 0.45602309703826904, 0.4552040100097656, 0.45435667037963867, 0.4535186290740967, 0.4527033865451813, 0.45188435912132263, 0.4510577619075775, 0.45022347569465637, 0.4494018256664276, 0.4485730230808258, 0.4477505683898926, 0.44692811369895935, 0.44610464572906494, 0.4452916383743286, 0.4444730281829834, 0.4436580240726471, 0.442847341299057, 0.44203853607177734, 0.44123515486717224, 0.4404381811618805, 0.4396355152130127, 0.4388333261013031, 0.43803343176841736, 0.43724068999290466, 0.43645820021629333, 0.4356672763824463, 0.43487784266471863, 0.43409204483032227, 0.43331122398376465, 0.43253085017204285, 0.43176427483558655, 0.43099498748779297, 0.4302334487438202, 0.4294748306274414, 0.4287225306034088, 0.42796528339385986, 0.4272061288356781, 0.4264559745788574, 0.42570820450782776, 0.4249667227268219, 0.4242330491542816, 0.4234955310821533, 0.4227747917175293, 0.42205390334129333, 0.4213358461856842, 0.42062267661094666, 0.4199172556400299, 0.41921448707580566, 0.41851186752319336, 0.41781672835350037, 0.41712701320648193, 0.4164334237575531, 0.41575124859809875, 0.4150729179382324, 0.41439294815063477, 0.41372814774513245, 0.4130610525608063, 0.41239866614341736, 0.41174015402793884, 0.4110938012599945, 0.4104473292827606, 0.4098062217235565, 0.4091678559780121, 0.40853333473205566, 0.40790238976478577, 0.40728068351745605, 0.406663179397583, 0.4060521423816681, 0.40544092655181885, 0.4048316776752472, 0.40422824025154114, 0.40363454818725586, 0.40304604172706604, 0.40245649218559265, 0.40187549591064453, 0.4012952744960785, 0.4007209837436676, 0.4001491963863373, 0.39958420395851135, 0.3990252912044525, 0.39847636222839355, 0.3979271352291107, 0.39738592505455017, 0.39684978127479553, 0.3963148295879364, 0.395783394575119, 0.39525601267814636, 0.39473435282707214, 0.3942134380340576, 0.3937018811702728, 0.393192857503891, 0.39268791675567627, 0.39219096302986145, 0.3916947543621063, 0.39120256900787354, 0.3907136917114258, 0.3902294635772705, 0.38974952697753906, 0.3892730176448822, 0.38880065083503723, 0.38833627104759216, 0.38787195086479187, 0.38741350173950195, 0.3869568407535553, 0.3865063488483429, 0.38605713844299316, 0.38561567664146423, 0.3851819336414337, 0.3847472667694092, 0.3843131959438324, 0.38389065861701965, 0.38346827030181885, 0.38304993510246277, 0.3826347887516022, 0.38222536444664, 0.38181746006011963, 0.381413072347641, 0.3810153007507324, 0.3806189000606537, 0.38022729754447937, 0.3798384368419647, 0.3794536590576172, 0.3790750205516815, 0.37869563698768616, 0.37832024693489075, 0.37794721126556396, 0.3775809705257416, 0.3772169351577759, 0.3768540322780609, 0.37649548053741455, 0.37614014744758606, 0.3757898509502411, 0.3754449784755707, 0.3751021921634674, 0.3747604787349701, 0.3744238317012787, 0.3740919530391693, 0.3737598657608032, 0.3734320402145386, 0.3731069564819336, 0.3727852404117584, 0.37246575951576233, 0.37215080857276917, 0.37183868885040283, 0.3715269863605499, 0.371222585439682, 0.37091782689094543, 0.37061718106269836, 0.37031999230384827, 0.3700251579284668, 0.3697328269481659, 0.3694419860839844, 0.36915603280067444, 0.3688722550868988, 0.3685903549194336, 0.3683135211467743, 0.3680383861064911, 0.36776575446128845, 0.3674940764904022, 0.3672252595424652, 0.36695948243141174, 0.36669692397117615, 0.3664371967315674, 0.3661787807941437, 0.36592379212379456, 0.3656703531742096, 0.3654211461544037, 0.36517134308815, 0.36492571234703064, 0.3646829128265381, 0.3644423484802246, 0.3642036020755768, 0.36396685242652893, 0.36373186111450195, 0.36349835991859436, 0.3632694184780121, 0.3630436360836029, 0.3628173768520355, 0.36259499192237854, 0.3623743951320648, 0.3621562421321869, 0.3619382381439209, 0.3617231845855713, 0.36151042580604553, 0.36130133271217346, 0.3610924780368805, 0.3608858585357666, 0.3606814444065094, 0.3604789674282074, 0.36027991771698, 0.36008119583129883, 0.3598853051662445, 0.35969042778015137, 0.3594989776611328, 0.359307199716568, 0.35911795496940613, 0.35893091559410095, 0.3587455451488495, 0.35856160521507263, 0.35837841033935547, 0.35819709300994873, 0.3580187261104584, 0.3578416407108307, 0.3576662540435791, 0.35749220848083496, 0.3573208749294281, 0.3571501672267914, 0.3569803237915039, 0.3568139970302582, 0.35664844512939453, 0.3564840853214264, 0.3563217222690582, 0.356160968542099, 0.35600030422210693, 0.355842262506485, 0.3556852340698242, 0.35552993416786194, 0.3553764820098877, 0.35522374510765076, 0.355072945356369, 0.35492363572120667, 0.3547755181789398, 0.354629248380661, 0.3544839322566986, 0.35433974862098694, 0.3541969954967499, 0.354055792093277, 0.35391607880592346, 0.3537770211696625, 0.3536405861377716, 0.35350456833839417, 0.3533691465854645, 0.35323548316955566, 0.35310253500938416, 0.3529717028141022, 0.3528408110141754, 0.3527117669582367, 0.3525841236114502, 0.3524568974971771]\n",
      "\r",
      "30/30 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.3544222414493561\n",
      "acc: 33.33333432674408%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c4df668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN = KerasNeuralNetwork(x_train, y_train, x_test, y_test)\n",
    "NN.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
