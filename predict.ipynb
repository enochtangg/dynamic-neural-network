{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is a classification neural network\n",
    "\n",
    "# Import all packages and libraries for NN\n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy\n",
    "import keras\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# Preparing Iris dataset for usage as a dataframe\n",
    "# Encoding output variables\n",
    "# Shuffle rows randomly for variety\n",
    "# Spliting dataset to test and train dataframes. Test dataset will be 80% of entire dataframe.\n",
    "# Parsing dataframe to features and labels (x_train and y_train)\n",
    "\n",
    "# Constants\n",
    "_training_split = 0.8\n",
    "\n",
    "\n",
    "# Spliting dataframe to training and testing\n",
    "def split_to_training(dataframe):\n",
    "    train_df = dataframe[:int(len(dataframe)*_training_split)]\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_to_testing(dataframe):\n",
    "    test_df = dataframe[int(len(dataframe)*_training_split):]\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "\n",
    "# Spliting training or testing dataset to x and y\n",
    "def split_to_x(dataframe):\n",
    "    x = dataframe[features].values\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def split_to_y(dataframe):\n",
    "    y = dataframe[labels].values\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Encoding all labels with HOT Encoder\n",
    "def encode_dataframe(label_y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(label_y)\n",
    "    encoded_Y = encoder.transform(label_y)\n",
    "    new_y = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    return new_y\n",
    "\n",
    "\n",
    "def prep_data(df):\n",
    "    pass\n",
    "    \n",
    "# Load in dataset\n",
    "dataframe = pandas.read_csv('iris.csv')\n",
    "df = shuffle(dataframe)\n",
    "features = list(df.columns.values)[:-1]\n",
    "labels = list(df.columns.values)[-1]\n",
    "input_dim = len(list(df.columns.values)[:-1])\n",
    "\n",
    "train_df = split_to_training(df)\n",
    "x_train = split_to_x(train_df)\n",
    "y_train = encode_dataframe(split_to_y(train_df))\n",
    "\n",
    "test_df = split_to_testing(df)\n",
    "x_test = split_to_x(test_df)\n",
    "y_test = encode_dataframe(split_to_y(test_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasNeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        _number_of_test_models = 5\n",
    "        \n",
    "        base_number_layers = int(input_dim**.5)\n",
    "        list_of_loss = []\n",
    "        list_of_models = []\n",
    "        list_of_history = []\n",
    "        for i in range(_number_of_test_models):\n",
    "            model = Sequential()\n",
    "            # Base number of layers\n",
    "            model.add(Dense(input_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "            for i in range(base_number_layers):\n",
    "                model.add(Dense(5, activation='sigmoid'))\n",
    "            model.add(Dense(3, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "            list_of_models.append(model)\n",
    "\n",
    "            \n",
    "            history = model.fit(self.x_train, self.y_train, epochs=30)\n",
    "            list_of_history.append(history)\n",
    "        \n",
    "        \n",
    "            # Compare loss after each epoch\n",
    "            loss_after_each_epoch = history.history['loss']\n",
    "            avg_roc = abs((loss_after_each_epoch[0]-loss_after_each_epoch[-1])/len(loss_after_each_epoch))\n",
    "            list_of_loss.append(avg_roc)\n",
    "            base_number_layers += 1\n",
    "            \n",
    "            scores = model.evaluate(self.x_test, self.y_test)\n",
    "            print(\"{}: {}\".format(model.metrics_names[0], scores[0]))\n",
    "            print(\"{}: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        # Take the first trough in list and use that model\n",
    "        minimal_loss_position = 0;\n",
    "        for index, value in enumerate(list_of_loss):\n",
    "            if value < list_of_loss[index+1]:\n",
    "                minimal_loss_position = index\n",
    "                break\n",
    "        print(list_of_loss)\n",
    "        print(list_of_models)\n",
    "        print('The optimal model is at index: {}'.format(minimal_loss_position))\n",
    "        optimized_model = list_of_models[minimal_loss_position]\n",
    "        optimized_history = list_of_history[minimal_loss_position]\n",
    "        \n",
    "        # Evaluating the model\n",
    "        print(optimized_history.history)\n",
    "        scores = optimized_model.evaluate(self.x_test, self.y_test)\n",
    "        print(\"{}: {}\".format(optimized_model.metrics_names[0], scores[0]))\n",
    "        print(\"{}: {}%\".format(optimized_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.4681 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4668 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.4654 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.4640 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4626 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.4612 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4599 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.4585 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4571 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.4557 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.4543 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4529 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.4515 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4502 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4488 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4474 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4461 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.4447 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.4433 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 102us/step - loss: 0.4420 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.4406 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4392 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4379 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.4365 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4352 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.4338 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.4325 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.4311 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.4298 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4284 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 1ms/step\n",
      "loss: 0.42759376764297485\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.4991 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.4980 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4969 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4958 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.4947 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4935 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.4925 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.4915 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.4904 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4893 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4883 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.4873 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4863 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.4852 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4842 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.4831 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.4821 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4811 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4801 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4790 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.4780 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4770 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4760 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4750 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4740 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4730 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4720 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.4710 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.4700 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.4690 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 2ms/step\n",
      "loss: 0.46837443113327026\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 6ms/step - loss: 0.4967 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4957 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4947 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4937 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4927 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4917 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4908 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4898 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4888 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4878 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4868 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4858 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4848 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4838 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4828 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4818 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4807 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4798 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4787 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4777 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4767 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4757 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4747 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4737 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4727 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4717 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4707 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4697 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4686 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4677 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "loss: 0.4669937193393707\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4990 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4978 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4967 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4955 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4944 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4932 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4921 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4909 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4897 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4885 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4874 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4862 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4850 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4838 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4826 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4815 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4802 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4790 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4779 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4766 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4754 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4742 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4729 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4717 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4704 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4692 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4680 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4667 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4654 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4641 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 5ms/step\n",
      "loss: 0.46329963207244873\n",
      "acc: 33.33333432674408%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.5270 - acc: 0.3333\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5257 - acc: 0.3333\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5244 - acc: 0.3333\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5231 - acc: 0.3333\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5218 - acc: 0.3333\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5205 - acc: 0.3333\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5192 - acc: 0.3333\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5180 - acc: 0.3333\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5167 - acc: 0.3333\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5154 - acc: 0.3333\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5142 - acc: 0.3333\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5130 - acc: 0.3333\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.5117 - acc: 0.3333\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5105 - acc: 0.3333\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5093 - acc: 0.3333\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5080 - acc: 0.3333\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5068 - acc: 0.3333\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5056 - acc: 0.3333\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5043 - acc: 0.3333\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.5031 - acc: 0.3333\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5019 - acc: 0.3333\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5007 - acc: 0.3333\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4995 - acc: 0.3333\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4983 - acc: 0.3333\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4971 - acc: 0.3333\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4959 - acc: 0.3333\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4947 - acc: 0.3333\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4935 - acc: 0.3333\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4923 - acc: 0.3333\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4912 - acc: 0.3333\n",
      "30/30 [==============================] - 0s 7ms/step\n",
      "loss: 0.49040207266807556\n",
      "acc: 33.33333432674408%\n",
      "[0.0013232628504435222, 0.0010024586651060318, 0.0009680727455351091, 0.0011644688579771246, 0.001196271710925632]\n",
      "[<keras.models.Sequential object at 0x115ec6470>, <keras.models.Sequential object at 0x115fce208>, <keras.models.Sequential object at 0x116a38ac8>, <keras.models.Sequential object at 0x117135630>, <keras.models.Sequential object at 0x117c2ce80>]\n",
      "The optimal model is at index: 2\n",
      "{'loss': [0.4967126131057739, 0.4957010308901469, 0.4946884433428446, 0.4937111079692841, 0.4927235225836436, 0.4917434811592102, 0.4907742460568746, 0.4897710939248403, 0.48877108494440713, 0.48778550227483114, 0.4868024428685506, 0.48579513231913246, 0.4847765306631724, 0.4837740977605184, 0.48278584480285647, 0.4817606230576833, 0.4807487726211548, 0.47975003719329834, 0.47873616218566895, 0.4777319411436717, 0.47671142816543577, 0.47570291757583616, 0.47468063831329343, 0.47370071013768517, 0.4726886093616486, 0.4716620683670044, 0.4706712007522583, 0.46965757211049397, 0.4686498602231344, 0.46767043073972064], 'acc': [0.3333333323399226, 0.3333333333333333, 0.33333333134651183, 0.33333333134651183, 0.3333333353201548, 0.3333333333333333, 0.3333333333333333, 0.33333333134651183, 0.3333333353201548, 0.33333333134651183, 0.33333333134651183, 0.3333333333333333, 0.3333333333333333, 0.3333333353201548, 0.3333333323399226, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333353201548, 0.33333333134651183, 0.33333333134651183, 0.3333333323399226, 0.3333333323399226, 0.3333333323399226, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.33333333432674406, 0.3333333323399226]}\n",
      "30/30 [==============================] - 0s 31us/step\n",
      "loss: 0.4669937193393707\n",
      "acc: 33.33333432674408%\n"
     ]
    }
   ],
   "source": [
    "NN = KerasNeuralNetwork(x_train, y_train, x_test, y_test)\n",
    "NN.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
