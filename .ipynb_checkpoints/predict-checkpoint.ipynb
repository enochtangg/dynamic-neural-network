{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is a classification neural network\n",
    "\n",
    "# Import all packages and libraries for NN\n",
    "\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "# from tensorflow import set_random_seed\n",
    "# set_random_seed(2)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from numpy import argmax\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy\n",
    "import keras\n",
    "import pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "# Preparing Iris dataset for usage as a dataframe\n",
    "# Encoding output variables\n",
    "# Shuffle rows randomly for variety\n",
    "# Spliting dataset to test and train dataframes. Test dataset will be 80% of entire dataframe.\n",
    "# Parsing dataframe to features and labels (x_train and y_train)\n",
    "\n",
    "# Constants\n",
    "_training_split = 0.8\n",
    "\n",
    "\n",
    "# Spliting dataframe to training and testing\n",
    "def split_to_training(dataframe):\n",
    "    train_df = dataframe[:int(len(dataframe)*_training_split)]\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def split_to_testing(dataframe):\n",
    "    test_df = dataframe[int(len(dataframe)*_training_split):]\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "\n",
    "# Spliting training or testing dataset to x and y\n",
    "def split_to_x(dataframe):\n",
    "    x = dataframe[features].values\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def split_to_y(dataframe):\n",
    "    y = dataframe[labels].values\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Encoding all labels with HOT Encoder\n",
    "def encode_dataframe(label_y):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(label_y)\n",
    "    encoded_Y = encoder.transform(label_y)\n",
    "    new_y = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    return new_y\n",
    "\n",
    "\n",
    "def prep_data(df):\n",
    "    pass\n",
    "    \n",
    "# Load in dataset\n",
    "dataframe = pandas.read_csv('iris.csv')\n",
    "df = shuffle(dataframe)\n",
    "features = list(df.columns.values)[:-1]\n",
    "labels = list(df.columns.values)[-1]\n",
    "input_dim = len(list(df.columns.values)[:-1])\n",
    "\n",
    "train_df = split_to_training(df)\n",
    "x_train = split_to_x(train_df)\n",
    "y_train = encode_dataframe(split_to_y(train_df))\n",
    "\n",
    "test_df = split_to_testing(df)\n",
    "x_test = split_to_x(test_df)\n",
    "y_test = encode_dataframe(split_to_y(test_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasNeuralNetwork:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        _number_of_test_models = 5\n",
    "        \n",
    "        base_number_layers = int(input_dim**.5)\n",
    "        list_of_loss = []\n",
    "        list_of_models = []\n",
    "        list_of_history = []\n",
    "        for i in range(_number_of_test_models):\n",
    "            model = Sequential()\n",
    "            # Base number of layers\n",
    "            model.add(Dense(input_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "            for i in range(base_number_layers):\n",
    "                model.add(Dense(5, activation='sigmoid'))\n",
    "            model.add(Dense(3, activation='sigmoid'))\n",
    "            model.summary()\n",
    "        \n",
    "            model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
    "            list_of_models.append(model)\n",
    "\n",
    "            \n",
    "            history = model.fit(self.x_train, self.y_train, epochs=30)\n",
    "            list_of_history.append(history)\n",
    "        \n",
    "        \n",
    "            # Compare loss after each epoch\n",
    "            loss_after_each_epoch = history.history['loss']\n",
    "            avg_roc = abs((loss_after_each_epoch[0]-loss_after_each_epoch[-1])/len(loss_after_each_epoch))\n",
    "            list_of_loss.append(avg_roc)\n",
    "            base_number_layers += 1\n",
    "            \n",
    "            scores = model.evaluate(self.x_test, self.y_test)\n",
    "            print(\"{}: {}\".format(model.metrics_names[0], scores[0]))\n",
    "            print(\"{}: {}%\".format(model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        # Take the first trough in list and use that model\n",
    "        minimal_loss_position = 0;\n",
    "        for index, value in enumerate(list_of_loss):\n",
    "            if value < list_of_loss[index+1]:\n",
    "                minimal_loss_position = index\n",
    "                break\n",
    "        print(list_of_loss)\n",
    "        print(list_of_models)\n",
    "        print('The optimal model is at index: {}'.format(minimal_loss_position))\n",
    "        optimized_model = list_of_models[minimal_loss_position]\n",
    "        optimized_history = list_of_history[minimal_loss_position]\n",
    "        \n",
    "        # Evaluating the model\n",
    "        print(optimized_history.history)\n",
    "        scores = optimized_model.evaluate(self.x_test, self.y_test)\n",
    "        print(\"{}: {}\".format(optimized_model.metrics_names[0], scores[0]))\n",
    "        print(\"{}: {}%\".format(optimized_model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5694 - acc: 0.3167\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5680 - acc: 0.3167\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.5666 - acc: 0.3167\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5652 - acc: 0.3167\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5639 - acc: 0.3167\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5625 - acc: 0.3167\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5611 - acc: 0.3167\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.5598 - acc: 0.3167\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5584 - acc: 0.3167\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5571 - acc: 0.3167\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.5557 - acc: 0.3167\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5544 - acc: 0.3167\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5530 - acc: 0.3167\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5517 - acc: 0.3167\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5503 - acc: 0.3167\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5490 - acc: 0.3167\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5476 - acc: 0.3167\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5463 - acc: 0.3167\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5399 - acc: 0.437 - 0s 169us/step - loss: 0.5450 - acc: 0.3167\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5437 - acc: 0.3167\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5423 - acc: 0.3167\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5410 - acc: 0.3167\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5397 - acc: 0.3167\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5385 - acc: 0.3167\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5372 - acc: 0.3167\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5359 - acc: 0.3167\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 212us/step - loss: 0.5346 - acc: 0.3167\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5334 - acc: 0.3167\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5321 - acc: 0.3167\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5309 - acc: 0.3167\n",
      "30/30 [==============================] - 0s 8ms/step\n",
      "loss: 0.5270834565162659\n",
      "acc: 40.00000059604645%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 123\n",
      "Trainable params: 123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4456 - acc: 0.3583\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.4442 - acc: 0.3583\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4428 - acc: 0.3583\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4415 - acc: 0.3583\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.4401 - acc: 0.3583\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4388 - acc: 0.3083\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 95us/step - loss: 0.4375 - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4361 - acc: 0.3083\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4348 - acc: 0.3167\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 92us/step - loss: 0.4334 - acc: 0.3167\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4321 - acc: 0.3167\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.4308 - acc: 0.3167\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.4294 - acc: 0.3167\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.4281 - acc: 0.3167\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4268 - acc: 0.3167\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.4255 - acc: 0.3167\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4241 - acc: 0.3167\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.4228 - acc: 0.3167\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4215 - acc: 0.3167\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4202 - acc: 0.3167\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 94us/step - loss: 0.4189 - acc: 0.3167\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 96us/step - loss: 0.4176 - acc: 0.3167\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4163 - acc: 0.3167\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4150 - acc: 0.3167\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4137 - acc: 0.3167\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4124 - acc: 0.3167\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4112 - acc: 0.3167\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4099 - acc: 0.3167\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4087 - acc: 0.3167\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4075 - acc: 0.3167\n",
      "30/30 [==============================] - 0s 9ms/step\n",
      "loss: 0.40667060017585754\n",
      "acc: 40.00000059604645%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 153\n",
      "Trainable params: 153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.5314 - acc: 0.3167\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.5299 - acc: 0.3167\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5285 - acc: 0.3167\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5271 - acc: 0.3167\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5257 - acc: 0.3167\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.5243 - acc: 0.3167\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5229 - acc: 0.3167\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.5215 - acc: 0.3167\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5200 - acc: 0.3167\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5186 - acc: 0.3167\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.5172 - acc: 0.3167\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.5158 - acc: 0.3167\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5143 - acc: 0.3167\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5129 - acc: 0.3167\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.5115 - acc: 0.3167\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.5101 - acc: 0.3167\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.5087 - acc: 0.3167\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5072 - acc: 0.3167\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5058 - acc: 0.3167\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5044 - acc: 0.3167\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5029 - acc: 0.3167\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.5015 - acc: 0.3167\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5001 - acc: 0.3167\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4987 - acc: 0.3167\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4973 - acc: 0.3167\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4959 - acc: 0.3167\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4945 - acc: 0.3167\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4931 - acc: 0.3167\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4916 - acc: 0.3167\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4902 - acc: 0.3167\n",
      "30/30 [==============================] - 0s 10ms/step\n",
      "loss: 0.48134395480155945\n",
      "acc: 40.00000059604645%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 183\n",
      "Trainable params: 183\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 11ms/step - loss: 0.5030 - acc: 0.3167\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.5018 - acc: 0.3167\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5006 - acc: 0.3167\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4994 - acc: 0.3167\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4982 - acc: 0.3167\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4970 - acc: 0.3167\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.4958 - acc: 0.3167\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4946 - acc: 0.3167\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4934 - acc: 0.3167\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.4922 - acc: 0.3167\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4910 - acc: 0.3167\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4899 - acc: 0.3167\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4887 - acc: 0.3167\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4875 - acc: 0.3167\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4863 - acc: 0.3167\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4852 - acc: 0.3167\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4840 - acc: 0.3167\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4828 - acc: 0.3167\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4817 - acc: 0.3167\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4805 - acc: 0.3167\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4793 - acc: 0.3167\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4782 - acc: 0.3167\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4770 - acc: 0.3167\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4758 - acc: 0.3167\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4747 - acc: 0.3167\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4735 - acc: 0.3167\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4723 - acc: 0.3167\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4711 - acc: 0.3167\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4700 - acc: 0.3167\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4688 - acc: 0.3167\n",
      "30/30 [==============================] - 0s 12ms/step\n",
      "loss: 0.4615671932697296\n",
      "acc: 40.00000059604645%\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 5)                 25        \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 5)                 30        \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "120/120 [==============================] - 1s 12ms/step - loss: 0.4964 - acc: 0.3167\n",
      "Epoch 2/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4950 - acc: 0.3167\n",
      "Epoch 3/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4936 - acc: 0.3167\n",
      "Epoch 4/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4922 - acc: 0.3167\n",
      "Epoch 5/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4907 - acc: 0.3167\n",
      "Epoch 6/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4892 - acc: 0.3167\n",
      "Epoch 7/30\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4878 - acc: 0.3167\n",
      "Epoch 8/30\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4865 - acc: 0.3167\n",
      "Epoch 9/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4851 - acc: 0.3167\n",
      "Epoch 10/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4837 - acc: 0.3250\n",
      "Epoch 11/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4822 - acc: 0.3250\n",
      "Epoch 12/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4809 - acc: 0.3250\n",
      "Epoch 13/30\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4795 - acc: 0.3250\n",
      "Epoch 14/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4781 - acc: 0.3250\n",
      "Epoch 15/30\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4766 - acc: 0.3250\n",
      "Epoch 16/30\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4753 - acc: 0.3250\n",
      "Epoch 17/30\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4739 - acc: 0.3250\n",
      "Epoch 18/30\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4726 - acc: 0.3250\n",
      "Epoch 19/30\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4712 - acc: 0.3250\n",
      "Epoch 20/30\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.4699 - acc: 0.3250\n",
      "Epoch 21/30\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4686 - acc: 0.3250\n",
      "Epoch 22/30\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4672 - acc: 0.3250\n",
      "Epoch 23/30\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4659 - acc: 0.3250\n",
      "Epoch 24/30\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4646 - acc: 0.3250\n",
      "Epoch 25/30\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.4633 - acc: 0.3250\n",
      "Epoch 26/30\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4620 - acc: 0.3250\n",
      "Epoch 27/30\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4607 - acc: 0.3250\n",
      "Epoch 28/30\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4594 - acc: 0.3250\n",
      "Epoch 29/30\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4581 - acc: 0.3250\n",
      "Epoch 30/30\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4569 - acc: 0.3250\n",
      "30/30 [==============================] - 0s 14ms/step\n",
      "loss: 0.453442245721817\n",
      "acc: 36.666667461395264%\n",
      "[0.001283259126875136, 0.001270986795425416, 0.0013704761531617908, 0.0011408850219514636, 0.0013157420688205282]\n",
      "[<keras.models.Sequential object at 0x112504550>, <keras.models.Sequential object at 0x1136dfef0>, <keras.models.Sequential object at 0x113c38eb8>, <keras.models.Sequential object at 0x11445d908>, <keras.models.Sequential object at 0x114df3c18>]\n",
      "The optimal model is at index: 1\n",
      "{'loss': [0.44558380246162416, 0.44422416885693866, 0.4428410907586416, 0.44149083495140073, 0.4401459435621897, 0.4388034403324127, 0.43745001951853435, 0.4361198484897614, 0.4347790022691091, 0.4334363361199697, 0.43209779262542725, 0.4307603279749552, 0.4294275939464569, 0.42810134689013163, 0.42676799098650614, 0.42545226216316223, 0.42414130369822184, 0.4228215297063192, 0.4215114136536916, 0.4202016254266103, 0.41889313658078514, 0.41760133107503256, 0.41629328529040016, 0.4150156080722809, 0.4137316922346751, 0.41244811018308003, 0.41118706663449606, 0.4099327623844147, 0.4086904307206472, 0.40745419859886167], 'acc': [0.35833333333333334, 0.3583333353201548, 0.35833333134651185, 0.35833333134651185, 0.35833333333333334, 0.3083333323399226, 0.0, 0.3083333323399226, 0.31666666666666665, 0.31666666467984517, 0.31666666666666665, 0.31666666467984517, 0.31666666467984517, 0.31666666666666665, 0.3166666676600774, 0.31666666666666665, 0.31666666666666665, 0.31666666467984517, 0.31666666865348814, 0.31666666865348814, 0.31666666467984517, 0.31666666666666665, 0.31666666666666665, 0.31666666666666665, 0.3166666676600774, 0.31666666467984517, 0.3166666656732559, 0.3166666676600774, 0.31666666467984517, 0.31666666467984517]}\n",
      "30/30 [==============================] - 0s 58us/step\n",
      "loss: 0.40667060017585754\n",
      "acc: 40.00000059604645%\n"
     ]
    }
   ],
   "source": [
    "NN = KerasNeuralNetwork(x_train, y_train, x_test, y_test)\n",
    "NN.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
